{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "\n",
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')\n",
    "            \n",
    "            \n",
    "    \n",
    "vis = VisdomLinePlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Honestly, Buffalo is the correct answer. I rem...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ah yes way could have been :( remember when he...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>He wouldn't have been a bad signing if we woul...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Easy. You use the piss and dry technique. Let ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           comments       subreddits\n",
       "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
       "1   1  Ah yes way could have been :( remember when he...              nba\n",
       "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
       "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
       "4   4  Easy. You use the piss and dry technique. Let ...            funny"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_train.csv')\n",
    "df2 = pd.read_csv('reddit_test.csv')\n",
    "# df = df.sample(1000, random_state=1).copy()\n",
    "# df2 = df2.sample(1000, random_state=1).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'], mapping = df['subreddits'].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.comments.values.shape[0]):\n",
    "    df.comments.values[i] = df.comments.values[i][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.category_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'honestly', ',', 'buffalo', 'is', 'the', 'correct', 'answer', '.', 'i', 'remember', 'people', '(', 'somewhat', ')', 'joking', 'that', 'buffalo', \"'\", 's', 'mantra', 'for', 'starting', 'goal', '##ies', 'was', '\"', 'win', 'a', 'game', ',', 'get', 'traded', '\"', '.', 'i', 'think', 'edmonton', \"'\", 's', 'front', 'office', 'was', 'a', 'tr', '##aves', '##ty', 'for', 'the', 'better', 'part', 'of', '10', 'years', ',', 'but', 'buffalo', \"'\", 's', 'systematic', 'destruction', 'of', 'the', 'term', \"'\", 'competitive', \"'\", 'was', 'much', 'more', 'responsible', 'for', 'the', 'change', 'to', 'the', 'draft', 'lottery', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.05)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 2\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=20)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1005c6155ff466081d32e17be7d7d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596d622d86584459a6c570ac8d51bf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='batch', max=33250, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step (out of 8750): 500\n",
      "Train loss: 3.0693488245010374\n",
      "Validation Accuracy: 0.04542857142857143\n",
      "step (out of 8750): 1000\n",
      "Train loss: 3.0487884504795075\n",
      "Validation Accuracy: 0.044\n",
      "step (out of 8750): 1500\n",
      "Train loss: 3.0393587586085\n",
      "Validation Accuracy: 0.049428571428571426\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ad32da8d29c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Update parameters and take a step using the computed gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 1\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in tnrange(epochs, desc=\"Epoch\"):\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    step = 0\n",
    "    # Train the data for one epoch\n",
    "    for batch in tqdm_notebook(train_dataloader,desc='batch',leave=False):\n",
    "        step = step + 1\n",
    "#         if(step%10==0):\n",
    "#             print('step (out of 8750): ' + str(step) + '\\r')\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())  \n",
    "        vis.plot('loss', 'train_loss', 'Loss',step,loss.item())\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if(step%500 == 0): # in [500, 1000, 3000, 6000]\n",
    "            print('step (out of 8750): ' + str(step))\n",
    "            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "            # Validation\n",
    "\n",
    "            # Put model in evaluation mode to evaluate loss on the validation set\n",
    "            model.eval()\n",
    "\n",
    "            # Tracking variables \n",
    "            eval_loss, eval_accuracy = 0, 0\n",
    "            nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "                # Add batch to GPU\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                # Unpack the inputs from our dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "                with torch.no_grad():\n",
    "                    # Forward pass, calculate logit predictions\n",
    "                    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "                tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "                nb_eval_steps += 1\n",
    "\n",
    "            print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "            vis.plot('accuracy', 'val_acc', 'val_acc',step,eval_accuracy/nb_eval_steps)\n",
    "            torch.save(model, 'random_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.comments.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df2.comments.values.shape[0]):\n",
    "    df2.comments.values[i] = df2.comments.values[i][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df2.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 512)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "# prediction_labels = torch.tensor(labels)\n",
    "  \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(prediction_dataloader,desc='batch',leave=False):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7164"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(main_preds==df.category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump( predictions, open( \"bert-preds-train.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pickle.load( open( \"bert-preds-train.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[1][30].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "main_preds = []\n",
    "for i in range(len(predictions)):\n",
    "    main_preds += list(predictions[i])\n",
    "    \n",
    "print(len(main_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "main_preds = []\n",
    "for i in range(len(predictions)):\n",
    "    main_preds += list(np.argmax(predictions[i], axis=1))\n",
    "    \n",
    "print(len(main_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 12,\n",
       " 14,\n",
       " 3,\n",
       " 9,\n",
       " 13,\n",
       " 13,\n",
       " 16,\n",
       " 11,\n",
       " 3,\n",
       " 16,\n",
       " 17,\n",
       " 8,\n",
       " 3,\n",
       " 17,\n",
       " 2,\n",
       " 15,\n",
       " 17,\n",
       " 17,\n",
       " 12,\n",
       " 0,\n",
       " 16,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 10,\n",
       " 4,\n",
       " 15,\n",
       " 8,\n",
       " 11,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 10,\n",
       " 19,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 18,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 6,\n",
       " 8,\n",
       " 12,\n",
       " 0,\n",
       " 13,\n",
       " 6,\n",
       " 11,\n",
       " 16,\n",
       " 17,\n",
       " 2,\n",
       " 12,\n",
       " 1,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 12,\n",
       " 9,\n",
       " 19,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 7,\n",
       " 0,\n",
       " 14,\n",
       " 11,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 19,\n",
       " 13,\n",
       " 2,\n",
       " 4,\n",
       " 14,\n",
       " 12,\n",
       " 17,\n",
       " 12,\n",
       " 5,\n",
       " 11,\n",
       " 9,\n",
       " 17,\n",
       " 2,\n",
       " 13,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 18,\n",
       " 13,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 4,\n",
       " 19,\n",
       " 18,\n",
       " 14,\n",
       " 19,\n",
       " 10,\n",
       " 16,\n",
       " 14,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 19,\n",
       " 1,\n",
       " 14,\n",
       " 18,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 9,\n",
       " 6,\n",
       " 17,\n",
       " 6,\n",
       " 18,\n",
       " 13,\n",
       " 16,\n",
       " 17,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 17,\n",
       " 18,\n",
       " 3,\n",
       " 17,\n",
       " 10,\n",
       " 18,\n",
       " 18,\n",
       " 7,\n",
       " 12,\n",
       " 2,\n",
       " 1,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 17,\n",
       " 5,\n",
       " 18,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 10,\n",
       " 15,\n",
       " 11,\n",
       " 6,\n",
       " 13,\n",
       " 18,\n",
       " 18,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 17,\n",
       " 14,\n",
       " 19,\n",
       " 4,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 0,\n",
       " 17,\n",
       " 7,\n",
       " 2,\n",
       " 15,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 8,\n",
       " 12,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 9,\n",
       " 19,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 16,\n",
       " 12,\n",
       " 18,\n",
       " 2,\n",
       " 7,\n",
       " 17,\n",
       " 2,\n",
       " 19,\n",
       " 8,\n",
       " 0,\n",
       " 15,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 19,\n",
       " 2,\n",
       " 7,\n",
       " 19,\n",
       " 17,\n",
       " 13,\n",
       " 17,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 12,\n",
       " 16,\n",
       " 18,\n",
       " 15,\n",
       " 5,\n",
       " 13,\n",
       " 17,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 18,\n",
       " 19,\n",
       " 17,\n",
       " 10,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 16,\n",
       " 11,\n",
       " 19,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 4,\n",
       " 4,\n",
       " 16,\n",
       " 12,\n",
       " 11,\n",
       " 6,\n",
       " 10,\n",
       " 17,\n",
       " 13,\n",
       " 0,\n",
       " 19,\n",
       " 8,\n",
       " 5,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 17,\n",
       " 6,\n",
       " 18,\n",
       " 9,\n",
       " 10,\n",
       " 17,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 16,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 17,\n",
       " 13,\n",
       " 13,\n",
       " 8,\n",
       " 16,\n",
       " 3,\n",
       " 13,\n",
       " 18,\n",
       " 19,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 14,\n",
       " 12,\n",
       " 16,\n",
       " 10,\n",
       " 7,\n",
       " 17,\n",
       " 16,\n",
       " 2,\n",
       " 17,\n",
       " 0,\n",
       " 17,\n",
       " 19,\n",
       " 19,\n",
       " 6,\n",
       " 18,\n",
       " 3,\n",
       " 12,\n",
       " 6,\n",
       " 19,\n",
       " 17,\n",
       " 8,\n",
       " 15,\n",
       " 10,\n",
       " 14,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 6,\n",
       " 11,\n",
       " 9,\n",
       " 12,\n",
       " 7,\n",
       " 1,\n",
       " 17,\n",
       " 10,\n",
       " 4,\n",
       " 17,\n",
       " 4,\n",
       " 16,\n",
       " 0,\n",
       " 7,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 3,\n",
       " 13,\n",
       " 0,\n",
       " 10,\n",
       " 3,\n",
       " 18,\n",
       " 5,\n",
       " 16,\n",
       " 11,\n",
       " 19,\n",
       " 6,\n",
       " 18,\n",
       " 3,\n",
       " 10,\n",
       " 2,\n",
       " 17,\n",
       " 17,\n",
       " 6,\n",
       " 17,\n",
       " 15,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 18,\n",
       " 15,\n",
       " 18,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 16,\n",
       " 8,\n",
       " 15,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 10,\n",
       " 16,\n",
       " 9,\n",
       " 16,\n",
       " 18,\n",
       " 2,\n",
       " 8,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 7,\n",
       " 12,\n",
       " 0,\n",
       " 19,\n",
       " 6,\n",
       " 3,\n",
       " 14,\n",
       " 7,\n",
       " 3,\n",
       " 15,\n",
       " 17,\n",
       " 14,\n",
       " 16,\n",
       " 14,\n",
       " 18,\n",
       " 16,\n",
       " 13,\n",
       " 11,\n",
       " 15,\n",
       " 17,\n",
       " 13,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 17,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 17,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 15,\n",
       " 2,\n",
       " 5,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 19,\n",
       " 19,\n",
       " 0,\n",
       " 2,\n",
       " 14,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 12,\n",
       " 17,\n",
       " 15,\n",
       " 12,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 16,\n",
       " 10,\n",
       " 17,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 0,\n",
       " 15,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 8,\n",
       " 17,\n",
       " 2,\n",
       " 17,\n",
       " 8,\n",
       " 13,\n",
       " 19,\n",
       " 17,\n",
       " 16,\n",
       " 2,\n",
       " 7,\n",
       " 15,\n",
       " 1,\n",
       " 2,\n",
       " 13,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 19,\n",
       " 6,\n",
       " 11,\n",
       " 11,\n",
       " 13,\n",
       " 4,\n",
       " 17,\n",
       " 7,\n",
       " 11,\n",
       " 13,\n",
       " 11,\n",
       " 19,\n",
       " 2,\n",
       " 3,\n",
       " 13,\n",
       " 16,\n",
       " 11,\n",
       " 17,\n",
       " 11,\n",
       " 11,\n",
       " 2,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 13,\n",
       " 9,\n",
       " 5,\n",
       " 14,\n",
       " 13,\n",
       " 15,\n",
       " 6,\n",
       " 13,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 15,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 17,\n",
       " 3,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 18,\n",
       " 17,\n",
       " 8,\n",
       " 17,\n",
       " 7,\n",
       " 13,\n",
       " 6,\n",
       " 8,\n",
       " 17,\n",
       " 8,\n",
       " 0,\n",
       " 11,\n",
       " 15,\n",
       " 17,\n",
       " 4,\n",
       " 5,\n",
       " 18,\n",
       " 11,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 10,\n",
       " 17,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 17,\n",
       " 7,\n",
       " 12,\n",
       " 10,\n",
       " 16,\n",
       " 13,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 15,\n",
       " 17,\n",
       " 13,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 19,\n",
       " 17,\n",
       " 18,\n",
       " 17,\n",
       " 13,\n",
       " 12,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 18,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 17,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 14,\n",
       " 18,\n",
       " 3,\n",
       " 11,\n",
       " 2,\n",
       " 16,\n",
       " 17,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 15,\n",
       " 2,\n",
       " 17,\n",
       " 4,\n",
       " 16,\n",
       " 7,\n",
       " 17,\n",
       " 14,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 10,\n",
       " 11,\n",
       " 3,\n",
       " 4,\n",
       " 17,\n",
       " 18,\n",
       " 3,\n",
       " 18,\n",
       " 2,\n",
       " 13,\n",
       " 13,\n",
       " 0,\n",
       " 11,\n",
       " 17,\n",
       " 4,\n",
       " 13,\n",
       " 10,\n",
       " 13,\n",
       " 2,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 2,\n",
       " 19,\n",
       " 7,\n",
       " 16,\n",
       " 11,\n",
       " 11,\n",
       " 18,\n",
       " 13,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 0,\n",
       " 12,\n",
       " 13,\n",
       " 16,\n",
       " 19,\n",
       " 4,\n",
       " 15,\n",
       " 7,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 19,\n",
       " 1,\n",
       " 10,\n",
       " 6,\n",
       " 2,\n",
       " 15,\n",
       " 1,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 16,\n",
       " 8,\n",
       " 9,\n",
       " 19,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 13,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 18,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 15,\n",
       " 14,\n",
       " 12,\n",
       " 19,\n",
       " 5,\n",
       " 7,\n",
       " 18,\n",
       " 15,\n",
       " 8,\n",
       " 17,\n",
       " 7,\n",
       " 17,\n",
       " 4,\n",
       " 4,\n",
       " 17,\n",
       " 9,\n",
       " 19,\n",
       " 6,\n",
       " 4,\n",
       " 11,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 19,\n",
       " 2,\n",
       " 19,\n",
       " 17,\n",
       " 12,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 17,\n",
       " 15,\n",
       " 7,\n",
       " 0,\n",
       " 16,\n",
       " 2,\n",
       " 0,\n",
       " 19,\n",
       " 17,\n",
       " 6,\n",
       " 18,\n",
       " 11,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 14,\n",
       " 10,\n",
       " 18,\n",
       " 7,\n",
       " 15,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 19,\n",
       " 8,\n",
       " 13,\n",
       " 19,\n",
       " 12,\n",
       " 14,\n",
       " 17,\n",
       " 10,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 11,\n",
       " 16,\n",
       " 4,\n",
       " 19,\n",
       " 11,\n",
       " 18,\n",
       " 3,\n",
       " 0,\n",
       " 14,\n",
       " 19,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 13,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 11,\n",
       " 1,\n",
       " 9,\n",
       " 15,\n",
       " 19,\n",
       " 16,\n",
       " 12,\n",
       " 16,\n",
       " 4,\n",
       " 19,\n",
       " 15,\n",
       " 16,\n",
       " 3,\n",
       " 2,\n",
       " 17,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 12,\n",
       " 18,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 17,\n",
       " 16,\n",
       " 11,\n",
       " 15,\n",
       " 4,\n",
       " 7,\n",
       " 16,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 15,\n",
       " 17,\n",
       " 12,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 2,\n",
       " 8,\n",
       " 18,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 19,\n",
       " 0,\n",
       " 12,\n",
       " 18,\n",
       " 5,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 4,\n",
       " 19,\n",
       " 16,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 16,\n",
       " 6,\n",
       " 19,\n",
       " 12,\n",
       " 16,\n",
       " 6,\n",
       " 17,\n",
       " 14,\n",
       " 9,\n",
       " 5,\n",
       " 16,\n",
       " 10,\n",
       " 6,\n",
       " 15,\n",
       " 17,\n",
       " 19,\n",
       " 14,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 18,\n",
       " 13,\n",
       " 2,\n",
       " 16,\n",
       " 17,\n",
       " 7,\n",
       " 11,\n",
       " 13,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 13,\n",
       " 8,\n",
       " 17,\n",
       " 13,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 10,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 12,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 12,\n",
       " 7,\n",
       " 19,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 16,\n",
       " 5,\n",
       " 17,\n",
       " 10,\n",
       " 5,\n",
       " 10,\n",
       " 16,\n",
       " 6,\n",
       " 11,\n",
       " 19,\n",
       " 15,\n",
       " 18,\n",
       " 17,\n",
       " 9,\n",
       " 2,\n",
       " 19,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 17,\n",
       " 9,\n",
       " 2,\n",
       " 18,\n",
       " 2,\n",
       " 6,\n",
       " 11,\n",
       " 3,\n",
       " 17,\n",
       " 0,\n",
       " 13,\n",
       " 11,\n",
       " 8,\n",
       " 18,\n",
       " 7,\n",
       " 4,\n",
       " 19,\n",
       " 8,\n",
       " 17,\n",
       " 14,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pd.DataFrame()\n",
    "test_preds['Id'] = df2['id']\n",
    "test_preds['Category'] = mapping[main_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1e34ed016a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_preds' is not defined"
     ]
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='test2.csv' target='_blank'>test2.csv</a><br>"
      ],
      "text/plain": [
       "/network/home/penmetss/comp551/test2.csv"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.to_csv(\"test2.csv\", index=False)\n",
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(a.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('models/main_model_v2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
