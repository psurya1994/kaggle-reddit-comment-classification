{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "\n",
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')\n",
    "            \n",
    "            \n",
    "    \n",
    "vis = VisdomLinePlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import AdamW, WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6670</td>\n",
       "      <td>6670</td>\n",
       "      <td>Yeah but euron's about to bring cersei tyrion ...</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49567</td>\n",
       "      <td>49567</td>\n",
       "      <td>All of his videos are sarcastic and funny...hi...</td>\n",
       "      <td>conspiracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50796</td>\n",
       "      <td>50796</td>\n",
       "      <td>I love those scenes but it wouldn't have made ...</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22310</td>\n",
       "      <td>22310</td>\n",
       "      <td>You do get a smidge of hp for every point of c...</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54037</td>\n",
       "      <td>54037</td>\n",
       "      <td>New MMORPG lets you play as someone playing a ...</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           comments     subreddits\n",
       "6670    6670  Yeah but euron's about to bring cersei tyrion ...  gameofthrones\n",
       "49567  49567  All of his videos are sarcastic and funny...hi...     conspiracy\n",
       "50796  50796  I love those scenes but it wouldn't have made ...         movies\n",
       "22310  22310  You do get a smidge of hp for every point of c...            wow\n",
       "54037  54037  New MMORPG lets you play as someone playing a ...            wow"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_train.csv')\n",
    "df2 = pd.read_csv('reddit_test.csv')\n",
    "df = df.sample(1000, random_state=1).copy()\n",
    "df2 = df2.sample(1000, random_state=1).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'], mapping = df['subreddits'].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.comments.values.shape[0]):\n",
    "    df.comments.values[i] = df.comments.values[i][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.category_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'yeah', 'but', 'euro', '##n', \"'\", 's', 'about', 'to', 'bring', 'ce', '##rse', '##i', 'ty', '##rion', 'as', 'a', 'gift', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.05)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 2\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=20)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta', 'Layer.Norm']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "# optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                      lr=2e-5,\n",
    "#                      warmup=.1)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=0, t_total=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "batch:   0%|          | 0/475 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   0%|          | 1/475 [00:00<03:08,  2.52it/s]\u001b[A\n",
      "batch:   0%|          | 2/475 [00:00<03:00,  2.61it/s]\u001b[A\n",
      "batch:   1%|          | 3/475 [00:01<02:54,  2.70it/s]\u001b[A\n",
      "batch:   1%|          | 4/475 [00:01<02:48,  2.79it/s]\u001b[A\n",
      "batch:   1%|          | 5/475 [00:01<02:45,  2.84it/s]\u001b[A\n",
      "batch:   1%|▏         | 6/475 [00:02<02:42,  2.89it/s]\u001b[A\n",
      "batch:   1%|▏         | 7/475 [00:02<02:40,  2.91it/s]\u001b[A\n",
      "batch:   2%|▏         | 8/475 [00:02<02:39,  2.94it/s]\u001b[A\n",
      "batch:   2%|▏         | 9/475 [00:03<02:37,  2.95it/s]\u001b[A\n",
      "batch:   2%|▏         | 10/475 [00:03<02:35,  2.99it/s]\u001b[A\n",
      "batch:   2%|▏         | 11/475 [00:03<02:37,  2.95it/s]\u001b[A\n",
      "batch:   3%|▎         | 12/475 [00:04<02:36,  2.95it/s]\u001b[A\n",
      "batch:   3%|▎         | 13/475 [00:04<02:35,  2.97it/s]\u001b[A\n",
      "batch:   3%|▎         | 14/475 [00:04<02:35,  2.97it/s]\u001b[A\n",
      "batch:   3%|▎         | 15/475 [00:05<02:35,  2.97it/s]\u001b[A\n",
      "batch:   3%|▎         | 16/475 [00:05<02:33,  2.99it/s]\u001b[A\n",
      "batch:   4%|▎         | 17/475 [00:05<02:31,  3.02it/s]\u001b[A\n",
      "batch:   4%|▍         | 18/475 [00:06<02:30,  3.03it/s]\u001b[A\n",
      "batch:   4%|▍         | 19/475 [00:06<02:29,  3.05it/s]\u001b[A\n",
      "batch:   4%|▍         | 20/475 [00:06<02:32,  2.99it/s]\u001b[A\n",
      "batch:   4%|▍         | 21/475 [00:07<02:34,  2.93it/s]\u001b[A\n",
      "batch:   5%|▍         | 22/475 [00:07<02:34,  2.93it/s]\u001b[A\n",
      "batch:   5%|▍         | 23/475 [00:07<02:33,  2.95it/s]\u001b[A\n",
      "batch:   5%|▌         | 24/475 [00:08<02:32,  2.96it/s]\u001b[A\n",
      "batch:   5%|▌         | 25/475 [00:08<02:31,  2.96it/s]\u001b[A\n",
      "batch:   5%|▌         | 26/475 [00:08<02:31,  2.97it/s]\u001b[A\n",
      "batch:   6%|▌         | 27/475 [00:09<02:31,  2.96it/s]\u001b[A\n",
      "batch:   6%|▌         | 28/475 [00:09<02:30,  2.97it/s]\u001b[A\n",
      "batch:   6%|▌         | 29/475 [00:09<02:30,  2.96it/s]\u001b[A\n",
      "batch:   6%|▋         | 30/475 [00:10<02:30,  2.96it/s]\u001b[A\n",
      "batch:   7%|▋         | 31/475 [00:10<02:29,  2.98it/s]\u001b[A\n",
      "batch:   7%|▋         | 32/475 [00:10<02:28,  2.98it/s]\u001b[A\n",
      "batch:   7%|▋         | 33/475 [00:11<02:28,  2.97it/s]\u001b[A\n",
      "batch:   7%|▋         | 34/475 [00:11<02:27,  2.98it/s]\u001b[A\n",
      "batch:   7%|▋         | 35/475 [00:11<02:26,  3.00it/s]\u001b[A\n",
      "batch:   8%|▊         | 36/475 [00:12<02:26,  3.00it/s]\u001b[A\n",
      "batch:   8%|▊         | 37/475 [00:12<02:26,  2.99it/s]\u001b[A\n",
      "batch:   8%|▊         | 38/475 [00:12<02:25,  3.00it/s]\u001b[A\n",
      "batch:   8%|▊         | 39/475 [00:13<02:26,  2.98it/s]\u001b[A\n",
      "batch:   8%|▊         | 40/475 [00:13<02:25,  2.99it/s]\u001b[A\n",
      "batch:   9%|▊         | 41/475 [00:13<02:24,  3.01it/s]\u001b[A\n",
      "batch:   9%|▉         | 42/475 [00:14<02:23,  3.02it/s]\u001b[A\n",
      "batch:   9%|▉         | 43/475 [00:14<02:22,  3.03it/s]\u001b[A\n",
      "batch:   9%|▉         | 44/475 [00:14<02:22,  3.03it/s]\u001b[A\n",
      "batch:   9%|▉         | 45/475 [00:15<02:21,  3.03it/s]\u001b[A\n",
      "batch:  10%|▉         | 46/475 [00:15<02:20,  3.04it/s]\u001b[A\n",
      "batch:  10%|▉         | 47/475 [00:15<02:20,  3.05it/s]\u001b[A\n",
      "batch:  10%|█         | 48/475 [00:16<02:21,  3.02it/s]\u001b[A\n",
      "batch:  10%|█         | 49/475 [00:16<02:20,  3.02it/s]\u001b[A\n",
      "batch:  11%|█         | 50/475 [00:16<02:21,  3.00it/s]\u001b[A\n",
      "batch:  11%|█         | 51/475 [00:17<02:22,  2.98it/s]\u001b[A\n",
      "batch:  11%|█         | 52/475 [00:17<02:22,  2.97it/s]\u001b[A\n",
      "batch:  11%|█         | 53/475 [00:17<02:22,  2.96it/s]\u001b[A\n",
      "batch:  11%|█▏        | 54/475 [00:18<02:21,  2.97it/s]\u001b[A\n",
      "batch:  12%|█▏        | 55/475 [00:18<02:21,  2.98it/s]\u001b[A\n",
      "batch:  12%|█▏        | 56/475 [00:18<02:20,  2.99it/s]\u001b[A\n",
      "batch:  12%|█▏        | 57/475 [00:19<02:21,  2.96it/s]\u001b[A\n",
      "batch:  12%|█▏        | 58/475 [00:19<02:20,  2.97it/s]\u001b[A\n",
      "batch:  12%|█▏        | 59/475 [00:19<02:19,  2.98it/s]\u001b[A\n",
      "batch:  13%|█▎        | 60/475 [00:20<02:18,  2.99it/s]\u001b[A\n",
      "batch:  13%|█▎        | 61/475 [00:20<02:17,  3.02it/s]\u001b[A\n",
      "batch:  13%|█▎        | 62/475 [00:20<02:15,  3.04it/s]\u001b[A\n",
      "batch:  13%|█▎        | 63/475 [00:21<02:14,  3.06it/s]\u001b[A\n",
      "batch:  13%|█▎        | 64/475 [00:21<02:14,  3.06it/s]\u001b[A\n",
      "batch:  14%|█▎        | 65/475 [00:21<02:14,  3.06it/s]\u001b[A\n",
      "batch:  14%|█▍        | 66/475 [00:22<02:13,  3.06it/s]\u001b[A\n",
      "batch:  14%|█▍        | 67/475 [00:22<02:13,  3.07it/s]\u001b[A\n",
      "batch:  14%|█▍        | 68/475 [00:22<02:14,  3.04it/s]\u001b[A\n",
      "batch:  15%|█▍        | 69/475 [00:23<02:15,  3.00it/s]\u001b[A\n",
      "batch:  15%|█▍        | 70/475 [00:23<02:14,  3.01it/s]\u001b[A\n",
      "batch:  15%|█▍        | 71/475 [00:23<02:15,  2.99it/s]\u001b[A\n",
      "batch:  15%|█▌        | 72/475 [00:24<02:15,  2.96it/s]\u001b[A\n",
      "batch:  15%|█▌        | 73/475 [00:24<02:14,  2.99it/s]\u001b[A\n",
      "batch:  16%|█▌        | 74/475 [00:24<02:14,  2.99it/s]\u001b[A\n",
      "batch:  16%|█▌        | 75/475 [00:25<02:15,  2.95it/s]\u001b[A\n",
      "batch:  16%|█▌        | 76/475 [00:25<02:15,  2.95it/s]\u001b[A\n",
      "batch:  16%|█▌        | 77/475 [00:25<02:14,  2.96it/s]\u001b[A\n",
      "batch:  16%|█▋        | 78/475 [00:26<02:14,  2.95it/s]\u001b[A\n",
      "batch:  17%|█▋        | 79/475 [00:26<02:12,  2.98it/s]\u001b[A\n",
      "batch:  17%|█▋        | 80/475 [00:26<02:11,  3.00it/s]\u001b[A\n",
      "batch:  17%|█▋        | 81/475 [00:27<02:10,  3.02it/s]\u001b[A\n",
      "batch:  17%|█▋        | 82/475 [00:27<02:11,  3.00it/s]\u001b[A\n",
      "batch:  17%|█▋        | 83/475 [00:27<02:11,  2.97it/s]\u001b[A\n",
      "batch:  18%|█▊        | 84/475 [00:28<02:13,  2.94it/s]\u001b[A\n",
      "batch:  18%|█▊        | 85/475 [00:28<02:12,  2.94it/s]\u001b[A\n",
      "batch:  18%|█▊        | 86/475 [00:28<02:12,  2.93it/s]\u001b[A\n",
      "batch:  18%|█▊        | 87/475 [00:29<02:10,  2.97it/s]\u001b[A\n",
      "batch:  19%|█▊        | 88/475 [00:29<02:09,  3.00it/s]\u001b[A\n",
      "batch:  19%|█▊        | 89/475 [00:29<02:08,  3.01it/s]\u001b[A\n",
      "batch:  19%|█▉        | 90/475 [00:30<02:07,  3.01it/s]\u001b[A\n",
      "batch:  19%|█▉        | 91/475 [00:30<02:06,  3.03it/s]\u001b[A\n",
      "batch:  19%|█▉        | 92/475 [00:30<02:05,  3.04it/s]\u001b[A\n",
      "batch:  20%|█▉        | 93/475 [00:31<02:06,  3.01it/s]\u001b[A\n",
      "batch:  20%|█▉        | 94/475 [00:31<02:07,  2.98it/s]\u001b[A\n",
      "batch:  20%|██        | 95/475 [00:31<02:07,  2.97it/s]\u001b[A\n",
      "batch:  20%|██        | 96/475 [00:32<02:06,  2.99it/s]\u001b[A\n",
      "batch:  20%|██        | 97/475 [00:32<02:06,  3.00it/s]\u001b[A\n",
      "batch:  21%|██        | 98/475 [00:32<02:04,  3.02it/s]\u001b[A\n",
      "batch:  21%|██        | 99/475 [00:33<02:06,  2.98it/s]\u001b[A\n",
      "batch:  21%|██        | 100/475 [00:33<02:05,  2.98it/s]\u001b[A\n",
      "batch:  21%|██▏       | 101/475 [00:33<02:05,  2.97it/s]\u001b[A\n",
      "batch:  21%|██▏       | 102/475 [00:34<02:05,  2.96it/s]\u001b[A\n",
      "batch:  22%|██▏       | 103/475 [00:34<02:06,  2.95it/s]\u001b[A\n",
      "batch:  22%|██▏       | 104/475 [00:34<02:05,  2.94it/s]\u001b[A\n",
      "batch:  22%|██▏       | 105/475 [00:35<02:04,  2.97it/s]\u001b[A\n",
      "batch:  22%|██▏       | 106/475 [00:35<02:04,  2.95it/s]\u001b[A\n",
      "batch:  23%|██▎       | 107/475 [00:35<02:04,  2.95it/s]\u001b[A\n",
      "batch:  23%|██▎       | 108/475 [00:36<02:04,  2.95it/s]\u001b[A\n",
      "batch:  23%|██▎       | 109/475 [00:36<02:03,  2.96it/s]\u001b[A\n",
      "batch:  23%|██▎       | 110/475 [00:36<02:02,  2.99it/s]\u001b[A\n",
      "batch:  23%|██▎       | 111/475 [00:37<02:01,  2.99it/s]\u001b[A\n",
      "batch:  24%|██▎       | 112/475 [00:37<02:01,  2.99it/s]\u001b[A\n",
      "batch:  24%|██▍       | 113/475 [00:37<02:01,  2.98it/s]\u001b[A\n",
      "batch:  24%|██▍       | 114/475 [00:38<02:01,  2.98it/s]\u001b[A\n",
      "batch:  24%|██▍       | 115/475 [00:38<02:01,  2.96it/s]\u001b[A\n",
      "batch:  24%|██▍       | 116/475 [00:38<02:02,  2.94it/s]\u001b[A\n",
      "batch:  25%|██▍       | 117/475 [00:39<02:03,  2.90it/s]\u001b[A\n",
      "batch:  25%|██▍       | 118/475 [00:39<02:02,  2.92it/s]\u001b[A\n",
      "batch:  25%|██▌       | 119/475 [00:39<02:01,  2.94it/s]\u001b[A\n",
      "batch:  25%|██▌       | 120/475 [00:40<02:00,  2.95it/s]\u001b[A\n",
      "batch:  25%|██▌       | 121/475 [00:40<01:59,  2.95it/s]\u001b[A\n",
      "batch:  26%|██▌       | 122/475 [00:40<01:59,  2.96it/s]\u001b[A\n",
      "batch:  26%|██▌       | 123/475 [00:41<02:01,  2.91it/s]\u001b[A\n",
      "batch:  26%|██▌       | 124/475 [00:41<01:59,  2.93it/s]\u001b[A\n",
      "batch:  26%|██▋       | 125/475 [00:41<01:58,  2.95it/s]\u001b[A\n",
      "batch:  27%|██▋       | 126/475 [00:42<01:57,  2.96it/s]\u001b[A\n",
      "batch:  27%|██▋       | 127/475 [00:42<01:57,  2.97it/s]\u001b[A\n",
      "batch:  27%|██▋       | 128/475 [00:42<01:56,  2.97it/s]\u001b[A\n",
      "batch:  27%|██▋       | 129/475 [00:43<01:58,  2.92it/s]\u001b[A\n",
      "batch:  27%|██▋       | 130/475 [00:43<01:58,  2.92it/s]\u001b[A\n",
      "batch:  28%|██▊       | 131/475 [00:44<01:57,  2.94it/s]\u001b[A\n",
      "batch:  28%|██▊       | 132/475 [00:44<01:56,  2.95it/s]\u001b[A\n",
      "batch:  28%|██▊       | 133/475 [00:44<02:09,  2.64it/s]\u001b[A\n",
      "batch:  28%|██▊       | 134/475 [00:45<02:10,  2.61it/s]\u001b[A\n",
      "batch:  28%|██▊       | 135/475 [00:45<02:08,  2.64it/s]\u001b[A\n",
      "batch:  29%|██▊       | 136/475 [00:45<02:05,  2.70it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  29%|██▉       | 137/475 [00:46<02:02,  2.76it/s]\u001b[A\n",
      "batch:  29%|██▉       | 138/475 [00:46<02:00,  2.80it/s]\u001b[A\n",
      "batch:  29%|██▉       | 139/475 [00:46<01:59,  2.81it/s]\u001b[A\n",
      "batch:  29%|██▉       | 140/475 [00:47<01:57,  2.86it/s]\u001b[A\n",
      "batch:  30%|██▉       | 141/475 [00:47<01:59,  2.80it/s]\u001b[A\n",
      "batch:  30%|██▉       | 142/475 [00:48<02:00,  2.75it/s]\u001b[A\n",
      "batch:  30%|███       | 143/475 [00:48<02:00,  2.76it/s]\u001b[A\n",
      "batch:  30%|███       | 144/475 [00:48<01:59,  2.77it/s]\u001b[A\n",
      "batch:  31%|███       | 145/475 [00:49<01:58,  2.78it/s]\u001b[A\n",
      "batch:  31%|███       | 146/475 [00:49<01:58,  2.78it/s]\u001b[A\n",
      "batch:  31%|███       | 147/475 [00:49<01:56,  2.82it/s]\u001b[A\n",
      "batch:  31%|███       | 148/475 [00:50<01:55,  2.83it/s]\u001b[A\n",
      "batch:  31%|███▏      | 149/475 [00:50<01:54,  2.84it/s]\u001b[A\n",
      "batch:  32%|███▏      | 150/475 [00:50<01:53,  2.86it/s]\u001b[A\n",
      "batch:  32%|███▏      | 151/475 [00:51<01:51,  2.89it/s]\u001b[A\n",
      "batch:  32%|███▏      | 152/475 [00:51<01:54,  2.82it/s]\u001b[A\n",
      "batch:  32%|███▏      | 153/475 [00:51<01:56,  2.77it/s]\u001b[A\n",
      "batch:  32%|███▏      | 154/475 [00:52<01:56,  2.75it/s]\u001b[A\n",
      "batch:  33%|███▎      | 155/475 [00:52<01:55,  2.76it/s]\u001b[A\n",
      "batch:  33%|███▎      | 156/475 [00:53<01:54,  2.79it/s]\u001b[A\n",
      "batch:  33%|███▎      | 157/475 [00:53<01:52,  2.82it/s]\u001b[A\n",
      "batch:  33%|███▎      | 158/475 [00:53<01:51,  2.85it/s]\u001b[A\n",
      "batch:  33%|███▎      | 159/475 [00:54<01:49,  2.89it/s]\u001b[A\n",
      "batch:  34%|███▎      | 160/475 [00:54<01:48,  2.92it/s]\u001b[A\n",
      "batch:  34%|███▍      | 161/475 [00:54<01:48,  2.90it/s]\u001b[A\n",
      "batch:  34%|███▍      | 162/475 [00:55<01:46,  2.93it/s]\u001b[A\n",
      "batch:  34%|███▍      | 163/475 [00:55<01:45,  2.95it/s]\u001b[A\n",
      "batch:  35%|███▍      | 164/475 [00:55<01:45,  2.96it/s]\u001b[A\n",
      "batch:  35%|███▍      | 165/475 [00:56<01:46,  2.92it/s]\u001b[A\n",
      "batch:  35%|███▍      | 166/475 [00:56<01:44,  2.95it/s]\u001b[A\n",
      "batch:  35%|███▌      | 167/475 [00:56<01:49,  2.80it/s]\u001b[A\n",
      "batch:  35%|███▌      | 168/475 [00:57<01:53,  2.72it/s]\u001b[A\n",
      "batch:  36%|███▌      | 169/475 [00:57<01:54,  2.67it/s]\u001b[A\n",
      "batch:  36%|███▌      | 170/475 [00:57<01:53,  2.69it/s]\u001b[A\n",
      "batch:  36%|███▌      | 171/475 [00:58<01:52,  2.70it/s]\u001b[A\n",
      "batch:  36%|███▌      | 172/475 [00:58<01:50,  2.74it/s]\u001b[A\n",
      "batch:  36%|███▋      | 173/475 [00:59<01:49,  2.76it/s]\u001b[A\n",
      "batch:  37%|███▋      | 174/475 [00:59<01:47,  2.80it/s]\u001b[A\n",
      "batch:  37%|███▋      | 175/475 [00:59<01:56,  2.58it/s]\u001b[A\n",
      "batch:  37%|███▋      | 176/475 [01:00<01:56,  2.57it/s]\u001b[A\n",
      "batch:  37%|███▋      | 177/475 [01:00<01:54,  2.60it/s]\u001b[A\n",
      "batch:  37%|███▋      | 178/475 [01:01<01:53,  2.62it/s]\u001b[A\n",
      "batch:  38%|███▊      | 179/475 [01:01<01:51,  2.66it/s]\u001b[A\n",
      "batch:  38%|███▊      | 180/475 [01:01<01:48,  2.71it/s]\u001b[A\n",
      "batch:  38%|███▊      | 181/475 [01:02<01:46,  2.76it/s]\u001b[A\n",
      "batch:  38%|███▊      | 182/475 [01:02<01:44,  2.81it/s]\u001b[A\n",
      "batch:  39%|███▊      | 183/475 [01:02<01:42,  2.84it/s]\u001b[A\n",
      "batch:  39%|███▊      | 184/475 [01:03<01:41,  2.87it/s]\u001b[A\n",
      "batch:  39%|███▉      | 185/475 [01:03<01:48,  2.68it/s]\u001b[A\n",
      "batch:  39%|███▉      | 186/475 [01:03<01:49,  2.65it/s]\u001b[A\n",
      "batch:  39%|███▉      | 187/475 [01:04<01:48,  2.65it/s]\u001b[A\n",
      "batch:  40%|███▉      | 188/475 [01:04<01:47,  2.68it/s]\u001b[A\n",
      "batch:  40%|███▉      | 189/475 [01:05<01:45,  2.70it/s]\u001b[A\n",
      "batch:  40%|████      | 190/475 [01:05<01:43,  2.74it/s]\u001b[A\n",
      "batch:  40%|████      | 191/475 [01:05<01:42,  2.78it/s]\u001b[A\n",
      "batch:  40%|████      | 192/475 [01:06<01:40,  2.81it/s]\u001b[A\n",
      "batch:  41%|████      | 193/475 [01:06<01:50,  2.54it/s]\u001b[A\n",
      "batch:  41%|████      | 194/475 [01:06<01:52,  2.50it/s]\u001b[A\n",
      "batch:  41%|████      | 195/475 [01:07<01:52,  2.49it/s]\u001b[A\n",
      "batch:  41%|████▏     | 196/475 [01:07<01:50,  2.53it/s]\u001b[A\n",
      "batch:  41%|████▏     | 197/475 [01:08<01:47,  2.58it/s]\u001b[A\n",
      "batch:  42%|████▏     | 198/475 [01:08<01:45,  2.64it/s]\u001b[A\n",
      "batch:  42%|████▏     | 199/475 [01:08<01:43,  2.66it/s]\u001b[A\n",
      "batch:  42%|████▏     | 200/475 [01:09<01:43,  2.65it/s]\u001b[A\n",
      "batch:  42%|████▏     | 201/475 [01:09<01:43,  2.65it/s]\u001b[A\n",
      "batch:  43%|████▎     | 202/475 [01:09<01:42,  2.67it/s]\u001b[A\n",
      "batch:  43%|████▎     | 203/475 [01:10<01:41,  2.69it/s]\u001b[A\n",
      "batch:  43%|████▎     | 204/475 [01:10<01:39,  2.73it/s]\u001b[A\n",
      "batch:  43%|████▎     | 205/475 [01:11<01:37,  2.77it/s]\u001b[A\n",
      "batch:  43%|████▎     | 206/475 [01:11<01:36,  2.80it/s]\u001b[A\n",
      "batch:  44%|████▎     | 207/475 [01:11<01:39,  2.69it/s]\u001b[A\n",
      "batch:  44%|████▍     | 208/475 [01:12<01:40,  2.65it/s]\u001b[A\n",
      "batch:  44%|████▍     | 209/475 [01:12<01:40,  2.64it/s]\u001b[A\n",
      "batch:  44%|████▍     | 210/475 [01:12<01:39,  2.66it/s]\u001b[A\n",
      "batch:  44%|████▍     | 211/475 [01:13<01:38,  2.67it/s]\u001b[A\n",
      "batch:  45%|████▍     | 212/475 [01:13<01:37,  2.70it/s]\u001b[A\n",
      "batch:  45%|████▍     | 213/475 [01:14<01:35,  2.74it/s]\u001b[A\n",
      "batch:  45%|████▌     | 214/475 [01:14<01:35,  2.75it/s]\u001b[A\n",
      "batch:  45%|████▌     | 215/475 [01:14<01:33,  2.78it/s]\u001b[A\n",
      "batch:  45%|████▌     | 216/475 [01:15<01:32,  2.80it/s]\u001b[A\n",
      "batch:  46%|████▌     | 217/475 [01:15<01:37,  2.65it/s]\u001b[A\n",
      "batch:  46%|████▌     | 218/475 [01:15<01:38,  2.60it/s]\u001b[A\n",
      "batch:  46%|████▌     | 219/475 [01:16<01:41,  2.52it/s]\u001b[A\n",
      "batch:  46%|████▋     | 220/475 [01:16<01:41,  2.52it/s]\u001b[A\n",
      "batch:  47%|████▋     | 221/475 [01:17<02:04,  2.03it/s]\u001b[A\n",
      "batch:  47%|████▋     | 222/475 [01:17<01:59,  2.12it/s]\u001b[A\n",
      "batch:  47%|████▋     | 223/475 [01:18<01:51,  2.26it/s]\u001b[A\n",
      "batch:  47%|████▋     | 224/475 [01:18<01:45,  2.37it/s]\u001b[A\n",
      "batch:  47%|████▋     | 225/475 [01:18<01:40,  2.48it/s]\u001b[A\n",
      "batch:  48%|████▊     | 226/475 [01:19<01:40,  2.47it/s]\u001b[A\n",
      "batch:  48%|████▊     | 227/475 [01:19<01:39,  2.50it/s]\u001b[A\n",
      "batch:  48%|████▊     | 228/475 [01:20<01:36,  2.55it/s]\u001b[A\n",
      "batch:  48%|████▊     | 229/475 [01:20<01:34,  2.61it/s]\u001b[A\n",
      "batch:  48%|████▊     | 230/475 [01:21<01:43,  2.37it/s]\u001b[A\n",
      "batch:  49%|████▊     | 231/475 [01:21<01:42,  2.37it/s]\u001b[A\n",
      "batch:  49%|████▉     | 232/475 [01:21<01:39,  2.43it/s]\u001b[A\n",
      "batch:  49%|████▉     | 233/475 [01:22<01:37,  2.49it/s]\u001b[A\n",
      "batch:  49%|████▉     | 234/475 [01:22<01:34,  2.54it/s]\u001b[A\n",
      "batch:  49%|████▉     | 235/475 [01:22<01:31,  2.61it/s]\u001b[A\n",
      "batch:  50%|████▉     | 236/475 [01:23<01:30,  2.64it/s]\u001b[A\n",
      "batch:  50%|████▉     | 237/475 [01:23<01:29,  2.67it/s]\u001b[A\n",
      "batch:  50%|█████     | 238/475 [01:24<01:28,  2.69it/s]\u001b[A\n",
      "batch:  50%|█████     | 239/475 [01:24<01:25,  2.76it/s]\u001b[A\n",
      "batch:  51%|█████     | 240/475 [01:24<01:24,  2.77it/s]\u001b[A\n",
      "batch:  51%|█████     | 241/475 [01:25<01:24,  2.79it/s]\u001b[A\n",
      "batch:  51%|█████     | 242/475 [01:25<01:22,  2.81it/s]\u001b[A\n",
      "batch:  51%|█████     | 243/475 [01:25<01:21,  2.86it/s]\u001b[A\n",
      "batch:  51%|█████▏    | 244/475 [01:26<01:19,  2.89it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 245/475 [01:26<01:19,  2.89it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 246/475 [01:26<01:27,  2.61it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 247/475 [01:27<01:32,  2.48it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 248/475 [01:27<01:31,  2.47it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 249/475 [01:28<01:30,  2.49it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 250/475 [01:28<01:33,  2.41it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 251/475 [01:29<01:32,  2.43it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 252/475 [01:29<01:30,  2.47it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 253/475 [01:29<01:27,  2.53it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 254/475 [01:30<01:24,  2.61it/s]\u001b[A\n",
      "batch:  54%|█████▎    | 255/475 [01:30<01:22,  2.67it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 256/475 [01:30<01:20,  2.74it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 257/475 [01:31<01:22,  2.63it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 258/475 [01:31<01:24,  2.58it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 259/475 [01:32<01:30,  2.40it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 260/475 [01:32<01:29,  2.41it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 261/475 [01:32<01:28,  2.43it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 262/475 [01:33<01:25,  2.49it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 263/475 [01:33<01:23,  2.54it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 264/475 [01:34<01:29,  2.36it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 265/475 [01:34<01:27,  2.39it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 266/475 [01:35<01:25,  2.45it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 267/475 [01:35<01:23,  2.48it/s]\u001b[A\n",
      "batch:  56%|█████▋    | 268/475 [01:35<01:21,  2.53it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 269/475 [01:36<01:24,  2.43it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 270/475 [01:36<01:23,  2.47it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 271/475 [01:36<01:20,  2.52it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 272/475 [01:37<01:19,  2.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  57%|█████▋    | 273/475 [01:37<01:17,  2.62it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 274/475 [01:38<01:16,  2.62it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 275/475 [01:38<01:14,  2.68it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 276/475 [01:38<01:13,  2.71it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 277/475 [01:39<01:12,  2.73it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 278/475 [01:39<01:11,  2.75it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 279/475 [01:39<01:10,  2.79it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 280/475 [01:40<01:09,  2.80it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 281/475 [01:40<01:08,  2.83it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 282/475 [01:41<01:12,  2.67it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 283/475 [01:41<01:14,  2.57it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 284/475 [01:41<01:15,  2.54it/s]\u001b[A\n",
      "batch:  60%|██████    | 285/475 [01:42<01:14,  2.55it/s]\u001b[A\n",
      "batch:  60%|██████    | 286/475 [01:42<01:15,  2.52it/s]\u001b[A\n",
      "batch:  60%|██████    | 287/475 [01:43<01:19,  2.37it/s]\u001b[A\n",
      "batch:  61%|██████    | 288/475 [01:43<01:18,  2.39it/s]\u001b[A\n",
      "batch:  61%|██████    | 289/475 [01:43<01:16,  2.43it/s]\u001b[A\n",
      "batch:  61%|██████    | 290/475 [01:44<01:14,  2.48it/s]\u001b[A\n",
      "batch:  61%|██████▏   | 291/475 [01:44<01:12,  2.54it/s]\u001b[A\n",
      "batch:  61%|██████▏   | 292/475 [01:45<01:15,  2.43it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 293/475 [01:45<01:14,  2.44it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 294/475 [01:45<01:12,  2.50it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 295/475 [01:46<01:14,  2.42it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 296/475 [01:46<01:13,  2.44it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 297/475 [01:47<01:12,  2.46it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 298/475 [01:47<01:10,  2.49it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 299/475 [01:47<01:09,  2.53it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 300/475 [01:48<01:07,  2.58it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 301/475 [01:48<01:06,  2.61it/s]\u001b[A\n",
      "batch:  64%|██████▎   | 302/475 [01:49<01:11,  2.43it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 303/475 [01:49<01:08,  2.52it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 304/475 [01:49<01:05,  2.60it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 305/475 [01:50<01:04,  2.65it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 306/475 [01:50<01:02,  2.69it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 307/475 [01:50<01:02,  2.71it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 308/475 [01:51<01:00,  2.75it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 309/475 [01:51<01:07,  2.48it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 310/475 [01:52<01:07,  2.44it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 311/475 [01:52<01:06,  2.46it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 312/475 [01:53<01:05,  2.49it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 313/475 [01:53<01:03,  2.54it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 314/475 [01:53<01:02,  2.56it/s]\u001b[A\n",
      "batch:  66%|██████▋   | 315/475 [01:54<01:01,  2.59it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 316/475 [01:54<01:01,  2.60it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 317/475 [01:54<00:59,  2.65it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 318/475 [01:55<00:57,  2.71it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 319/475 [01:55<00:56,  2.75it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 320/475 [01:55<00:55,  2.78it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 321/475 [01:56<00:56,  2.72it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 322/475 [01:56<00:55,  2.74it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 323/475 [01:57<00:54,  2.78it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 324/475 [01:57<00:54,  2.78it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 325/475 [01:57<00:54,  2.76it/s]\u001b[A\n",
      "batch:  69%|██████▊   | 326/475 [01:58<01:01,  2.44it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 327/475 [01:58<01:03,  2.33it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 328/475 [01:59<01:02,  2.36it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 329/475 [01:59<01:01,  2.37it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 330/475 [01:59<00:59,  2.44it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 331/475 [02:00<00:57,  2.49it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 332/475 [02:00<01:00,  2.37it/s]\u001b[A\n",
      "batch:  70%|███████   | 333/475 [02:01<01:00,  2.35it/s]\u001b[A\n",
      "batch:  70%|███████   | 334/475 [02:01<00:58,  2.40it/s]\u001b[A\n",
      "batch:  71%|███████   | 335/475 [02:02<00:57,  2.44it/s]\u001b[A\n",
      "batch:  71%|███████   | 336/475 [02:02<00:55,  2.50it/s]\u001b[A\n",
      "batch:  71%|███████   | 337/475 [02:02<00:54,  2.55it/s]\u001b[A\n",
      "batch:  71%|███████   | 338/475 [02:03<00:53,  2.57it/s]\u001b[A\n",
      "batch:  71%|███████▏  | 339/475 [02:03<00:53,  2.56it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 340/475 [02:03<00:51,  2.60it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 341/475 [02:04<00:54,  2.48it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 342/475 [02:04<00:53,  2.47it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 343/475 [02:05<00:52,  2.50it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 344/475 [02:05<00:52,  2.50it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 345/475 [02:06<00:56,  2.29it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 346/475 [02:06<00:55,  2.31it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 347/475 [02:06<00:53,  2.38it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 348/475 [02:07<00:51,  2.45it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 349/475 [02:07<00:50,  2.51it/s]\u001b[A\n",
      "batch:  74%|███████▎  | 350/475 [02:08<00:49,  2.54it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 351/475 [02:08<00:47,  2.58it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 352/475 [02:08<00:54,  2.26it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 353/475 [02:09<00:54,  2.24it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 354/475 [02:09<00:56,  2.14it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 355/475 [02:10<00:54,  2.20it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 356/475 [02:10<00:51,  2.30it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 357/475 [02:11<00:49,  2.38it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 358/475 [02:11<00:47,  2.49it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 359/475 [02:11<00:45,  2.57it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 360/475 [02:12<00:44,  2.60it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 361/475 [02:12<00:43,  2.63it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 362/475 [02:13<00:47,  2.37it/s]\u001b[A\n",
      "batch:  76%|███████▋  | 363/475 [02:13<00:47,  2.37it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 364/475 [02:13<00:45,  2.43it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 365/475 [02:14<00:47,  2.33it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 366/475 [02:14<00:45,  2.37it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 367/475 [02:15<00:44,  2.42it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 368/475 [02:15<00:42,  2.50it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 369/475 [02:16<00:43,  2.45it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 370/475 [02:16<00:42,  2.48it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 371/475 [02:16<00:41,  2.53it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 372/475 [02:17<00:40,  2.56it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 373/475 [02:17<00:39,  2.59it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 374/475 [02:18<00:41,  2.44it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 375/475 [02:18<00:40,  2.45it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 376/475 [02:18<00:39,  2.48it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 377/475 [02:19<00:39,  2.51it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 378/475 [02:19<00:41,  2.34it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 379/475 [02:20<00:41,  2.32it/s]\u001b[A\n",
      "batch:  80%|████████  | 380/475 [02:20<00:39,  2.38it/s]\u001b[A\n",
      "batch:  80%|████████  | 381/475 [02:20<00:37,  2.48it/s]\u001b[A\n",
      "batch:  80%|████████  | 382/475 [02:21<00:36,  2.56it/s]\u001b[A\n",
      "batch:  81%|████████  | 383/475 [02:21<00:35,  2.62it/s]\u001b[A\n",
      "batch:  81%|████████  | 384/475 [02:21<00:34,  2.66it/s]\u001b[A\n",
      "batch:  81%|████████  | 385/475 [02:22<00:33,  2.68it/s]\u001b[A\n",
      "batch:  81%|████████▏ | 386/475 [02:22<00:33,  2.69it/s]\u001b[A\n",
      "batch:  81%|████████▏ | 387/475 [02:23<00:32,  2.72it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 388/475 [02:23<00:31,  2.77it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 389/475 [02:23<00:30,  2.82it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 390/475 [02:24<00:29,  2.84it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 391/475 [02:24<00:30,  2.72it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 392/475 [02:24<00:33,  2.48it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 393/475 [02:25<00:33,  2.44it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 394/475 [02:25<00:32,  2.48it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 395/475 [02:26<00:32,  2.47it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 396/475 [02:26<00:31,  2.52it/s]\u001b[A\n",
      "batch:  84%|████████▎ | 397/475 [02:26<00:30,  2.56it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 398/475 [02:27<00:31,  2.42it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 399/475 [02:27<00:31,  2.42it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 400/475 [02:28<00:30,  2.47it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 401/475 [02:28<00:29,  2.51it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 402/475 [02:29<00:28,  2.54it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 403/475 [02:29<00:28,  2.56it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 404/475 [02:29<00:27,  2.60it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 405/475 [02:30<00:26,  2.62it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 406/475 [02:30<00:25,  2.66it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 407/475 [02:30<00:25,  2.71it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 408/475 [02:31<00:24,  2.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  86%|████████▌ | 409/475 [02:31<00:23,  2.79it/s]\u001b[A\n",
      "batch:  86%|████████▋ | 410/475 [02:31<00:23,  2.79it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 411/475 [02:32<00:25,  2.55it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 412/475 [02:32<00:25,  2.44it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 413/475 [02:33<00:25,  2.45it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 414/475 [02:33<00:24,  2.51it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 415/475 [02:33<00:23,  2.53it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 416/475 [02:34<00:22,  2.60it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 417/475 [02:34<00:21,  2.67it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 418/475 [02:35<00:20,  2.73it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 419/475 [02:35<00:25,  2.23it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 420/475 [02:36<00:24,  2.25it/s]\u001b[A\n",
      "batch:  89%|████████▊ | 421/475 [02:36<00:23,  2.33it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 422/475 [02:36<00:21,  2.42it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 423/475 [02:37<00:22,  2.32it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 424/475 [02:37<00:21,  2.34it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 425/475 [02:38<00:20,  2.39it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 426/475 [02:38<00:19,  2.46it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 427/475 [02:38<00:19,  2.53it/s]\u001b[A\n",
      "batch:  90%|█████████ | 428/475 [02:39<00:22,  2.12it/s]\u001b[A\n",
      "batch:  90%|█████████ | 429/475 [02:40<00:21,  2.16it/s]\u001b[A\n",
      "batch:  91%|█████████ | 430/475 [02:40<00:20,  2.24it/s]\u001b[A\n",
      "batch:  91%|█████████ | 431/475 [02:40<00:18,  2.35it/s]\u001b[A\n",
      "batch:  91%|█████████ | 432/475 [02:41<00:17,  2.45it/s]\u001b[A\n",
      "batch:  91%|█████████ | 433/475 [02:41<00:16,  2.52it/s]\u001b[A\n",
      "batch:  91%|█████████▏| 434/475 [02:41<00:15,  2.59it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 435/475 [02:42<00:17,  2.28it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 436/475 [02:42<00:17,  2.23it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 437/475 [02:43<00:16,  2.29it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 438/475 [02:43<00:15,  2.37it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 439/475 [02:44<00:14,  2.43it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 440/475 [02:44<00:13,  2.51it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 441/475 [02:44<00:13,  2.56it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 442/475 [02:45<00:12,  2.61it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 443/475 [02:45<00:12,  2.65it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 444/475 [02:46<00:15,  1.98it/s]\u001b[A\n",
      "batch:  94%|█████████▎| 445/475 [02:46<00:14,  2.09it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 446/475 [02:47<00:13,  2.21it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 447/475 [02:47<00:11,  2.33it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 448/475 [02:47<00:11,  2.45it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 449/475 [02:48<00:10,  2.53it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 450/475 [02:48<00:09,  2.61it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 451/475 [02:49<00:09,  2.66it/s]\u001b[A\n",
      "batch:  95%|█████████▌| 452/475 [02:49<00:08,  2.69it/s]\u001b[A\n",
      "batch:  95%|█████████▌| 453/475 [02:50<00:10,  2.14it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 454/475 [02:50<00:09,  2.20it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 455/475 [02:50<00:08,  2.28it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 456/475 [02:51<00:08,  2.37it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 457/475 [02:51<00:07,  2.46it/s]\u001b[A\n",
      "batch:  96%|█████████▋| 458/475 [02:52<00:06,  2.54it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 459/475 [02:52<00:06,  2.60it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 460/475 [02:52<00:05,  2.64it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 461/475 [02:53<00:05,  2.64it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 462/475 [02:53<00:04,  2.67it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 463/475 [02:53<00:04,  2.49it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 464/475 [02:54<00:04,  2.47it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 465/475 [02:54<00:04,  2.50it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 466/475 [02:55<00:03,  2.55it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 467/475 [02:55<00:03,  2.41it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 468/475 [02:56<00:02,  2.41it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 469/475 [02:56<00:02,  2.45it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 470/475 [02:56<00:01,  2.50it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 471/475 [02:57<00:01,  2.56it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 472/475 [02:57<00:01,  2.58it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 473/475 [02:57<00:00,  2.62it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 474/475 [02:58<00:00,  2.39it/s]\u001b[A\n",
      "batch: 100%|██████████| 475/475 [02:58<00:00,  2.38it/s]\u001b[A\n",
      "Epoch:  25%|██▌       | 1/4 [02:58<08:56, 178.84s/it]   \u001b[A\n",
      "batch:   0%|          | 0/475 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   0%|          | 1/475 [00:00<03:08,  2.52it/s]\u001b[A\n",
      "batch:   0%|          | 2/475 [00:00<03:06,  2.54it/s]\u001b[A\n",
      "batch:   1%|          | 3/475 [00:01<03:40,  2.15it/s]\u001b[A\n",
      "batch:   1%|          | 4/475 [00:01<03:31,  2.22it/s]\u001b[A\n",
      "batch:   1%|          | 5/475 [00:02<03:23,  2.31it/s]\u001b[A\n",
      "batch:   1%|▏         | 6/475 [00:02<03:13,  2.42it/s]\u001b[A\n",
      "batch:   1%|▏         | 7/475 [00:02<03:05,  2.52it/s]\u001b[A\n",
      "batch:   2%|▏         | 8/475 [00:03<03:00,  2.59it/s]\u001b[A\n",
      "batch:   2%|▏         | 9/475 [00:03<02:57,  2.63it/s]\u001b[A\n",
      "batch:   2%|▏         | 10/475 [00:04<02:54,  2.67it/s]\u001b[A\n",
      "batch:   2%|▏         | 11/475 [00:04<02:50,  2.72it/s]\u001b[A\n",
      "batch:   3%|▎         | 12/475 [00:04<02:47,  2.77it/s]\u001b[A\n",
      "batch:   3%|▎         | 13/475 [00:05<02:43,  2.83it/s]\u001b[A\n",
      "batch:   3%|▎         | 14/475 [00:05<02:41,  2.86it/s]\u001b[A\n",
      "batch:   3%|▎         | 15/475 [00:05<02:49,  2.71it/s]\u001b[A\n",
      "batch:   3%|▎         | 16/475 [00:06<02:57,  2.58it/s]\u001b[A\n",
      "batch:   4%|▎         | 17/475 [00:06<03:01,  2.52it/s]\u001b[A\n",
      "batch:   4%|▍         | 18/475 [00:07<03:01,  2.52it/s]\u001b[A\n",
      "batch:   4%|▍         | 19/475 [00:07<02:57,  2.56it/s]\u001b[A\n",
      "batch:   4%|▍         | 20/475 [00:07<02:55,  2.59it/s]\u001b[A\n",
      "batch:   4%|▍         | 21/475 [00:08<02:53,  2.61it/s]\u001b[A\n",
      "batch:   5%|▍         | 22/475 [00:08<02:51,  2.65it/s]\u001b[A\n",
      "batch:   5%|▍         | 23/475 [00:08<02:49,  2.66it/s]\u001b[A\n",
      "batch:   5%|▌         | 24/475 [00:09<02:48,  2.67it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step (out of 8750): 500\n",
      "Train loss: 3.2455451583862303\n",
      "Validation Accuracy: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   5%|▌         | 25/475 [00:25<37:23,  4.99s/it]\u001b[A\n",
      "batch:   5%|▌         | 26/475 [00:25<26:54,  3.60s/it]\u001b[A\n",
      "batch:   6%|▌         | 27/475 [00:25<19:31,  2.62s/it]\u001b[A\n",
      "batch:   6%|▌         | 28/475 [00:26<14:22,  1.93s/it]\u001b[A\n",
      "batch:   6%|▌         | 29/475 [00:26<10:46,  1.45s/it]\u001b[A\n",
      "batch:   6%|▋         | 30/475 [00:26<08:18,  1.12s/it]\u001b[A\n",
      "batch:   7%|▋         | 31/475 [00:27<06:33,  1.13it/s]\u001b[A\n",
      "batch:   7%|▋         | 32/475 [00:27<05:20,  1.38it/s]\u001b[A\n",
      "batch:   7%|▋         | 33/475 [00:27<04:28,  1.65it/s]\u001b[A\n",
      "batch:   7%|▋         | 34/475 [00:28<03:53,  1.89it/s]\u001b[A\n",
      "batch:   7%|▋         | 35/475 [00:28<03:27,  2.12it/s]\u001b[A\n",
      "batch:   8%|▊         | 36/475 [00:28<03:09,  2.32it/s]\u001b[A\n",
      "batch:   8%|▊         | 37/475 [00:29<02:56,  2.48it/s]\u001b[A\n",
      "batch:   8%|▊         | 38/475 [00:29<02:48,  2.59it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "step = 0\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    \n",
    "    # Train the data for one epoch\n",
    "    for batch in tqdm(train_dataloader,desc='batch',leave=False):\n",
    "        step = step + 1\n",
    "#         if(step%10==0):\n",
    "#             print('step (out of 8750): ' + str(step) + '\\r')\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())  \n",
    "        vis.plot('loss', 'train_loss', 'Loss',step,loss.item())\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if(step%500 == 0): # in [500, 1000, 3000, 6000]\n",
    "            print('step (out of 8750): ' + str(step))\n",
    "            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "            # Validation\n",
    "\n",
    "            # Put model in evaluation mode to evaluate loss on the validation set\n",
    "            model.eval()\n",
    "\n",
    "            # Tracking variables \n",
    "            eval_loss, eval_accuracy = 0, 0\n",
    "            nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "                # Add batch to GPU\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                # Unpack the inputs from our dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "                with torch.no_grad():\n",
    "                    # Forward pass, calculate logit predictions\n",
    "                    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "                tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "                nb_eval_steps += 1\n",
    "\n",
    "            print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "            vis.plot('accuracy', 'val_acc', 'val_acc',step,eval_accuracy/nb_eval_steps)\n",
    "            torch.save(model, 'bert_large.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.comments.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df2.comments.values.shape[0]):\n",
    "    df2.comments.values[i] = df2.comments.values[i][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df2.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 512)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "# prediction_labels = torch.tensor(labels)\n",
    "  \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(prediction_dataloader,desc='batch',leave=False):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "main_preds = []\n",
    "for i in range(len(predictions)):\n",
    "    main_preds += list(np.argmax(predictions[i], axis=1))\n",
    "    \n",
    "print(len(main_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 16,\n",
       " 6,\n",
       " 14,\n",
       " 4,\n",
       " 7,\n",
       " 19,\n",
       " 6,\n",
       " 13,\n",
       " 16,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 17,\n",
       " 3,\n",
       " 14,\n",
       " 18,\n",
       " 14,\n",
       " 9,\n",
       " 13,\n",
       " 5,\n",
       " 17,\n",
       " 15,\n",
       " 19,\n",
       " 15,\n",
       " 11,\n",
       " 7,\n",
       " 12,\n",
       " 8,\n",
       " 4,\n",
       " 10,\n",
       " 7,\n",
       " 1,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 19,\n",
       " 4,\n",
       " 1,\n",
       " 12,\n",
       " 0,\n",
       " 6,\n",
       " 16,\n",
       " 4,\n",
       " 12,\n",
       " 9,\n",
       " 17,\n",
       " 15,\n",
       " 5,\n",
       " 11,\n",
       " 2,\n",
       " 6,\n",
       " 17,\n",
       " 5,\n",
       " 7,\n",
       " 17,\n",
       " 18,\n",
       " 13,\n",
       " 16,\n",
       " 1,\n",
       " 8,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 17,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 15,\n",
       " 16,\n",
       " 7,\n",
       " 16,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 14,\n",
       " 17,\n",
       " 9,\n",
       " 8,\n",
       " 17,\n",
       " 13,\n",
       " 6,\n",
       " 13,\n",
       " 0,\n",
       " 19,\n",
       " 13,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 6,\n",
       " 5,\n",
       " 17,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 19,\n",
       " 15,\n",
       " 8,\n",
       " 4,\n",
       " 16,\n",
       " 11,\n",
       " 2,\n",
       " 16,\n",
       " 10,\n",
       " 2,\n",
       " 13,\n",
       " 5,\n",
       " 11,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 17,\n",
       " 3,\n",
       " 19,\n",
       " 14,\n",
       " 10,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 2,\n",
       " 18,\n",
       " 19,\n",
       " 0,\n",
       " 19,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 13,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 12,\n",
       " 16,\n",
       " 3,\n",
       " 4,\n",
       " 16,\n",
       " 15,\n",
       " 4,\n",
       " 9,\n",
       " 12,\n",
       " 16,\n",
       " 10,\n",
       " 13,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 17,\n",
       " 12,\n",
       " 5,\n",
       " 17,\n",
       " 9,\n",
       " 6,\n",
       " 15,\n",
       " 5,\n",
       " 4,\n",
       " 13,\n",
       " 15,\n",
       " 15,\n",
       " 4,\n",
       " 18,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 13,\n",
       " 7,\n",
       " 3,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 10,\n",
       " 12,\n",
       " 5,\n",
       " 4,\n",
       " 17,\n",
       " 16,\n",
       " 6,\n",
       " 17,\n",
       " 10,\n",
       " 2,\n",
       " 6,\n",
       " 16,\n",
       " 11,\n",
       " 2,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 17,\n",
       " 10,\n",
       " 17,\n",
       " 2,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 12,\n",
       " 19,\n",
       " 18,\n",
       " 13,\n",
       " 13,\n",
       " 2,\n",
       " 16,\n",
       " 10,\n",
       " 16,\n",
       " 19,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 14,\n",
       " 18,\n",
       " 17,\n",
       " 8,\n",
       " 4,\n",
       " 13,\n",
       " 14,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 9,\n",
       " 10,\n",
       " 0,\n",
       " 17,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 15,\n",
       " 2,\n",
       " 17,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 13,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 18,\n",
       " 6,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 0,\n",
       " 18,\n",
       " 3,\n",
       " 19,\n",
       " 9,\n",
       " 12,\n",
       " 16,\n",
       " 4,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 15,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 13,\n",
       " 3,\n",
       " 10,\n",
       " 19,\n",
       " 11,\n",
       " 14,\n",
       " 13,\n",
       " 5,\n",
       " 10,\n",
       " 12,\n",
       " 16,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 5,\n",
       " 11,\n",
       " 0,\n",
       " 4,\n",
       " 12,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 13,\n",
       " 10,\n",
       " 13,\n",
       " 8,\n",
       " 11,\n",
       " 13,\n",
       " 2,\n",
       " 18,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 15,\n",
       " 17,\n",
       " 13,\n",
       " 7,\n",
       " 19,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 17,\n",
       " 15,\n",
       " 17,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 14,\n",
       " 12,\n",
       " 15,\n",
       " 5,\n",
       " 2,\n",
       " 11,\n",
       " 13,\n",
       " 1,\n",
       " 13,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 3,\n",
       " 19,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 18,\n",
       " 0,\n",
       " 10,\n",
       " 17,\n",
       " 5,\n",
       " 2,\n",
       " 10,\n",
       " 17,\n",
       " 2,\n",
       " 15,\n",
       " 13,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 16,\n",
       " 17,\n",
       " 6,\n",
       " 5,\n",
       " 17,\n",
       " 7,\n",
       " 17,\n",
       " 14,\n",
       " 15,\n",
       " 11,\n",
       " 5,\n",
       " 11,\n",
       " 2,\n",
       " 2,\n",
       " 14,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 17,\n",
       " 0,\n",
       " 18,\n",
       " 13,\n",
       " 3,\n",
       " 18,\n",
       " 7,\n",
       " 17,\n",
       " 3,\n",
       " 17,\n",
       " 15,\n",
       " 11,\n",
       " 5,\n",
       " 10,\n",
       " 13,\n",
       " 8,\n",
       " 5,\n",
       " 10,\n",
       " 18,\n",
       " 2,\n",
       " 17,\n",
       " 6,\n",
       " 17,\n",
       " 2,\n",
       " 9,\n",
       " 10,\n",
       " 5,\n",
       " 19,\n",
       " 11,\n",
       " 16,\n",
       " 2,\n",
       " 5,\n",
       " 19,\n",
       " 15,\n",
       " 8,\n",
       " 13,\n",
       " 5,\n",
       " 19,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 15,\n",
       " 17,\n",
       " 8,\n",
       " 17,\n",
       " 7,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 9,\n",
       " 11,\n",
       " 15,\n",
       " 2,\n",
       " 12,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 18,\n",
       " 11,\n",
       " 18,\n",
       " 13,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 15,\n",
       " 15,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 18,\n",
       " 17,\n",
       " 15,\n",
       " 12,\n",
       " 8,\n",
       " 17,\n",
       " 19,\n",
       " 12,\n",
       " 0,\n",
       " 10,\n",
       " 18,\n",
       " 16,\n",
       " 13,\n",
       " 8,\n",
       " 15,\n",
       " 7,\n",
       " 10,\n",
       " 17,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 10,\n",
       " 19,\n",
       " 5,\n",
       " 2,\n",
       " 17,\n",
       " 2,\n",
       " 17,\n",
       " 9,\n",
       " 16,\n",
       " 0,\n",
       " 19,\n",
       " 6,\n",
       " 4,\n",
       " 19,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 11,\n",
       " 19,\n",
       " 5,\n",
       " 19,\n",
       " 7,\n",
       " 17,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 2,\n",
       " 19,\n",
       " 6,\n",
       " 17,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 17,\n",
       " 18,\n",
       " 11,\n",
       " 15,\n",
       " 12,\n",
       " 11,\n",
       " 6,\n",
       " 12,\n",
       " 5,\n",
       " 16,\n",
       " 1,\n",
       " 5,\n",
       " 10,\n",
       " 17,\n",
       " 16,\n",
       " 3,\n",
       " 15,\n",
       " 6,\n",
       " 12,\n",
       " 13,\n",
       " 4,\n",
       " 11,\n",
       " 6,\n",
       " 18,\n",
       " 7,\n",
       " 19,\n",
       " 6,\n",
       " 19,\n",
       " 2,\n",
       " 17,\n",
       " 4,\n",
       " 11,\n",
       " 15,\n",
       " 15,\n",
       " 17,\n",
       " 0,\n",
       " 14,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 17,\n",
       " 8,\n",
       " 17,\n",
       " 3,\n",
       " 17,\n",
       " 11,\n",
       " 15,\n",
       " 10,\n",
       " 18,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 15,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 11,\n",
       " 9,\n",
       " 4,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 11,\n",
       " 6,\n",
       " 18,\n",
       " 19,\n",
       " 17,\n",
       " 19,\n",
       " 17,\n",
       " 7,\n",
       " 14,\n",
       " 16,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 8,\n",
       " 4,\n",
       " 18,\n",
       " 15,\n",
       " 4,\n",
       " 2,\n",
       " 19,\n",
       " 16,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 18,\n",
       " 6,\n",
       " 17,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 17,\n",
       " 3,\n",
       " 0,\n",
       " 12,\n",
       " 9,\n",
       " 15,\n",
       " 11,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 17,\n",
       " 15,\n",
       " 2,\n",
       " 17,\n",
       " 14,\n",
       " 9,\n",
       " 4,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 17,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 19,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 17,\n",
       " 1,\n",
       " 11,\n",
       " 8,\n",
       " 9,\n",
       " 19,\n",
       " 6,\n",
       " 13,\n",
       " 4,\n",
       " 15,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 8,\n",
       " 16,\n",
       " 17,\n",
       " 4,\n",
       " 17,\n",
       " 7,\n",
       " 14,\n",
       " 17,\n",
       " 13,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 13,\n",
       " 6,\n",
       " 16,\n",
       " 17,\n",
       " 4,\n",
       " 8,\n",
       " 17,\n",
       " 4,\n",
       " 14,\n",
       " 17,\n",
       " 15,\n",
       " 7,\n",
       " 8,\n",
       " 16,\n",
       " 5,\n",
       " 9,\n",
       " 15,\n",
       " 4,\n",
       " 6,\n",
       " 17,\n",
       " 6,\n",
       " 19,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 17,\n",
       " 7,\n",
       " 12,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 15,\n",
       " 0,\n",
       " 16,\n",
       " 5,\n",
       " 18,\n",
       " 10,\n",
       " 3,\n",
       " 5,\n",
       " 13,\n",
       " 7,\n",
       " 3,\n",
       " 13,\n",
       " 10,\n",
       " 17,\n",
       " 2,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 17,\n",
       " 3,\n",
       " 10,\n",
       " 19,\n",
       " 7,\n",
       " 15,\n",
       " 0,\n",
       " 2,\n",
       " 13,\n",
       " 19,\n",
       " 10,\n",
       " 0,\n",
       " 16,\n",
       " 17,\n",
       " 15,\n",
       " 14,\n",
       " 18,\n",
       " 17,\n",
       " 19,\n",
       " 3,\n",
       " 11,\n",
       " 3,\n",
       " 10,\n",
       " 8,\n",
       " 13,\n",
       " 16,\n",
       " 17,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 17,\n",
       " 12,\n",
       " 5,\n",
       " 10,\n",
       " 6,\n",
       " 17,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 19,\n",
       " 2,\n",
       " 11,\n",
       " 0,\n",
       " 1,\n",
       " 10,\n",
       " 3,\n",
       " 11,\n",
       " 2,\n",
       " 11,\n",
       " 16,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 12,\n",
       " 3,\n",
       " 16,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 17,\n",
       " 10,\n",
       " 1,\n",
       " 16,\n",
       " 7,\n",
       " 12,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 19,\n",
       " 0,\n",
       " 3,\n",
       " 19,\n",
       " 4,\n",
       " 18,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 16,\n",
       " 18,\n",
       " 16,\n",
       " 4,\n",
       " 14,\n",
       " 6,\n",
       " 13,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 10,\n",
       " 12,\n",
       " 19,\n",
       " 4,\n",
       " 14,\n",
       " 8,\n",
       " 17,\n",
       " 4,\n",
       " 16,\n",
       " 19,\n",
       " 17,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 19,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 11,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 14,\n",
       " 16,\n",
       " 4,\n",
       " 16,\n",
       " 0,\n",
       " 17,\n",
       " 12,\n",
       " 14,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 17,\n",
       " 9,\n",
       " 15,\n",
       " 12,\n",
       " 16,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 4,\n",
       " 18,\n",
       " 14,\n",
       " 7,\n",
       " 16,\n",
       " 17,\n",
       " 6,\n",
       " 15,\n",
       " 17,\n",
       " 0,\n",
       " 17,\n",
       " 9,\n",
       " 10,\n",
       " 6,\n",
       " 19,\n",
       " 15,\n",
       " 17,\n",
       " 3,\n",
       " 17,\n",
       " 1,\n",
       " 13,\n",
       " 17,\n",
       " 10,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 17,\n",
       " 15,\n",
       " 5,\n",
       " 13,\n",
       " 15,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 16,\n",
       " 17,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 4,\n",
       " 17,\n",
       " 13,\n",
       " 1,\n",
       " 18,\n",
       " 6,\n",
       " 19,\n",
       " 10,\n",
       " 18,\n",
       " 12,\n",
       " 17,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 11,\n",
       " 19,\n",
       " 1,\n",
       " 6,\n",
       " 12,\n",
       " 16,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 13,\n",
       " 17,\n",
       " 13,\n",
       " 8,\n",
       " 17,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 16,\n",
       " 19,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 13,\n",
       " 19,\n",
       " 5,\n",
       " 2,\n",
       " 19,\n",
       " 14,\n",
       " 4,\n",
       " 16,\n",
       " 2,\n",
       " 0,\n",
       " 18,\n",
       " 7,\n",
       " 8,\n",
       " 12,\n",
       " 9,\n",
       " 13,\n",
       " 5,\n",
       " 18,\n",
       " 11,\n",
       " 4,\n",
       " 16,\n",
       " 15,\n",
       " 9,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pd.DataFrame()\n",
    "test_preds['Id'] = df2['id']\n",
    "test_preds['Category'] = mapping[main_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29995</td>\n",
       "      <td>29995</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29996</td>\n",
       "      <td>29996</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29997</td>\n",
       "      <td>29997</td>\n",
       "      <td>Overwatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29998</td>\n",
       "      <td>29998</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29999</td>\n",
       "      <td>29999</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id       Category\n",
       "0          0       baseball\n",
       "1          1         europe\n",
       "2          2          anime\n",
       "3          3      worldnews\n",
       "4          4          funny\n",
       "...      ...            ...\n",
       "29995  29995         movies\n",
       "29996  29996         movies\n",
       "29997  29997      Overwatch\n",
       "29998  29998  gameofthrones\n",
       "29999  29999            wow\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='test2.csv' target='_blank'>test2.csv</a><br>"
      ],
      "text/plain": [
       "/network/home/penmetss/comp551/test2.csv"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.to_csv(\"test2.csv\", index=False)\n",
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(a.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('main_model_v2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
