{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "\n",
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')\n",
    "            \n",
    "            \n",
    "    \n",
    "vis = VisdomLinePlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from pytorch_transformers import AdamW\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "# device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Honestly, Buffalo is the correct answer. I rem...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ah yes way could have been :( remember when he...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>He wouldn't have been a bad signing if we woul...</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Easy. You use the piss and dry technique. Let ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           comments       subreddits\n",
       "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
       "1   1  Ah yes way could have been :( remember when he...              nba\n",
       "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
       "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
       "4   4  Easy. You use the piss and dry technique. Let ...            funny"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_train.csv')\n",
    "df2 = pd.read_csv('reddit_test.csv')\n",
    "# df = df.sample(1000, random_state=1).copy()\n",
    "# df2 = df2.sample(1000, random_state=1).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'], mapping = df['subreddits'].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "labels = df.category_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁honestly', ',', '▁', 'buffalo', '▁is', '▁the', '▁correct', '▁answer', '.', '▁', 'i', '▁remember', '▁people', '▁', '(', 'some', 'what', ')', '▁joking', '▁that', '▁', 'buffalo', \"'\", 's', '▁man', 'tra', '▁for', '▁starting', '▁goalie', 's', '▁was', '▁', '\"', 'win', '▁a', '▁game', ',', '▁get', '▁traded', '\"', '.', '▁', 'i', '▁think', '▁', 'ed', 'mont', 'on', \"'\", 's', '▁front', '▁office', '▁was', '▁a', '▁', 'tra', 've', 's', 'ty', '▁for', '▁the', '▁better', '▁part', '▁of', '▁10', '▁years', ',', '▁but', '▁', 'buffalo', \"'\", 's', '▁systematic', '▁destruction', '▁of', '▁the', '▁term', '▁', \"'\", 'competitive', \"'\", '▁was', '▁much', '▁more', '▁responsible', '▁for', '▁the', '▁change', '▁to', '▁the', '▁draft', '▁lottery', '.', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.05)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 8\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5633561643835616\n"
     ]
    }
   ],
   "source": [
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "                logits = output[0]\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=20)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "batch:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   1%|          | 1/119 [00:00<01:55,  1.02it/s]\u001b[A\n",
      "batch:   2%|▏         | 2/119 [00:01<01:54,  1.03it/s]\u001b[A\n",
      "batch:   3%|▎         | 3/119 [00:02<01:52,  1.03it/s]\u001b[A\n",
      "batch:   3%|▎         | 4/119 [00:03<01:51,  1.04it/s]\u001b[A\n",
      "batch:   4%|▍         | 5/119 [00:04<01:50,  1.03it/s]\u001b[A\n",
      "batch:   5%|▌         | 6/119 [00:05<01:48,  1.04it/s]\u001b[A\n",
      "batch:   6%|▌         | 7/119 [00:06<01:46,  1.05it/s]\u001b[A\n",
      "batch:   7%|▋         | 8/119 [00:07<01:45,  1.05it/s]\u001b[A\n",
      "batch:   8%|▊         | 9/119 [00:08<01:45,  1.04it/s]\u001b[A\n",
      "batch:   8%|▊         | 10/119 [00:09<01:44,  1.04it/s]\u001b[A\n",
      "batch:   9%|▉         | 11/119 [00:10<01:44,  1.04it/s]\u001b[A\n",
      "batch:  10%|█         | 12/119 [00:11<01:42,  1.04it/s]\u001b[A\n",
      "batch:  11%|█         | 13/119 [00:12<01:41,  1.05it/s]\u001b[A\n",
      "batch:  12%|█▏        | 14/119 [00:13<01:40,  1.05it/s]\u001b[A\n",
      "batch:  13%|█▎        | 15/119 [00:14<01:38,  1.05it/s]\u001b[A\n",
      "batch:  13%|█▎        | 16/119 [00:15<01:37,  1.05it/s]\u001b[A\n",
      "batch:  14%|█▍        | 17/119 [00:16<01:36,  1.06it/s]\u001b[A\n",
      "batch:  15%|█▌        | 18/119 [00:17<01:35,  1.06it/s]\u001b[A\n",
      "batch:  16%|█▌        | 19/119 [00:18<01:34,  1.05it/s]\u001b[A\n",
      "batch:  17%|█▋        | 20/119 [00:19<01:34,  1.05it/s]\u001b[A\n",
      "batch:  18%|█▊        | 21/119 [00:20<01:33,  1.05it/s]\u001b[A\n",
      "batch:  18%|█▊        | 22/119 [00:21<01:32,  1.05it/s]\u001b[A\n",
      "batch:  19%|█▉        | 23/119 [00:21<01:31,  1.05it/s]\u001b[A\n",
      "batch:  20%|██        | 24/119 [00:22<01:30,  1.05it/s]\u001b[A\n",
      "batch:  21%|██        | 25/119 [00:23<01:29,  1.05it/s]\u001b[A\n",
      "batch:  22%|██▏       | 26/119 [00:24<01:28,  1.05it/s]\u001b[A\n",
      "batch:  23%|██▎       | 27/119 [00:25<01:27,  1.05it/s]\u001b[A\n",
      "batch:  24%|██▎       | 28/119 [00:26<01:26,  1.05it/s]\u001b[A\n",
      "batch:  24%|██▍       | 29/119 [00:27<01:25,  1.05it/s]\u001b[A\n",
      "batch:  25%|██▌       | 30/119 [00:28<01:24,  1.05it/s]\u001b[A\n",
      "batch:  26%|██▌       | 31/119 [00:29<01:24,  1.04it/s]\u001b[A\n",
      "batch:  27%|██▋       | 32/119 [00:30<01:23,  1.04it/s]\u001b[A\n",
      "batch:  28%|██▊       | 33/119 [00:31<01:22,  1.04it/s]\u001b[A\n",
      "batch:  29%|██▊       | 34/119 [00:32<01:21,  1.05it/s]\u001b[A\n",
      "batch:  29%|██▉       | 35/119 [00:33<01:20,  1.05it/s]\u001b[A\n",
      "batch:  30%|███       | 36/119 [00:34<01:18,  1.05it/s]\u001b[A\n",
      "batch:  31%|███       | 37/119 [00:35<01:17,  1.06it/s]\u001b[A\n",
      "batch:  32%|███▏      | 38/119 [00:36<01:16,  1.06it/s]\u001b[A\n",
      "batch:  33%|███▎      | 39/119 [00:37<01:15,  1.06it/s]\u001b[A\n",
      "batch:  34%|███▎      | 40/119 [00:38<01:14,  1.06it/s]\u001b[A\n",
      "batch:  34%|███▍      | 41/119 [00:39<01:14,  1.05it/s]\u001b[A\n",
      "batch:  35%|███▌      | 42/119 [00:40<01:13,  1.05it/s]\u001b[A\n",
      "batch:  36%|███▌      | 43/119 [00:40<01:12,  1.06it/s]\u001b[A\n",
      "batch:  37%|███▋      | 44/119 [00:41<01:11,  1.05it/s]\u001b[A\n",
      "batch:  38%|███▊      | 45/119 [00:42<01:10,  1.05it/s]\u001b[A\n",
      "batch:  39%|███▊      | 46/119 [00:43<01:09,  1.05it/s]\u001b[A\n",
      "batch:  39%|███▉      | 47/119 [00:44<01:08,  1.05it/s]\u001b[A\n",
      "batch:  40%|████      | 48/119 [00:45<01:07,  1.05it/s]\u001b[A\n",
      "batch:  41%|████      | 49/119 [00:46<01:06,  1.05it/s]\u001b[A\n",
      "batch:  42%|████▏     | 50/119 [00:47<01:05,  1.05it/s]\u001b[A\n",
      "batch:  43%|████▎     | 51/119 [00:48<01:05,  1.04it/s]\u001b[A\n",
      "batch:  44%|████▎     | 52/119 [00:49<01:04,  1.04it/s]\u001b[A\n",
      "batch:  45%|████▍     | 53/119 [00:50<01:02,  1.05it/s]\u001b[A\n",
      "batch:  45%|████▌     | 54/119 [00:51<01:01,  1.05it/s]\u001b[A\n",
      "batch:  46%|████▌     | 55/119 [00:52<01:00,  1.06it/s]\u001b[A\n",
      "batch:  47%|████▋     | 56/119 [00:53<00:59,  1.06it/s]\u001b[A\n",
      "batch:  48%|████▊     | 57/119 [00:54<00:58,  1.06it/s]\u001b[A\n",
      "batch:  49%|████▊     | 58/119 [00:55<00:57,  1.06it/s]\u001b[A\n",
      "batch:  50%|████▉     | 59/119 [00:56<00:56,  1.06it/s]\u001b[A\n",
      "batch:  50%|█████     | 60/119 [00:57<00:56,  1.05it/s]\u001b[A\n",
      "batch:  51%|█████▏    | 61/119 [00:58<00:55,  1.05it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 62/119 [00:59<00:54,  1.05it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 63/119 [01:00<00:53,  1.05it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 64/119 [01:00<00:52,  1.05it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 65/119 [01:01<00:51,  1.05it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 66/119 [01:02<00:50,  1.05it/s]\u001b[A\n",
      "batch:  56%|█████▋    | 67/119 [01:03<00:49,  1.05it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 68/119 [01:04<00:48,  1.05it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 69/119 [01:05<00:47,  1.05it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 70/119 [01:06<00:46,  1.04it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 71/119 [01:07<00:46,  1.04it/s]\u001b[A\n",
      "batch:  61%|██████    | 72/119 [01:08<00:45,  1.04it/s]\u001b[A\n",
      "batch:  61%|██████▏   | 73/119 [01:09<00:44,  1.04it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 74/119 [01:10<00:42,  1.05it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 75/119 [01:11<00:41,  1.06it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 76/119 [01:12<00:40,  1.06it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 77/119 [01:13<00:39,  1.06it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 78/119 [01:14<00:38,  1.06it/s]\u001b[A\n",
      "batch:  66%|██████▋   | 79/119 [01:15<00:37,  1.06it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 80/119 [01:16<00:36,  1.05it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 81/119 [01:17<00:36,  1.05it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 82/119 [01:18<00:35,  1.05it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 83/119 [01:19<00:34,  1.05it/s]\u001b[A\n",
      "batch:  71%|███████   | 84/119 [01:20<00:33,  1.05it/s]\u001b[A\n",
      "batch:  71%|███████▏  | 85/119 [01:20<00:32,  1.05it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 86/119 [01:21<00:31,  1.05it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 87/119 [01:22<00:30,  1.05it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 88/119 [01:23<00:29,  1.05it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 89/119 [01:24<00:28,  1.05it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 90/119 [01:25<00:27,  1.05it/s]\u001b[A\n",
      "batch:  76%|███████▋  | 91/119 [01:26<00:26,  1.05it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 92/119 [01:27<00:25,  1.05it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 93/119 [01:28<00:24,  1.05it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 94/119 [01:29<00:23,  1.04it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 95/119 [01:30<00:22,  1.05it/s]\u001b[A\n",
      "batch:  81%|████████  | 96/119 [01:31<00:21,  1.05it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 97/119 [01:32<00:20,  1.05it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 98/119 [01:33<00:19,  1.06it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 99/119 [01:34<00:18,  1.06it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 100/119 [01:35<00:17,  1.06it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 101/119 [01:36<00:17,  1.06it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 102/119 [01:37<00:16,  1.05it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 103/119 [01:38<00:15,  1.05it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 104/119 [01:39<00:14,  1.05it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 105/119 [01:39<00:13,  1.06it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 106/119 [01:40<00:12,  1.05it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 107/119 [01:41<00:11,  1.05it/s]\u001b[A\n",
      "batch:  91%|█████████ | 108/119 [01:42<00:10,  1.05it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 109/119 [01:43<00:09,  1.05it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 110/119 [01:44<00:08,  1.05it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 111/119 [01:45<00:07,  1.05it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 112/119 [01:46<00:06,  1.05it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 113/119 [01:47<00:05,  1.05it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 114/119 [01:48<00:04,  1.04it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 115/119 [01:49<00:03,  1.05it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 116/119 [01:50<00:02,  1.05it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 117/119 [01:51<00:01,  1.06it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 118/119 [01:52<00:00,  1.05it/s]\u001b[A\n",
      "batch: 100%|██████████| 119/119 [01:53<00:00,  1.15it/s]\u001b[A\n",
      "Epoch:  25%|██▌       | 1/4 [01:53<05:39, 113.04s/it]   \u001b[A\n",
      "batch:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.0547075572134066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   1%|          | 1/119 [00:00<01:52,  1.05it/s]\u001b[A\n",
      "batch:   2%|▏         | 2/119 [00:01<01:51,  1.05it/s]\u001b[A\n",
      "batch:   3%|▎         | 3/119 [00:02<01:50,  1.05it/s]\u001b[A\n",
      "batch:   3%|▎         | 4/119 [00:03<01:49,  1.05it/s]\u001b[A\n",
      "batch:   4%|▍         | 5/119 [00:04<01:48,  1.05it/s]\u001b[A\n",
      "batch:   5%|▌         | 6/119 [00:05<01:47,  1.05it/s]\u001b[A\n",
      "batch:   6%|▌         | 7/119 [00:06<01:46,  1.05it/s]\u001b[A\n",
      "batch:   7%|▋         | 8/119 [00:07<01:45,  1.05it/s]\u001b[A\n",
      "batch:   8%|▊         | 9/119 [00:08<01:45,  1.05it/s]\u001b[A\n",
      "batch:   8%|▊         | 10/119 [00:09<01:44,  1.04it/s]\u001b[A\n",
      "batch:   9%|▉         | 11/119 [00:10<01:42,  1.05it/s]\u001b[A\n",
      "batch:  10%|█         | 12/119 [00:11<01:42,  1.05it/s]\u001b[A\n",
      "batch:  11%|█         | 13/119 [00:12<01:41,  1.05it/s]\u001b[A\n",
      "batch:  12%|█▏        | 14/119 [00:13<01:39,  1.05it/s]\u001b[A\n",
      "batch:  13%|█▎        | 15/119 [00:14<01:38,  1.05it/s]\u001b[A\n",
      "batch:  13%|█▎        | 16/119 [00:15<01:37,  1.06it/s]\u001b[A\n",
      "batch:  14%|█▍        | 17/119 [00:16<01:36,  1.06it/s]\u001b[A\n",
      "batch:  15%|█▌        | 18/119 [00:17<01:35,  1.05it/s]\u001b[A\n",
      "batch:  16%|█▌        | 19/119 [00:18<01:35,  1.05it/s]\u001b[A\n",
      "batch:  17%|█▋        | 20/119 [00:19<01:34,  1.05it/s]\u001b[A\n",
      "batch:  18%|█▊        | 21/119 [00:19<01:33,  1.05it/s]\u001b[A\n",
      "batch:  18%|█▊        | 22/119 [00:20<01:32,  1.05it/s]\u001b[A\n",
      "batch:  19%|█▉        | 23/119 [00:21<01:31,  1.05it/s]\u001b[A\n",
      "batch:  20%|██        | 24/119 [00:22<01:31,  1.04it/s]\u001b[A\n",
      "batch:  21%|██        | 25/119 [00:23<01:29,  1.05it/s]\u001b[A\n",
      "batch:  22%|██▏       | 26/119 [00:24<01:29,  1.04it/s]\u001b[A\n",
      "batch:  23%|██▎       | 27/119 [00:25<01:27,  1.05it/s]\u001b[A\n",
      "batch:  24%|██▎       | 28/119 [00:26<01:26,  1.05it/s]\u001b[A\n",
      "batch:  24%|██▍       | 29/119 [00:27<01:25,  1.05it/s]\u001b[A\n",
      "batch:  25%|██▌       | 30/119 [00:28<01:24,  1.05it/s]\u001b[A\n",
      "batch:  26%|██▌       | 31/119 [00:29<01:24,  1.05it/s]\u001b[A\n",
      "batch:  27%|██▋       | 32/119 [00:30<01:22,  1.05it/s]\u001b[A\n",
      "batch:  28%|██▊       | 33/119 [00:31<01:21,  1.05it/s]\u001b[A\n",
      "batch:  29%|██▊       | 34/119 [00:32<01:20,  1.06it/s]\u001b[A\n",
      "batch:  29%|██▉       | 35/119 [00:33<01:19,  1.06it/s]\u001b[A\n",
      "batch:  30%|███       | 36/119 [00:34<01:18,  1.05it/s]\u001b[A\n",
      "batch:  31%|███       | 37/119 [00:35<01:17,  1.05it/s]\u001b[A\n",
      "batch:  32%|███▏      | 38/119 [00:36<01:16,  1.06it/s]\u001b[A\n",
      "batch:  33%|███▎      | 39/119 [00:37<01:15,  1.05it/s]\u001b[A\n",
      "batch:  34%|███▎      | 40/119 [00:38<01:14,  1.05it/s]\u001b[A\n",
      "batch:  34%|███▍      | 41/119 [00:39<01:14,  1.05it/s]\u001b[A\n",
      "batch:  35%|███▌      | 42/119 [00:39<01:13,  1.05it/s]\u001b[A\n",
      "batch:  36%|███▌      | 43/119 [00:40<01:12,  1.05it/s]\u001b[A\n",
      "batch:  37%|███▋      | 44/119 [00:41<01:11,  1.05it/s]\u001b[A\n",
      "batch:  38%|███▊      | 45/119 [00:42<01:10,  1.05it/s]\u001b[A\n",
      "batch:  39%|███▊      | 46/119 [00:43<01:09,  1.05it/s]\u001b[A\n",
      "batch:  39%|███▉      | 47/119 [00:44<01:08,  1.05it/s]\u001b[A\n",
      "batch:  40%|████      | 48/119 [00:45<01:07,  1.05it/s]\u001b[A\n",
      "batch:  41%|████      | 49/119 [00:46<01:06,  1.05it/s]\u001b[A\n",
      "batch:  42%|████▏     | 50/119 [00:47<01:06,  1.04it/s]\u001b[A\n",
      "batch:  43%|████▎     | 51/119 [00:48<01:05,  1.04it/s]\u001b[A\n",
      "batch:  44%|████▎     | 52/119 [00:49<01:04,  1.04it/s]\u001b[A\n",
      "batch:  45%|████▍     | 53/119 [00:50<01:03,  1.04it/s]\u001b[A\n",
      "batch:  45%|████▌     | 54/119 [00:51<01:02,  1.04it/s]\u001b[A\n",
      "batch:  46%|████▌     | 55/119 [00:52<01:01,  1.05it/s]\u001b[A\n",
      "batch:  47%|████▋     | 56/119 [00:53<00:59,  1.05it/s]\u001b[A\n",
      "batch:  48%|████▊     | 57/119 [00:54<00:59,  1.05it/s]\u001b[A\n",
      "batch:  49%|████▊     | 58/119 [00:55<00:58,  1.05it/s]\u001b[A\n",
      "batch:  50%|████▉     | 59/119 [00:56<00:57,  1.05it/s]\u001b[A\n",
      "batch:  50%|█████     | 60/119 [00:57<00:56,  1.05it/s]\u001b[A\n",
      "batch:  51%|█████▏    | 61/119 [00:58<00:55,  1.05it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 62/119 [00:59<00:54,  1.05it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 63/119 [01:00<00:53,  1.05it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 64/119 [01:00<00:52,  1.05it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 65/119 [01:01<00:51,  1.05it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 66/119 [01:02<00:50,  1.04it/s]\u001b[A\n",
      "batch:  56%|█████▋    | 67/119 [01:03<00:49,  1.05it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 68/119 [01:04<00:48,  1.05it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 69/119 [01:05<00:47,  1.04it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 70/119 [01:06<00:46,  1.05it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 71/119 [01:07<00:46,  1.04it/s]\u001b[A\n",
      "batch:  61%|██████    | 72/119 [01:08<00:45,  1.04it/s]\u001b[A\n",
      "batch:  61%|██████▏   | 73/119 [01:09<00:44,  1.04it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 74/119 [01:10<00:43,  1.04it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 75/119 [01:11<00:42,  1.04it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 76/119 [01:12<00:41,  1.05it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 77/119 [01:13<00:39,  1.05it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 78/119 [01:14<00:38,  1.05it/s]\u001b[A\n",
      "batch:  66%|██████▋   | 79/119 [01:15<00:38,  1.05it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 80/119 [01:16<00:37,  1.05it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 81/119 [01:17<00:36,  1.05it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 82/119 [01:18<00:35,  1.06it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 83/119 [01:19<00:34,  1.05it/s]\u001b[A\n",
      "batch:  71%|███████   | 84/119 [01:20<00:33,  1.05it/s]\u001b[A\n",
      "batch:  71%|███████▏  | 85/119 [01:21<00:32,  1.05it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 86/119 [01:22<00:31,  1.04it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 87/119 [01:22<00:30,  1.04it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 88/119 [01:23<00:29,  1.04it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 89/119 [01:24<00:28,  1.05it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 90/119 [01:25<00:27,  1.05it/s]\u001b[A\n",
      "batch:  76%|███████▋  | 91/119 [01:26<00:26,  1.05it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 92/119 [01:27<00:25,  1.05it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 93/119 [01:28<00:24,  1.05it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 94/119 [01:29<00:23,  1.04it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 95/119 [01:30<00:22,  1.04it/s]\u001b[A\n",
      "batch:  81%|████████  | 96/119 [01:31<00:22,  1.04it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 97/119 [01:32<00:21,  1.04it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 98/119 [01:33<00:20,  1.05it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 99/119 [01:34<00:19,  1.05it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 100/119 [01:35<00:18,  1.05it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 101/119 [01:36<00:17,  1.05it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 102/119 [01:37<00:16,  1.05it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 103/119 [01:38<00:15,  1.05it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 104/119 [01:39<00:14,  1.05it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 105/119 [01:40<00:13,  1.05it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 106/119 [01:41<00:12,  1.05it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 107/119 [01:42<00:11,  1.04it/s]\u001b[A\n",
      "batch:  91%|█████████ | 108/119 [01:43<00:10,  1.04it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 109/119 [01:43<00:09,  1.04it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 110/119 [01:44<00:08,  1.04it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 111/119 [01:45<00:07,  1.04it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 112/119 [01:46<00:06,  1.04it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 113/119 [01:47<00:05,  1.05it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 114/119 [01:48<00:04,  1.05it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 115/119 [01:49<00:03,  1.05it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 116/119 [01:50<00:02,  1.05it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 117/119 [01:51<00:01,  1.04it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 118/119 [01:52<00:00,  1.04it/s]\u001b[A\n",
      "batch: 100%|██████████| 119/119 [01:53<00:00,  1.14it/s]\u001b[A\n",
      "Epoch:  50%|█████     | 2/4 [03:46<03:46, 113.12s/it]   \u001b[A\n",
      "batch:   0%|          | 0/119 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.8485440526689803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   1%|          | 1/119 [00:00<01:52,  1.05it/s]\u001b[A\n",
      "batch:   2%|▏         | 2/119 [00:01<01:51,  1.05it/s]\u001b[A\n",
      "batch:   3%|▎         | 3/119 [00:02<01:50,  1.05it/s]\u001b[A\n",
      "batch:   3%|▎         | 4/119 [00:03<01:49,  1.05it/s]\u001b[A\n",
      "batch:   4%|▍         | 5/119 [00:04<01:48,  1.05it/s]\u001b[A\n",
      "batch:   5%|▌         | 6/119 [00:05<01:47,  1.05it/s]\u001b[A\n",
      "batch:   6%|▌         | 7/119 [00:06<01:46,  1.05it/s]\u001b[A\n",
      "batch:   7%|▋         | 8/119 [00:07<01:45,  1.05it/s]\u001b[A\n",
      "batch:   8%|▊         | 9/119 [00:08<01:44,  1.05it/s]\u001b[A\n",
      "batch:   8%|▊         | 10/119 [00:09<01:44,  1.05it/s]\u001b[A\n",
      "batch:   9%|▉         | 11/119 [00:10<01:42,  1.05it/s]\u001b[A\n",
      "batch:  10%|█         | 12/119 [00:11<01:42,  1.05it/s]\u001b[A\n",
      "batch:  11%|█         | 13/119 [00:12<01:41,  1.05it/s]\u001b[A\n",
      "batch:  12%|█▏        | 14/119 [00:13<01:40,  1.05it/s]\u001b[A\n",
      "batch:  13%|█▎        | 15/119 [00:14<01:39,  1.05it/s]\u001b[A\n",
      "batch:  13%|█▎        | 16/119 [00:15<01:38,  1.05it/s]\u001b[A\n",
      "batch:  14%|█▍        | 17/119 [00:16<01:37,  1.04it/s]\u001b[A\n",
      "batch:  15%|█▌        | 18/119 [00:17<01:36,  1.05it/s]\u001b[A\n",
      "batch:  16%|█▌        | 19/119 [00:18<01:35,  1.05it/s]\u001b[A\n",
      "batch:  17%|█▋        | 20/119 [00:19<01:34,  1.05it/s]\u001b[A\n",
      "batch:  18%|█▊        | 21/119 [00:20<01:32,  1.05it/s]\u001b[A\n",
      "batch:  18%|█▊        | 22/119 [00:20<01:32,  1.05it/s]\u001b[A\n",
      "batch:  19%|█▉        | 23/119 [00:21<01:31,  1.05it/s]\u001b[A\n",
      "batch:  20%|██        | 24/119 [00:22<01:30,  1.05it/s]\u001b[A\n",
      "batch:  21%|██        | 25/119 [00:23<01:29,  1.05it/s]\u001b[A\n",
      "batch:  22%|██▏       | 26/119 [00:24<01:28,  1.05it/s]\u001b[A\n",
      "batch:  23%|██▎       | 27/119 [00:25<01:27,  1.05it/s]\u001b[A\n",
      "batch:  24%|██▎       | 28/119 [00:26<01:27,  1.04it/s]\u001b[A\n",
      "batch:  24%|██▍       | 29/119 [00:27<01:26,  1.04it/s]\u001b[A\n",
      "batch:  25%|██▌       | 30/119 [00:28<01:25,  1.05it/s]\u001b[A\n",
      "batch:  26%|██▌       | 31/119 [00:29<01:24,  1.04it/s]\u001b[A\n",
      "batch:  27%|██▋       | 32/119 [00:30<01:23,  1.04it/s]\u001b[A\n",
      "batch:  28%|██▊       | 33/119 [00:31<01:22,  1.05it/s]\u001b[A\n",
      "batch:  29%|██▊       | 34/119 [00:32<01:21,  1.05it/s]\u001b[A\n",
      "batch:  29%|██▉       | 35/119 [00:33<01:20,  1.05it/s]\u001b[A\n",
      "batch:  30%|███       | 36/119 [00:34<01:19,  1.05it/s]\u001b[A\n",
      "batch:  31%|███       | 37/119 [00:35<01:18,  1.05it/s]\u001b[A\n",
      "batch:  32%|███▏      | 38/119 [00:36<01:17,  1.04it/s]\u001b[A\n",
      "batch:  33%|███▎      | 39/119 [00:37<01:16,  1.04it/s]\u001b[A\n",
      "batch:  34%|███▎      | 40/119 [00:38<01:15,  1.04it/s]\u001b[A\n",
      "batch:  34%|███▍      | 41/119 [00:39<01:14,  1.05it/s]\u001b[A\n",
      "batch:  35%|███▌      | 42/119 [00:40<01:13,  1.05it/s]\u001b[A\n",
      "batch:  36%|███▌      | 43/119 [00:41<01:12,  1.05it/s]\u001b[A\n",
      "batch:  37%|███▋      | 44/119 [00:41<01:11,  1.05it/s]\u001b[A\n",
      "batch:  38%|███▊      | 45/119 [00:42<01:10,  1.05it/s]\u001b[A\n",
      "batch:  39%|███▊      | 46/119 [00:43<01:09,  1.05it/s]\u001b[A\n",
      "batch:  39%|███▉      | 47/119 [00:44<01:08,  1.05it/s]\u001b[A\n",
      "batch:  40%|████      | 48/119 [00:45<01:07,  1.05it/s]\u001b[A\n",
      "batch:  41%|████      | 49/119 [00:46<01:07,  1.04it/s]\u001b[A\n",
      "batch:  42%|████▏     | 50/119 [00:47<01:06,  1.04it/s]\u001b[A\n",
      "batch:  43%|████▎     | 51/119 [00:48<01:05,  1.04it/s]\u001b[A\n",
      "batch:  44%|████▎     | 52/119 [00:49<01:04,  1.04it/s]\u001b[A\n",
      "batch:  45%|████▍     | 53/119 [00:50<01:03,  1.04it/s]\u001b[A\n",
      "batch:  45%|████▌     | 54/119 [00:51<01:02,  1.04it/s]\u001b[A\n",
      "batch:  46%|████▌     | 55/119 [00:52<01:01,  1.04it/s]\u001b[A\n",
      "batch:  47%|████▋     | 56/119 [00:53<01:00,  1.04it/s]\u001b[A\n",
      "batch:  48%|████▊     | 57/119 [00:54<00:59,  1.04it/s]\u001b[A\n",
      "batch:  49%|████▊     | 58/119 [00:55<00:58,  1.04it/s]\u001b[A\n",
      "batch:  50%|████▉     | 59/119 [00:56<00:57,  1.04it/s]\u001b[A\n",
      "batch:  50%|█████     | 60/119 [00:57<00:56,  1.04it/s]\u001b[A\n",
      "batch:  51%|█████▏    | 61/119 [00:58<00:55,  1.04it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 62/119 [00:59<00:54,  1.04it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 63/119 [01:00<00:53,  1.04it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 64/119 [01:01<00:52,  1.05it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 65/119 [01:02<00:51,  1.05it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 66/119 [01:03<00:50,  1.05it/s]\u001b[A\n",
      "batch:  56%|█████▋    | 67/119 [01:04<00:49,  1.05it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 68/119 [01:04<00:48,  1.05it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 69/119 [01:05<00:47,  1.05it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 70/119 [01:06<00:46,  1.05it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 71/119 [01:07<00:45,  1.05it/s]\u001b[A\n",
      "batch:  61%|██████    | 72/119 [01:08<00:44,  1.05it/s]\u001b[A\n",
      "batch:  61%|██████▏   | 73/119 [01:09<00:44,  1.04it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 74/119 [01:10<00:43,  1.04it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 75/119 [01:11<00:42,  1.04it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 76/119 [01:12<00:41,  1.04it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 77/119 [01:13<00:40,  1.04it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 78/119 [01:14<00:39,  1.04it/s]\u001b[A\n",
      "batch:  66%|██████▋   | 79/119 [01:15<00:38,  1.04it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 80/119 [01:16<00:37,  1.04it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 81/119 [01:17<00:36,  1.04it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 82/119 [01:18<00:35,  1.04it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 83/119 [01:19<00:34,  1.04it/s]\u001b[A\n",
      "batch:  71%|███████   | 84/119 [01:20<00:33,  1.04it/s]\u001b[A\n",
      "batch:  71%|███████▏  | 85/119 [01:21<00:32,  1.04it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 86/119 [01:22<00:31,  1.04it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 87/119 [01:23<00:30,  1.04it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 88/119 [01:24<00:29,  1.04it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 89/119 [01:25<00:28,  1.04it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 90/119 [01:26<00:27,  1.05it/s]\u001b[A\n",
      "batch:  76%|███████▋  | 91/119 [01:27<00:26,  1.05it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 92/119 [01:27<00:25,  1.05it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 93/119 [01:28<00:24,  1.05it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 94/119 [01:29<00:23,  1.05it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5e8871ab28e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Update parameters and take a step using the computed gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "step = 0\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    \n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    # Train the data for one epoch\n",
    "    for batch in tqdm(train_dataloader,desc='batch',leave=False):\n",
    "        step = step + 1\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        train_loss_set.append(loss.item()) \n",
    "        vis.plot('loss', 'train_loss', 'Loss',step,loss.item())   \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        \n",
    "        \n",
    "    # Validation\n",
    "\n",
    "    if(step%500==0):\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "                logits = output[0]\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "        vis.plot('accuracy', 'val_acc', 'val_acc',step,eval_accuracy/nb_eval_steps)\n",
    "        torch.save(model, 'random_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.comments.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df2.comments.values.shape[0]):\n",
    "    df2.comments.values[i] = df2.comments.values[i][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df2.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 512)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "# prediction_labels = torch.tensor(labels)\n",
    "  \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(prediction_dataloader,desc='batch',leave=False):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = output[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( predictions, open( \"preds/xlnet-preds-train.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n"
     ]
    }
   ],
   "source": [
    "main_preds = []\n",
    "for i in range(len(predictions)):\n",
    "    main_preds += list(np.argmax(predictions[i], axis=1))\n",
    "    \n",
    "print(len(main_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 11,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 11,\n",
       " 16,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 11,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 14,\n",
       " 3,\n",
       " 9,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 11,\n",
       " 3,\n",
       " 13,\n",
       " 17,\n",
       " 18,\n",
       " 3,\n",
       " 17,\n",
       " 2,\n",
       " 15,\n",
       " 7,\n",
       " 17,\n",
       " 12,\n",
       " 17,\n",
       " 16,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 10,\n",
       " 11,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 3,\n",
       " 0,\n",
       " 10,\n",
       " 5,\n",
       " 10,\n",
       " 19,\n",
       " 9,\n",
       " 13,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 18,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 17,\n",
       " 6,\n",
       " 18,\n",
       " 12,\n",
       " 0,\n",
       " 13,\n",
       " 6,\n",
       " 11,\n",
       " 16,\n",
       " 17,\n",
       " 2,\n",
       " 12,\n",
       " 1,\n",
       " 13,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 7,\n",
       " 12,\n",
       " 9,\n",
       " 19,\n",
       " 4,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 0,\n",
       " 14,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 13,\n",
       " 6,\n",
       " 6,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 5,\n",
       " 5,\n",
       " 11,\n",
       " 9,\n",
       " 17,\n",
       " 7,\n",
       " 2,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 18,\n",
       " 13,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 18,\n",
       " 18,\n",
       " 4,\n",
       " 10,\n",
       " 18,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 4,\n",
       " 14,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 19,\n",
       " 1,\n",
       " 16,\n",
       " 18,\n",
       " 16,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 11,\n",
       " 16,\n",
       " 7,\n",
       " 6,\n",
       " 14,\n",
       " 6,\n",
       " 18,\n",
       " 4,\n",
       " 11,\n",
       " 13,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 17,\n",
       " 11,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 18,\n",
       " 18,\n",
       " 7,\n",
       " 12,\n",
       " 2,\n",
       " 1,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 3,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 13,\n",
       " 11,\n",
       " 18,\n",
       " 1,\n",
       " 18,\n",
       " 4,\n",
       " 5,\n",
       " 11,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 8,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 0,\n",
       " 17,\n",
       " 7,\n",
       " 2,\n",
       " 15,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 8,\n",
       " 11,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 9,\n",
       " 19,\n",
       " 16,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 16,\n",
       " 12,\n",
       " 18,\n",
       " 2,\n",
       " 7,\n",
       " 17,\n",
       " 7,\n",
       " 19,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 13,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 19,\n",
       " 13,\n",
       " 11,\n",
       " 17,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 12,\n",
       " 17,\n",
       " 18,\n",
       " 15,\n",
       " 5,\n",
       " 6,\n",
       " 17,\n",
       " 17,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 18,\n",
       " 19,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 16,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 14,\n",
       " 5,\n",
       " 12,\n",
       " 4,\n",
       " 16,\n",
       " 16,\n",
       " 12,\n",
       " 11,\n",
       " 6,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 0,\n",
       " 19,\n",
       " 8,\n",
       " 5,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 11,\n",
       " 5,\n",
       " 18,\n",
       " 9,\n",
       " 0,\n",
       " 11,\n",
       " 15,\n",
       " 2,\n",
       " 13,\n",
       " 16,\n",
       " 9,\n",
       " 13,\n",
       " 4,\n",
       " 17,\n",
       " 13,\n",
       " 11,\n",
       " 8,\n",
       " 14,\n",
       " 3,\n",
       " 13,\n",
       " 18,\n",
       " 19,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 6,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 4,\n",
       " 0,\n",
       " 17,\n",
       " 19,\n",
       " 0,\n",
       " 7,\n",
       " 18,\n",
       " 3,\n",
       " 12,\n",
       " 6,\n",
       " 19,\n",
       " 13,\n",
       " 8,\n",
       " 15,\n",
       " 10,\n",
       " 16,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 6,\n",
       " 11,\n",
       " 9,\n",
       " 12,\n",
       " 7,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 4,\n",
       " 17,\n",
       " 4,\n",
       " 16,\n",
       " 0,\n",
       " 7,\n",
       " 18,\n",
       " 11,\n",
       " 18,\n",
       " 3,\n",
       " 17,\n",
       " 1,\n",
       " 10,\n",
       " 3,\n",
       " 18,\n",
       " 6,\n",
       " 16,\n",
       " 11,\n",
       " 4,\n",
       " 6,\n",
       " 18,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 11,\n",
       " 11,\n",
       " 6,\n",
       " 14,\n",
       " 15,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 18,\n",
       " 15,\n",
       " 18,\n",
       " 3,\n",
       " 14,\n",
       " 1,\n",
       " 13,\n",
       " 8,\n",
       " 15,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 10,\n",
       " 16,\n",
       " 9,\n",
       " 16,\n",
       " 11,\n",
       " 2,\n",
       " 8,\n",
       " 16,\n",
       " 16,\n",
       " 4,\n",
       " 7,\n",
       " 12,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 13,\n",
       " 7,\n",
       " 3,\n",
       " 15,\n",
       " 16,\n",
       " 13,\n",
       " 17,\n",
       " 14,\n",
       " 18,\n",
       " 16,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 17,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 17,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 15,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 19,\n",
       " 10,\n",
       " 0,\n",
       " 2,\n",
       " 14,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 12,\n",
       " 17,\n",
       " 15,\n",
       " 12,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 16,\n",
       " 10,\n",
       " 17,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 15,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 16,\n",
       " 2,\n",
       " 13,\n",
       " 8,\n",
       " 13,\n",
       " 0,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 7,\n",
       " 15,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 1,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 3,\n",
       " 19,\n",
       " 17,\n",
       " 7,\n",
       " 11,\n",
       " 4,\n",
       " 11,\n",
       " 19,\n",
       " 15,\n",
       " 3,\n",
       " 14,\n",
       " 17,\n",
       " 11,\n",
       " 17,\n",
       " 11,\n",
       " 12,\n",
       " 2,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 13,\n",
       " 3,\n",
       " 7,\n",
       " 16,\n",
       " 13,\n",
       " 15,\n",
       " 6,\n",
       " 13,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 15,\n",
       " 13,\n",
       " 16,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 18,\n",
       " 17,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 13,\n",
       " 6,\n",
       " 8,\n",
       " 17,\n",
       " 8,\n",
       " 0,\n",
       " 11,\n",
       " 15,\n",
       " 17,\n",
       " 4,\n",
       " 5,\n",
       " 11,\n",
       " 11,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 10,\n",
       " 17,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 17,\n",
       " 7,\n",
       " 12,\n",
       " 4,\n",
       " 16,\n",
       " 14,\n",
       " 10,\n",
       " 1,\n",
       " 12,\n",
       " 6,\n",
       " 3,\n",
       " 13,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 17,\n",
       " 13,\n",
       " 12,\n",
       " 4,\n",
       " 16,\n",
       " 18,\n",
       " 18,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 16,\n",
       " 12,\n",
       " 0,\n",
       " 10,\n",
       " 16,\n",
       " 8,\n",
       " 3,\n",
       " 11,\n",
       " 2,\n",
       " 16,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 10,\n",
       " 15,\n",
       " 7,\n",
       " 17,\n",
       " 8,\n",
       " 16,\n",
       " 15,\n",
       " 17,\n",
       " 14,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 10,\n",
       " 11,\n",
       " 3,\n",
       " 11,\n",
       " 17,\n",
       " 18,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 13,\n",
       " 13,\n",
       " 0,\n",
       " 11,\n",
       " 4,\n",
       " 3,\n",
       " 13,\n",
       " 10,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 7,\n",
       " 16,\n",
       " 11,\n",
       " 11,\n",
       " 6,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 14,\n",
       " 16,\n",
       " 10,\n",
       " 4,\n",
       " 15,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 19,\n",
       " 6,\n",
       " 18,\n",
       " 19,\n",
       " 1,\n",
       " 10,\n",
       " 6,\n",
       " 2,\n",
       " 15,\n",
       " 1,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 16,\n",
       " 8,\n",
       " 9,\n",
       " 19,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 18,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 15,\n",
       " 4,\n",
       " 12,\n",
       " 19,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 15,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 12,\n",
       " 7,\n",
       " 11,\n",
       " 15,\n",
       " 1,\n",
       " 16,\n",
       " 4,\n",
       " 11,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 17,\n",
       " 12,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 17,\n",
       " 15,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 13,\n",
       " 6,\n",
       " 18,\n",
       " 11,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 13,\n",
       " 10,\n",
       " 18,\n",
       " 7,\n",
       " 15,\n",
       " 12,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 19,\n",
       " 8,\n",
       " 13,\n",
       " 19,\n",
       " 5,\n",
       " 14,\n",
       " 17,\n",
       " 10,\n",
       " 5,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 11,\n",
       " 16,\n",
       " 11,\n",
       " 19,\n",
       " 11,\n",
       " 18,\n",
       " 3,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 13,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 11,\n",
       " 1,\n",
       " 15,\n",
       " 15,\n",
       " 19,\n",
       " 16,\n",
       " 12,\n",
       " 17,\n",
       " 4,\n",
       " 19,\n",
       " 15,\n",
       " 16,\n",
       " 3,\n",
       " 2,\n",
       " 17,\n",
       " 3,\n",
       " 13,\n",
       " 18,\n",
       " 14,\n",
       " 4,\n",
       " 11,\n",
       " 18,\n",
       " 6,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 11,\n",
       " 11,\n",
       " 15,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 16,\n",
       " 11,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 15,\n",
       " 4,\n",
       " 6,\n",
       " 15,\n",
       " 15,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 13,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 11,\n",
       " 19,\n",
       " 11,\n",
       " 12,\n",
       " 18,\n",
       " 5,\n",
       " 15,\n",
       " 11,\n",
       " 11,\n",
       " 14,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 10,\n",
       " 19,\n",
       " 16,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 5,\n",
       " 16,\n",
       " 6,\n",
       " 19,\n",
       " 12,\n",
       " 17,\n",
       " 6,\n",
       " 17,\n",
       " 14,\n",
       " 7,\n",
       " 5,\n",
       " 16,\n",
       " 10,\n",
       " 6,\n",
       " 15,\n",
       " 11,\n",
       " 19,\n",
       " 16,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 18,\n",
       " 5,\n",
       " 2,\n",
       " 14,\n",
       " 17,\n",
       " 7,\n",
       " 11,\n",
       " 13,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 19,\n",
       " 13,\n",
       " 18,\n",
       " 11,\n",
       " 11,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 13,\n",
       " 0,\n",
       " 10,\n",
       " 11,\n",
       " 1,\n",
       " 8,\n",
       " 12,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 12,\n",
       " 7,\n",
       " 6,\n",
       " 11,\n",
       " 4,\n",
       " 5,\n",
       " 16,\n",
       " 5,\n",
       " 14,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 4,\n",
       " 6,\n",
       " 11,\n",
       " 19,\n",
       " 15,\n",
       " 8,\n",
       " 17,\n",
       " 9,\n",
       " 2,\n",
       " 19,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 18,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 3,\n",
       " 17,\n",
       " 1,\n",
       " 13,\n",
       " 11,\n",
       " 11,\n",
       " 18,\n",
       " 7,\n",
       " 4,\n",
       " 19,\n",
       " 8,\n",
       " 4,\n",
       " 14,\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-50f48d85ffc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mapping' is not defined"
     ]
    }
   ],
   "source": [
    "test_preds = pd.DataFrame()\n",
    "test_preds['Id'] = df2['id']\n",
    "test_preds['Category'] = mapping[main_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29995</td>\n",
       "      <td>29995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29996</td>\n",
       "      <td>29996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29997</td>\n",
       "      <td>29997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29998</td>\n",
       "      <td>29998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29999</td>\n",
       "      <td>29999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id\n",
       "0          0\n",
       "1          1\n",
       "2          2\n",
       "3          3\n",
       "4          4\n",
       "...      ...\n",
       "29995  29995\n",
       "29996  29996\n",
       "29997  29997\n",
       "29998  29998\n",
       "29999  29999\n",
       "\n",
       "[30000 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='test2.csv' target='_blank'>test2.csv</a><br>"
      ],
      "text/plain": [
       "/network/home/penmetss/comp551/test2.csv"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.to_csv(\"test2.csv\", index=False)\n",
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(a.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('models/xlnet_model_final_standardEp1.5_fulldata.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
