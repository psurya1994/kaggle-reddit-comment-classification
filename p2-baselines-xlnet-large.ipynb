{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "\n",
    "class VisdomLinePlotter(object):\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    def __init__(self, env_name='main'):\n",
    "        self.viz = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "                legend=[split_name],\n",
    "                title=title_name,\n",
    "                xlabel='Epochs',\n",
    "                ylabel=var_name\n",
    "            ))\n",
    "        else:\n",
    "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = 'append')\n",
    "            \n",
    "            \n",
    "    \n",
    "vis = VisdomLinePlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from pytorch_transformers import AdamW\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "# device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6670</td>\n",
       "      <td>6670</td>\n",
       "      <td>Yeah but euron's about to bring cersei tyrion ...</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49567</td>\n",
       "      <td>49567</td>\n",
       "      <td>All of his videos are sarcastic and funny...hi...</td>\n",
       "      <td>conspiracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50796</td>\n",
       "      <td>50796</td>\n",
       "      <td>I love those scenes but it wouldn't have made ...</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22310</td>\n",
       "      <td>22310</td>\n",
       "      <td>You do get a smidge of hp for every point of c...</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54037</td>\n",
       "      <td>54037</td>\n",
       "      <td>New MMORPG lets you play as someone playing a ...</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           comments     subreddits\n",
       "6670    6670  Yeah but euron's about to bring cersei tyrion ...  gameofthrones\n",
       "49567  49567  All of his videos are sarcastic and funny...hi...     conspiracy\n",
       "50796  50796  I love those scenes but it wouldn't have made ...         movies\n",
       "22310  22310  You do get a smidge of hp for every point of c...            wow\n",
       "54037  54037  New MMORPG lets you play as someone playing a ...            wow"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_train.csv')\n",
    "df2 = pd.read_csv('reddit_test.csv')\n",
    "df = df.sample(1000, random_state=1).copy()\n",
    "df2 = df2.sample(1000, random_state=1).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'], mapping = df['subreddits'].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "labels = df.category_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁yeah', '▁but', '▁euro', 'n', \"'\", 's', '▁about', '▁to', '▁bring', '▁', 'cer', 's', 'ei', '▁', 'ty', 'rion', '▁as', '▁a', '▁gift', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.05)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 2\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=1024, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-large-cased\", num_labels=20)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "batch:   0%|          | 0/475 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   0%|          | 1/475 [00:00<04:56,  1.60it/s]\u001b[A\n",
      "batch:   0%|          | 2/475 [00:01<04:54,  1.61it/s]\u001b[A\n",
      "batch:   1%|          | 3/475 [00:01<04:54,  1.60it/s]\u001b[A\n",
      "batch:   1%|          | 4/475 [00:02<04:53,  1.61it/s]\u001b[A\n",
      "batch:   1%|          | 5/475 [00:03<04:53,  1.60it/s]\u001b[A\n",
      "batch:   1%|▏         | 6/475 [00:03<04:52,  1.60it/s]\u001b[A\n",
      "batch:   1%|▏         | 7/475 [00:04<04:49,  1.62it/s]\u001b[A\n",
      "batch:   2%|▏         | 8/475 [00:04<04:49,  1.61it/s]\u001b[A\n",
      "batch:   2%|▏         | 9/475 [00:05<04:47,  1.62it/s]\u001b[A\n",
      "batch:   2%|▏         | 10/475 [00:06<04:46,  1.62it/s]\u001b[A\n",
      "batch:   2%|▏         | 11/475 [00:06<04:46,  1.62it/s]\u001b[A\n",
      "batch:   3%|▎         | 12/475 [00:07<04:45,  1.62it/s]\u001b[A\n",
      "batch:   3%|▎         | 13/475 [00:08<04:46,  1.61it/s]\u001b[A\n",
      "batch:   3%|▎         | 14/475 [00:08<04:45,  1.61it/s]\u001b[A\n",
      "batch:   3%|▎         | 15/475 [00:09<04:45,  1.61it/s]\u001b[A\n",
      "batch:   3%|▎         | 16/475 [00:09<04:45,  1.61it/s]\u001b[A\n",
      "batch:   4%|▎         | 17/475 [00:10<04:45,  1.60it/s]\u001b[A\n",
      "batch:   4%|▍         | 18/475 [00:11<04:44,  1.61it/s]\u001b[A\n",
      "batch:   4%|▍         | 19/475 [00:11<04:43,  1.61it/s]\u001b[A\n",
      "batch:   4%|▍         | 20/475 [00:12<04:41,  1.62it/s]\u001b[A\n",
      "batch:   4%|▍         | 21/475 [00:13<04:40,  1.62it/s]\u001b[A\n",
      "batch:   5%|▍         | 22/475 [00:13<04:39,  1.62it/s]\u001b[A\n",
      "batch:   5%|▍         | 23/475 [00:14<04:39,  1.62it/s]\u001b[A\n",
      "batch:   5%|▌         | 24/475 [00:14<04:40,  1.61it/s]\u001b[A\n",
      "batch:   5%|▌         | 25/475 [00:15<04:38,  1.62it/s]\u001b[A\n",
      "batch:   5%|▌         | 26/475 [00:16<04:41,  1.60it/s]\u001b[A\n",
      "batch:   6%|▌         | 27/475 [00:16<04:41,  1.59it/s]\u001b[A\n",
      "batch:   6%|▌         | 28/475 [00:17<04:39,  1.60it/s]\u001b[A\n",
      "batch:   6%|▌         | 29/475 [00:18<04:39,  1.60it/s]\u001b[A\n",
      "batch:   6%|▋         | 30/475 [00:18<04:36,  1.61it/s]\u001b[A\n",
      "batch:   7%|▋         | 31/475 [00:19<04:37,  1.60it/s]\u001b[A\n",
      "batch:   7%|▋         | 32/475 [00:19<04:36,  1.60it/s]\u001b[A\n",
      "batch:   7%|▋         | 33/475 [00:20<04:36,  1.60it/s]\u001b[A\n",
      "batch:   7%|▋         | 34/475 [00:21<04:35,  1.60it/s]\u001b[A\n",
      "batch:   7%|▋         | 35/475 [00:21<04:34,  1.60it/s]\u001b[A\n",
      "batch:   8%|▊         | 36/475 [00:22<04:33,  1.61it/s]\u001b[A\n",
      "batch:   8%|▊         | 37/475 [00:22<04:30,  1.62it/s]\u001b[A\n",
      "batch:   8%|▊         | 38/475 [00:23<04:29,  1.62it/s]\u001b[A\n",
      "batch:   8%|▊         | 39/475 [00:24<04:31,  1.61it/s]\u001b[A\n",
      "batch:   8%|▊         | 40/475 [00:24<04:28,  1.62it/s]\u001b[A\n",
      "batch:   9%|▊         | 41/475 [00:25<04:26,  1.63it/s]\u001b[A\n",
      "batch:   9%|▉         | 42/475 [00:26<04:27,  1.62it/s]\u001b[A\n",
      "batch:   9%|▉         | 43/475 [00:26<04:25,  1.63it/s]\u001b[A\n",
      "batch:   9%|▉         | 44/475 [00:27<04:23,  1.63it/s]\u001b[A\n",
      "batch:   9%|▉         | 45/475 [00:27<04:23,  1.63it/s]\u001b[A\n",
      "batch:  10%|▉         | 46/475 [00:28<04:24,  1.62it/s]\u001b[A\n",
      "batch:  10%|▉         | 47/475 [00:29<04:23,  1.62it/s]\u001b[A\n",
      "batch:  10%|█         | 48/475 [00:29<04:25,  1.61it/s]\u001b[A\n",
      "batch:  10%|█         | 49/475 [00:30<04:24,  1.61it/s]\u001b[A\n",
      "batch:  11%|█         | 50/475 [00:31<04:23,  1.61it/s]\u001b[A\n",
      "batch:  11%|█         | 51/475 [00:31<04:21,  1.62it/s]\u001b[A\n",
      "batch:  11%|█         | 52/475 [00:32<04:21,  1.62it/s]\u001b[A\n",
      "batch:  11%|█         | 53/475 [00:32<04:20,  1.62it/s]\u001b[A\n",
      "batch:  11%|█▏        | 54/475 [00:33<04:18,  1.63it/s]\u001b[A\n",
      "batch:  12%|█▏        | 55/475 [00:34<04:18,  1.62it/s]\u001b[A\n",
      "batch:  12%|█▏        | 56/475 [00:34<04:16,  1.63it/s]\u001b[A\n",
      "batch:  12%|█▏        | 57/475 [00:35<04:16,  1.63it/s]\u001b[A\n",
      "batch:  12%|█▏        | 58/475 [00:35<04:18,  1.62it/s]\u001b[A\n",
      "batch:  12%|█▏        | 59/475 [00:36<04:19,  1.60it/s]\u001b[A\n",
      "batch:  13%|█▎        | 60/475 [00:37<04:17,  1.61it/s]\u001b[A\n",
      "batch:  13%|█▎        | 61/475 [00:37<04:18,  1.60it/s]\u001b[A\n",
      "batch:  13%|█▎        | 62/475 [00:38<04:16,  1.61it/s]\u001b[A\n",
      "batch:  13%|█▎        | 63/475 [00:39<04:15,  1.61it/s]\u001b[A\n",
      "batch:  13%|█▎        | 64/475 [00:39<04:14,  1.61it/s]\u001b[A\n",
      "batch:  14%|█▎        | 65/475 [00:40<04:15,  1.61it/s]\u001b[A\n",
      "batch:  14%|█▍        | 66/475 [00:40<04:13,  1.61it/s]\u001b[A\n",
      "batch:  14%|█▍        | 67/475 [00:41<04:12,  1.62it/s]\u001b[A\n",
      "batch:  14%|█▍        | 68/475 [00:42<04:11,  1.62it/s]\u001b[A\n",
      "batch:  15%|█▍        | 69/475 [00:42<04:11,  1.62it/s]\u001b[A\n",
      "batch:  15%|█▍        | 70/475 [00:43<04:10,  1.62it/s]\u001b[A\n",
      "batch:  15%|█▍        | 71/475 [00:44<04:11,  1.61it/s]\u001b[A\n",
      "batch:  15%|█▌        | 72/475 [00:44<04:09,  1.61it/s]\u001b[A\n",
      "batch:  15%|█▌        | 73/475 [00:45<04:07,  1.62it/s]\u001b[A\n",
      "batch:  16%|█▌        | 74/475 [00:45<04:06,  1.63it/s]\u001b[A\n",
      "batch:  16%|█▌        | 75/475 [00:46<04:05,  1.63it/s]\u001b[A\n",
      "batch:  16%|█▌        | 76/475 [00:47<04:07,  1.61it/s]\u001b[A\n",
      "batch:  16%|█▌        | 77/475 [00:47<04:05,  1.62it/s]\u001b[A\n",
      "batch:  16%|█▋        | 78/475 [00:48<04:03,  1.63it/s]\u001b[A\n",
      "batch:  17%|█▋        | 79/475 [00:48<04:02,  1.63it/s]\u001b[A\n",
      "batch:  17%|█▋        | 80/475 [00:49<04:02,  1.63it/s]\u001b[A\n",
      "batch:  17%|█▋        | 81/475 [00:50<04:02,  1.63it/s]\u001b[A\n",
      "batch:  17%|█▋        | 82/475 [00:50<04:04,  1.61it/s]\u001b[A\n",
      "batch:  17%|█▋        | 83/475 [00:51<04:02,  1.62it/s]\u001b[A\n",
      "batch:  18%|█▊        | 84/475 [00:52<04:02,  1.61it/s]\u001b[A\n",
      "batch:  18%|█▊        | 85/475 [00:52<04:00,  1.62it/s]\u001b[A\n",
      "batch:  18%|█▊        | 86/475 [00:53<04:00,  1.61it/s]\u001b[A\n",
      "batch:  18%|█▊        | 87/475 [00:53<04:00,  1.62it/s]\u001b[A\n",
      "batch:  19%|█▊        | 88/475 [00:54<03:57,  1.63it/s]\u001b[A\n",
      "batch:  19%|█▊        | 89/475 [00:55<03:59,  1.61it/s]\u001b[A\n",
      "batch:  19%|█▉        | 90/475 [00:55<04:00,  1.60it/s]\u001b[A\n",
      "batch:  19%|█▉        | 91/475 [00:56<04:00,  1.59it/s]\u001b[A\n",
      "batch:  19%|█▉        | 92/475 [00:57<03:59,  1.60it/s]\u001b[A\n",
      "batch:  20%|█▉        | 93/475 [00:57<03:57,  1.61it/s]\u001b[A\n",
      "batch:  20%|█▉        | 94/475 [00:58<03:55,  1.62it/s]\u001b[A\n",
      "batch:  20%|██        | 95/475 [00:58<03:55,  1.62it/s]\u001b[A\n",
      "batch:  20%|██        | 96/475 [00:59<03:53,  1.62it/s]\u001b[A\n",
      "batch:  20%|██        | 97/475 [01:00<03:52,  1.63it/s]\u001b[A\n",
      "batch:  21%|██        | 98/475 [01:00<03:51,  1.63it/s]\u001b[A\n",
      "batch:  21%|██        | 99/475 [01:01<03:51,  1.62it/s]\u001b[A\n",
      "batch:  21%|██        | 100/475 [01:01<03:52,  1.61it/s]\u001b[A\n",
      "batch:  21%|██▏       | 101/475 [01:02<03:50,  1.62it/s]\u001b[A\n",
      "batch:  21%|██▏       | 102/475 [01:03<03:51,  1.61it/s]\u001b[A\n",
      "batch:  22%|██▏       | 103/475 [01:03<03:51,  1.61it/s]\u001b[A\n",
      "batch:  22%|██▏       | 104/475 [01:04<03:51,  1.60it/s]\u001b[A\n",
      "batch:  22%|██▏       | 105/475 [01:05<03:50,  1.60it/s]\u001b[A\n",
      "batch:  22%|██▏       | 106/475 [01:05<03:48,  1.61it/s]\u001b[A\n",
      "batch:  23%|██▎       | 107/475 [01:06<03:47,  1.62it/s]\u001b[A\n",
      "batch:  23%|██▎       | 108/475 [01:06<03:45,  1.63it/s]\u001b[A\n",
      "batch:  23%|██▎       | 109/475 [01:07<03:46,  1.62it/s]\u001b[A\n",
      "batch:  23%|██▎       | 110/475 [01:08<03:45,  1.62it/s]\u001b[A\n",
      "batch:  23%|██▎       | 111/475 [01:08<03:43,  1.63it/s]\u001b[A\n",
      "batch:  24%|██▎       | 112/475 [01:09<03:44,  1.62it/s]\u001b[A\n",
      "batch:  24%|██▍       | 113/475 [01:09<03:44,  1.61it/s]\u001b[A\n",
      "batch:  24%|██▍       | 114/475 [01:10<03:42,  1.62it/s]\u001b[A\n",
      "batch:  24%|██▍       | 115/475 [01:11<03:40,  1.63it/s]\u001b[A\n",
      "batch:  24%|██▍       | 116/475 [01:11<03:41,  1.62it/s]\u001b[A\n",
      "batch:  25%|██▍       | 117/475 [01:12<03:41,  1.62it/s]\u001b[A\n",
      "batch:  25%|██▍       | 118/475 [01:13<03:40,  1.62it/s]\u001b[A\n",
      "batch:  25%|██▌       | 119/475 [01:13<03:39,  1.62it/s]\u001b[A\n",
      "batch:  25%|██▌       | 120/475 [01:14<03:40,  1.61it/s]\u001b[A\n",
      "batch:  25%|██▌       | 121/475 [01:14<03:40,  1.61it/s]\u001b[A\n",
      "batch:  26%|██▌       | 122/475 [01:15<03:38,  1.61it/s]\u001b[A\n",
      "batch:  26%|██▌       | 123/475 [01:16<03:37,  1.61it/s]\u001b[A\n",
      "batch:  26%|██▌       | 124/475 [01:16<03:38,  1.61it/s]\u001b[A\n",
      "batch:  26%|██▋       | 125/475 [01:17<03:37,  1.61it/s]\u001b[A\n",
      "batch:  27%|██▋       | 126/475 [01:18<03:35,  1.62it/s]\u001b[A\n",
      "batch:  27%|██▋       | 127/475 [01:18<03:34,  1.62it/s]\u001b[A\n",
      "batch:  27%|██▋       | 128/475 [01:19<03:33,  1.63it/s]\u001b[A\n",
      "batch:  27%|██▋       | 129/475 [01:19<03:32,  1.63it/s]\u001b[A\n",
      "batch:  27%|██▋       | 130/475 [01:20<03:31,  1.63it/s]\u001b[A\n",
      "batch:  28%|██▊       | 131/475 [01:21<03:32,  1.62it/s]\u001b[A\n",
      "batch:  28%|██▊       | 132/475 [01:21<03:34,  1.60it/s]\u001b[A\n",
      "batch:  28%|██▊       | 133/475 [01:22<03:32,  1.61it/s]\u001b[A\n",
      "batch:  28%|██▊       | 134/475 [01:22<03:32,  1.60it/s]\u001b[A\n",
      "batch:  28%|██▊       | 135/475 [01:23<03:32,  1.60it/s]\u001b[A\n",
      "batch:  29%|██▊       | 136/475 [01:24<03:30,  1.61it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  29%|██▉       | 137/475 [01:24<03:30,  1.61it/s]\u001b[A\n",
      "batch:  29%|██▉       | 138/475 [01:25<03:29,  1.61it/s]\u001b[A\n",
      "batch:  29%|██▉       | 139/475 [01:26<03:29,  1.60it/s]\u001b[A\n",
      "batch:  29%|██▉       | 140/475 [01:26<03:28,  1.60it/s]\u001b[A\n",
      "batch:  30%|██▉       | 141/475 [01:27<03:29,  1.60it/s]\u001b[A\n",
      "batch:  30%|██▉       | 142/475 [01:27<03:29,  1.59it/s]\u001b[A\n",
      "batch:  30%|███       | 143/475 [01:28<03:27,  1.60it/s]\u001b[A\n",
      "batch:  30%|███       | 144/475 [01:29<03:26,  1.60it/s]\u001b[A\n",
      "batch:  31%|███       | 145/475 [01:29<03:25,  1.60it/s]\u001b[A\n",
      "batch:  31%|███       | 146/475 [01:30<03:25,  1.60it/s]\u001b[A\n",
      "batch:  31%|███       | 147/475 [01:31<03:24,  1.60it/s]\u001b[A\n",
      "batch:  31%|███       | 148/475 [01:31<03:23,  1.61it/s]\u001b[A\n",
      "batch:  31%|███▏      | 149/475 [01:32<03:23,  1.60it/s]\u001b[A\n",
      "batch:  32%|███▏      | 150/475 [01:32<03:23,  1.60it/s]\u001b[A\n",
      "batch:  32%|███▏      | 151/475 [01:33<03:21,  1.61it/s]\u001b[A\n",
      "batch:  32%|███▏      | 152/475 [01:34<03:21,  1.60it/s]\u001b[A\n",
      "batch:  32%|███▏      | 153/475 [01:34<03:20,  1.60it/s]\u001b[A\n",
      "batch:  32%|███▏      | 154/475 [01:35<03:19,  1.61it/s]\u001b[A\n",
      "batch:  33%|███▎      | 155/475 [01:36<03:19,  1.60it/s]\u001b[A\n",
      "batch:  33%|███▎      | 156/475 [01:36<03:18,  1.61it/s]\u001b[A\n",
      "batch:  33%|███▎      | 157/475 [01:37<03:18,  1.60it/s]\u001b[A\n",
      "batch:  33%|███▎      | 158/475 [01:37<03:18,  1.59it/s]\u001b[A\n",
      "batch:  33%|███▎      | 159/475 [01:38<03:17,  1.60it/s]\u001b[A\n",
      "batch:  34%|███▎      | 160/475 [01:39<03:14,  1.62it/s]\u001b[A\n",
      "batch:  34%|███▍      | 161/475 [01:39<03:15,  1.61it/s]\u001b[A\n",
      "batch:  34%|███▍      | 162/475 [01:40<03:14,  1.61it/s]\u001b[A\n",
      "batch:  34%|███▍      | 163/475 [01:41<03:13,  1.62it/s]\u001b[A\n",
      "batch:  35%|███▍      | 164/475 [01:41<03:12,  1.62it/s]\u001b[A\n",
      "batch:  35%|███▍      | 165/475 [01:42<03:12,  1.61it/s]\u001b[A\n",
      "batch:  35%|███▍      | 166/475 [01:42<03:11,  1.61it/s]\u001b[A\n",
      "batch:  35%|███▌      | 167/475 [01:43<03:10,  1.62it/s]\u001b[A\n",
      "batch:  35%|███▌      | 168/475 [01:44<03:10,  1.62it/s]\u001b[A\n",
      "batch:  36%|███▌      | 169/475 [01:44<03:09,  1.61it/s]\u001b[A\n",
      "batch:  36%|███▌      | 170/475 [01:45<03:09,  1.61it/s]\u001b[A\n",
      "batch:  36%|███▌      | 171/475 [01:46<03:08,  1.61it/s]\u001b[A\n",
      "batch:  36%|███▌      | 172/475 [01:46<03:07,  1.62it/s]\u001b[A\n",
      "batch:  36%|███▋      | 173/475 [01:47<03:06,  1.62it/s]\u001b[A\n",
      "batch:  37%|███▋      | 174/475 [01:47<03:06,  1.62it/s]\u001b[A\n",
      "batch:  37%|███▋      | 175/475 [01:48<03:04,  1.62it/s]\u001b[A\n",
      "batch:  37%|███▋      | 176/475 [01:49<03:05,  1.61it/s]\u001b[A\n",
      "batch:  37%|███▋      | 177/475 [01:49<03:06,  1.60it/s]\u001b[A\n",
      "batch:  37%|███▋      | 178/475 [01:50<03:04,  1.61it/s]\u001b[A\n",
      "batch:  38%|███▊      | 179/475 [01:50<03:04,  1.61it/s]\u001b[A\n",
      "batch:  38%|███▊      | 180/475 [01:51<03:02,  1.61it/s]\u001b[A\n",
      "batch:  38%|███▊      | 181/475 [01:52<03:01,  1.62it/s]\u001b[A\n",
      "batch:  38%|███▊      | 182/475 [01:52<03:01,  1.62it/s]\u001b[A\n",
      "batch:  39%|███▊      | 183/475 [01:53<03:01,  1.61it/s]\u001b[A\n",
      "batch:  39%|███▊      | 184/475 [01:54<03:01,  1.60it/s]\u001b[A\n",
      "batch:  39%|███▉      | 185/475 [01:54<02:59,  1.62it/s]\u001b[A\n",
      "batch:  39%|███▉      | 186/475 [01:55<03:01,  1.60it/s]\u001b[A\n",
      "batch:  39%|███▉      | 187/475 [01:55<02:59,  1.61it/s]\u001b[A\n",
      "batch:  40%|███▉      | 188/475 [01:56<02:59,  1.60it/s]\u001b[A\n",
      "batch:  40%|███▉      | 189/475 [01:57<02:57,  1.61it/s]\u001b[A\n",
      "batch:  40%|████      | 190/475 [01:57<02:57,  1.61it/s]\u001b[A\n",
      "batch:  40%|████      | 191/475 [01:58<02:56,  1.61it/s]\u001b[A\n",
      "batch:  40%|████      | 192/475 [01:59<02:55,  1.61it/s]\u001b[A\n",
      "batch:  41%|████      | 193/475 [01:59<02:54,  1.62it/s]\u001b[A\n",
      "batch:  41%|████      | 194/475 [02:00<02:54,  1.61it/s]\u001b[A\n",
      "batch:  41%|████      | 195/475 [02:00<02:52,  1.62it/s]\u001b[A\n",
      "batch:  41%|████▏     | 196/475 [02:01<02:52,  1.61it/s]\u001b[A\n",
      "batch:  41%|████▏     | 197/475 [02:02<02:53,  1.60it/s]\u001b[A\n",
      "batch:  42%|████▏     | 198/475 [02:02<02:53,  1.60it/s]\u001b[A\n",
      "batch:  42%|████▏     | 199/475 [02:03<02:51,  1.61it/s]\u001b[A\n",
      "batch:  42%|████▏     | 200/475 [02:04<02:51,  1.60it/s]\u001b[A\n",
      "batch:  42%|████▏     | 201/475 [02:04<02:50,  1.61it/s]\u001b[A\n",
      "batch:  43%|████▎     | 202/475 [02:05<02:49,  1.61it/s]\u001b[A\n",
      "batch:  43%|████▎     | 203/475 [02:05<02:49,  1.60it/s]\u001b[A\n",
      "batch:  43%|████▎     | 204/475 [02:06<02:49,  1.60it/s]\u001b[A\n",
      "batch:  43%|████▎     | 205/475 [02:07<02:48,  1.60it/s]\u001b[A\n",
      "batch:  43%|████▎     | 206/475 [02:07<02:47,  1.60it/s]\u001b[A\n",
      "batch:  44%|████▎     | 207/475 [02:08<02:46,  1.61it/s]\u001b[A\n",
      "batch:  44%|████▍     | 208/475 [02:09<02:47,  1.59it/s]\u001b[A\n",
      "batch:  44%|████▍     | 209/475 [02:09<02:46,  1.60it/s]\u001b[A\n",
      "batch:  44%|████▍     | 210/475 [02:10<02:46,  1.59it/s]\u001b[A\n",
      "batch:  44%|████▍     | 211/475 [02:10<02:44,  1.60it/s]\u001b[A\n",
      "batch:  45%|████▍     | 212/475 [02:11<02:44,  1.60it/s]\u001b[A\n",
      "batch:  45%|████▍     | 213/475 [02:12<02:42,  1.61it/s]\u001b[A\n",
      "batch:  45%|████▌     | 214/475 [02:12<02:42,  1.61it/s]\u001b[A\n",
      "batch:  45%|████▌     | 215/475 [02:13<02:41,  1.61it/s]\u001b[A\n",
      "batch:  45%|████▌     | 216/475 [02:13<02:40,  1.61it/s]\u001b[A\n",
      "batch:  46%|████▌     | 217/475 [02:14<02:40,  1.61it/s]\u001b[A\n",
      "batch:  46%|████▌     | 218/475 [02:15<02:39,  1.61it/s]\u001b[A\n",
      "batch:  46%|████▌     | 219/475 [02:15<02:40,  1.60it/s]\u001b[A\n",
      "batch:  46%|████▋     | 220/475 [02:16<02:38,  1.61it/s]\u001b[A\n",
      "batch:  47%|████▋     | 221/475 [02:17<02:37,  1.61it/s]\u001b[A\n",
      "batch:  47%|████▋     | 222/475 [02:17<02:38,  1.60it/s]\u001b[A\n",
      "batch:  47%|████▋     | 223/475 [02:18<02:36,  1.61it/s]\u001b[A\n",
      "batch:  47%|████▋     | 224/475 [02:18<02:35,  1.61it/s]\u001b[A\n",
      "batch:  47%|████▋     | 225/475 [02:19<02:35,  1.61it/s]\u001b[A\n",
      "batch:  48%|████▊     | 226/475 [02:20<02:34,  1.61it/s]\u001b[A\n",
      "batch:  48%|████▊     | 227/475 [02:20<02:35,  1.60it/s]\u001b[A\n",
      "batch:  48%|████▊     | 228/475 [02:21<02:34,  1.60it/s]\u001b[A\n",
      "batch:  48%|████▊     | 229/475 [02:22<02:32,  1.61it/s]\u001b[A\n",
      "batch:  48%|████▊     | 230/475 [02:22<02:34,  1.59it/s]\u001b[A\n",
      "batch:  49%|████▊     | 231/475 [02:23<02:33,  1.59it/s]\u001b[A\n",
      "batch:  49%|████▉     | 232/475 [02:24<02:33,  1.59it/s]\u001b[A\n",
      "batch:  49%|████▉     | 233/475 [02:24<02:31,  1.60it/s]\u001b[A\n",
      "batch:  49%|████▉     | 234/475 [02:25<02:29,  1.61it/s]\u001b[A\n",
      "batch:  49%|████▉     | 235/475 [02:25<02:29,  1.61it/s]\u001b[A\n",
      "batch:  50%|████▉     | 236/475 [02:26<02:27,  1.62it/s]\u001b[A\n",
      "batch:  50%|████▉     | 237/475 [02:27<02:27,  1.61it/s]\u001b[A\n",
      "batch:  50%|█████     | 238/475 [02:27<02:27,  1.61it/s]\u001b[A\n",
      "batch:  50%|█████     | 239/475 [02:28<02:28,  1.59it/s]\u001b[A\n",
      "batch:  51%|█████     | 240/475 [02:28<02:27,  1.59it/s]\u001b[A\n",
      "batch:  51%|█████     | 241/475 [02:29<02:26,  1.60it/s]\u001b[A\n",
      "batch:  51%|█████     | 242/475 [02:30<02:26,  1.59it/s]\u001b[A\n",
      "batch:  51%|█████     | 243/475 [02:30<02:25,  1.60it/s]\u001b[A\n",
      "batch:  51%|█████▏    | 244/475 [02:31<02:23,  1.60it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 245/475 [02:32<02:24,  1.59it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 246/475 [02:32<02:24,  1.59it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 247/475 [02:33<02:23,  1.58it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 248/475 [02:34<02:22,  1.59it/s]\u001b[A\n",
      "batch:  52%|█████▏    | 249/475 [02:34<02:20,  1.61it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 250/475 [02:35<02:20,  1.60it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 251/475 [02:35<02:20,  1.59it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 252/475 [02:36<02:19,  1.60it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 253/475 [02:37<02:18,  1.60it/s]\u001b[A\n",
      "batch:  53%|█████▎    | 254/475 [02:37<02:18,  1.60it/s]\u001b[A\n",
      "batch:  54%|█████▎    | 255/475 [02:38<02:17,  1.60it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 256/475 [02:39<02:18,  1.58it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 257/475 [02:39<02:17,  1.59it/s]\u001b[A\n",
      "batch:  54%|█████▍    | 258/475 [02:40<02:16,  1.59it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 259/475 [02:40<02:15,  1.60it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 260/475 [02:41<02:14,  1.60it/s]\u001b[A\n",
      "batch:  55%|█████▍    | 261/475 [02:42<02:13,  1.60it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 262/475 [02:42<02:13,  1.60it/s]\u001b[A\n",
      "batch:  55%|█████▌    | 263/475 [02:43<02:12,  1.60it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 264/475 [02:44<02:11,  1.60it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 265/475 [02:44<02:10,  1.61it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 266/475 [02:45<02:11,  1.59it/s]\u001b[A\n",
      "batch:  56%|█████▌    | 267/475 [02:45<02:14,  1.54it/s]\u001b[A\n",
      "batch:  56%|█████▋    | 268/475 [02:46<02:18,  1.49it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 269/475 [02:47<02:20,  1.46it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 270/475 [02:48<02:19,  1.47it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 271/475 [02:48<02:24,  1.41it/s]\u001b[A\n",
      "batch:  57%|█████▋    | 272/475 [02:49<02:23,  1.42it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  57%|█████▋    | 273/475 [02:50<02:23,  1.41it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 274/475 [02:51<02:24,  1.39it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 275/475 [02:51<02:30,  1.33it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 276/475 [02:52<02:28,  1.34it/s]\u001b[A\n",
      "batch:  58%|█████▊    | 277/475 [02:53<02:25,  1.36it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 278/475 [02:54<02:27,  1.34it/s]\u001b[A\n",
      "batch:  59%|█████▊    | 279/475 [02:54<02:25,  1.35it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 280/475 [02:55<02:25,  1.34it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 281/475 [02:56<02:26,  1.32it/s]\u001b[A\n",
      "batch:  59%|█████▉    | 282/475 [02:57<02:22,  1.35it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 283/475 [02:57<02:19,  1.38it/s]\u001b[A\n",
      "batch:  60%|█████▉    | 284/475 [02:58<02:16,  1.40it/s]\u001b[A\n",
      "batch:  60%|██████    | 285/475 [02:59<02:19,  1.36it/s]\u001b[A\n",
      "batch:  60%|██████    | 286/475 [03:00<02:24,  1.31it/s]\u001b[A\n",
      "batch:  60%|██████    | 287/475 [03:00<02:20,  1.34it/s]\u001b[A\n",
      "batch:  61%|██████    | 288/475 [03:01<02:14,  1.39it/s]\u001b[A\n",
      "batch:  61%|██████    | 289/475 [03:02<02:14,  1.38it/s]\u001b[A\n",
      "batch:  61%|██████    | 290/475 [03:02<02:16,  1.36it/s]\u001b[A\n",
      "batch:  61%|██████▏   | 291/475 [03:03<02:12,  1.38it/s]\u001b[A\n",
      "batch:  61%|██████▏   | 292/475 [03:04<02:14,  1.36it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 293/475 [03:05<02:17,  1.33it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 294/475 [03:05<02:16,  1.33it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 295/475 [03:06<02:13,  1.35it/s]\u001b[A\n",
      "batch:  62%|██████▏   | 296/475 [03:07<02:11,  1.36it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 297/475 [03:08<02:11,  1.36it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 298/475 [03:08<02:09,  1.37it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 299/475 [03:09<02:04,  1.41it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 300/475 [03:10<02:01,  1.45it/s]\u001b[A\n",
      "batch:  63%|██████▎   | 301/475 [03:10<02:01,  1.44it/s]\u001b[A\n",
      "batch:  64%|██████▎   | 302/475 [03:11<02:05,  1.38it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 303/475 [03:12<02:05,  1.37it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 304/475 [03:13<02:03,  1.38it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 305/475 [03:13<01:59,  1.42it/s]\u001b[A\n",
      "batch:  64%|██████▍   | 306/475 [03:14<01:59,  1.41it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 307/475 [03:15<01:56,  1.44it/s]\u001b[A\n",
      "batch:  65%|██████▍   | 308/475 [03:15<01:57,  1.42it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 309/475 [03:16<01:57,  1.42it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 310/475 [03:17<01:56,  1.42it/s]\u001b[A\n",
      "batch:  65%|██████▌   | 311/475 [03:17<01:52,  1.46it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 312/475 [03:18<01:53,  1.44it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 313/475 [03:19<01:53,  1.43it/s]\u001b[A\n",
      "batch:  66%|██████▌   | 314/475 [03:19<01:52,  1.43it/s]\u001b[A\n",
      "batch:  66%|██████▋   | 315/475 [03:20<01:50,  1.45it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 316/475 [03:21<01:52,  1.42it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 317/475 [03:22<01:51,  1.41it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 318/475 [03:22<01:51,  1.41it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 319/475 [03:23<01:50,  1.41it/s]\u001b[A\n",
      "batch:  67%|██████▋   | 320/475 [03:24<01:46,  1.45it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 321/475 [03:24<01:44,  1.48it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 322/475 [03:25<01:45,  1.45it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 323/475 [03:26<01:44,  1.45it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 324/475 [03:26<01:43,  1.46it/s]\u001b[A\n",
      "batch:  68%|██████▊   | 325/475 [03:27<01:41,  1.48it/s]\u001b[A\n",
      "batch:  69%|██████▊   | 326/475 [03:28<01:38,  1.52it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 327/475 [03:28<01:44,  1.42it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 328/475 [03:29<01:42,  1.43it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 329/475 [03:30<01:41,  1.44it/s]\u001b[A\n",
      "batch:  69%|██████▉   | 330/475 [03:31<01:38,  1.47it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 331/475 [03:31<01:36,  1.49it/s]\u001b[A\n",
      "batch:  70%|██████▉   | 332/475 [03:32<01:35,  1.50it/s]\u001b[A\n",
      "batch:  70%|███████   | 333/475 [03:32<01:33,  1.51it/s]\u001b[A\n",
      "batch:  70%|███████   | 334/475 [03:33<01:32,  1.52it/s]\u001b[A\n",
      "batch:  71%|███████   | 335/475 [03:34<01:32,  1.52it/s]\u001b[A\n",
      "batch:  71%|███████   | 336/475 [03:34<01:32,  1.50it/s]\u001b[A\n",
      "batch:  71%|███████   | 337/475 [03:35<01:34,  1.46it/s]\u001b[A\n",
      "batch:  71%|███████   | 338/475 [03:36<01:41,  1.35it/s]\u001b[A\n",
      "batch:  71%|███████▏  | 339/475 [03:37<01:38,  1.38it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 340/475 [03:37<01:34,  1.42it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 341/475 [03:38<01:31,  1.46it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 342/475 [03:39<01:29,  1.48it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 343/475 [03:39<01:28,  1.49it/s]\u001b[A\n",
      "batch:  72%|███████▏  | 344/475 [03:40<01:26,  1.52it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 345/475 [03:41<01:26,  1.51it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 346/475 [03:41<01:27,  1.48it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 347/475 [03:42<01:26,  1.48it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 348/475 [03:43<01:25,  1.49it/s]\u001b[A\n",
      "batch:  73%|███████▎  | 349/475 [03:43<01:25,  1.47it/s]\u001b[A\n",
      "batch:  74%|███████▎  | 350/475 [03:44<01:28,  1.42it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 351/475 [03:45<01:26,  1.43it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 352/475 [03:46<01:24,  1.45it/s]\u001b[A\n",
      "batch:  74%|███████▍  | 353/475 [03:46<01:22,  1.48it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 354/475 [03:47<01:24,  1.44it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 355/475 [03:48<01:23,  1.44it/s]\u001b[A\n",
      "batch:  75%|███████▍  | 356/475 [03:48<01:22,  1.43it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 357/475 [03:49<01:27,  1.35it/s]\u001b[A\n",
      "batch:  75%|███████▌  | 358/475 [03:50<01:25,  1.38it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 359/475 [03:51<01:23,  1.39it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 360/475 [03:51<01:20,  1.42it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 361/475 [03:52<01:18,  1.46it/s]\u001b[A\n",
      "batch:  76%|███████▌  | 362/475 [03:52<01:15,  1.49it/s]\u001b[A\n",
      "batch:  76%|███████▋  | 363/475 [03:53<01:13,  1.52it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 364/475 [03:54<01:15,  1.47it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 365/475 [03:55<01:20,  1.36it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 366/475 [03:55<01:18,  1.38it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 367/475 [03:56<01:15,  1.42it/s]\u001b[A\n",
      "batch:  77%|███████▋  | 368/475 [03:57<01:13,  1.46it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 369/475 [03:57<01:12,  1.46it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 370/475 [03:58<01:11,  1.46it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 371/475 [03:59<01:09,  1.49it/s]\u001b[A\n",
      "batch:  78%|███████▊  | 372/475 [03:59<01:08,  1.50it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 373/475 [04:00<01:08,  1.48it/s]\u001b[A\n",
      "batch:  79%|███████▊  | 374/475 [04:01<01:07,  1.49it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 375/475 [04:01<01:06,  1.50it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 376/475 [04:02<01:05,  1.51it/s]\u001b[A\n",
      "batch:  79%|███████▉  | 377/475 [04:03<01:05,  1.49it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 378/475 [04:03<01:05,  1.48it/s]\u001b[A\n",
      "batch:  80%|███████▉  | 379/475 [04:04<01:04,  1.50it/s]\u001b[A\n",
      "batch:  80%|████████  | 380/475 [04:05<01:03,  1.50it/s]\u001b[A\n",
      "batch:  80%|████████  | 381/475 [04:05<01:02,  1.50it/s]\u001b[A\n",
      "batch:  80%|████████  | 382/475 [04:06<01:01,  1.51it/s]\u001b[A\n",
      "batch:  81%|████████  | 383/475 [04:07<01:00,  1.51it/s]\u001b[A\n",
      "batch:  81%|████████  | 384/475 [04:07<00:59,  1.52it/s]\u001b[A\n",
      "batch:  81%|████████  | 385/475 [04:08<00:59,  1.51it/s]\u001b[A\n",
      "batch:  81%|████████▏ | 386/475 [04:09<01:00,  1.48it/s]\u001b[A\n",
      "batch:  81%|████████▏ | 387/475 [04:09<00:59,  1.47it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 388/475 [04:10<00:59,  1.46it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 389/475 [04:11<01:00,  1.43it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 390/475 [04:12<00:58,  1.45it/s]\u001b[A\n",
      "batch:  82%|████████▏ | 391/475 [04:12<00:56,  1.48it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 392/475 [04:13<00:56,  1.47it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 393/475 [04:14<00:55,  1.48it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 394/475 [04:14<00:54,  1.49it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 395/475 [04:15<00:53,  1.50it/s]\u001b[A\n",
      "batch:  83%|████████▎ | 396/475 [04:16<00:54,  1.46it/s]\u001b[A\n",
      "batch:  84%|████████▎ | 397/475 [04:16<00:53,  1.47it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 398/475 [04:17<00:51,  1.49it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 399/475 [04:18<00:51,  1.48it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 400/475 [04:18<00:50,  1.47it/s]\u001b[A\n",
      "batch:  84%|████████▍ | 401/475 [04:19<00:50,  1.48it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 402/475 [04:20<00:48,  1.50it/s]\u001b[A\n",
      "batch:  85%|████████▍ | 403/475 [04:20<00:47,  1.51it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 404/475 [04:21<00:47,  1.51it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 405/475 [04:22<00:46,  1.52it/s]\u001b[A\n",
      "batch:  85%|████████▌ | 406/475 [04:22<00:45,  1.51it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 407/475 [04:23<00:45,  1.51it/s]\u001b[A\n",
      "batch:  86%|████████▌ | 408/475 [04:24<00:44,  1.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  86%|████████▌ | 409/475 [04:24<00:43,  1.51it/s]\u001b[A\n",
      "batch:  86%|████████▋ | 410/475 [04:25<00:42,  1.51it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 411/475 [04:26<00:44,  1.44it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 412/475 [04:26<00:43,  1.44it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 413/475 [04:27<00:42,  1.46it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 414/475 [04:28<00:43,  1.42it/s]\u001b[A\n",
      "batch:  87%|████████▋ | 415/475 [04:28<00:42,  1.42it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 416/475 [04:29<00:42,  1.37it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 417/475 [04:30<00:41,  1.40it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 418/475 [04:31<00:40,  1.42it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 419/475 [04:31<00:39,  1.41it/s]\u001b[A\n",
      "batch:  88%|████████▊ | 420/475 [04:32<00:40,  1.35it/s]\u001b[A\n",
      "batch:  89%|████████▊ | 421/475 [04:33<00:39,  1.38it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 422/475 [04:33<00:37,  1.43it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 423/475 [04:34<00:37,  1.39it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 424/475 [04:35<00:36,  1.41it/s]\u001b[A\n",
      "batch:  89%|████████▉ | 425/475 [04:36<00:35,  1.43it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 426/475 [04:36<00:33,  1.47it/s]\u001b[A\n",
      "batch:  90%|████████▉ | 427/475 [04:37<00:33,  1.44it/s]\u001b[A\n",
      "batch:  90%|█████████ | 428/475 [04:38<00:32,  1.45it/s]\u001b[A\n",
      "batch:  90%|█████████ | 429/475 [04:38<00:31,  1.47it/s]\u001b[A\n",
      "batch:  91%|█████████ | 430/475 [04:39<00:30,  1.46it/s]\u001b[A\n",
      "batch:  91%|█████████ | 431/475 [04:40<00:34,  1.27it/s]\u001b[A\n",
      "batch:  91%|█████████ | 432/475 [04:41<00:32,  1.32it/s]\u001b[A\n",
      "batch:  91%|█████████ | 433/475 [04:41<00:30,  1.37it/s]\u001b[A\n",
      "batch:  91%|█████████▏| 434/475 [04:42<00:28,  1.43it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 435/475 [04:43<00:27,  1.47it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 436/475 [04:43<00:26,  1.49it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 437/475 [04:44<00:27,  1.41it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 438/475 [04:45<00:26,  1.41it/s]\u001b[A\n",
      "batch:  92%|█████████▏| 439/475 [04:45<00:24,  1.44it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 440/475 [04:46<00:24,  1.43it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 441/475 [04:47<00:23,  1.42it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 442/475 [04:48<00:23,  1.43it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 443/475 [04:48<00:21,  1.46it/s]\u001b[A\n",
      "batch:  93%|█████████▎| 444/475 [04:49<00:21,  1.43it/s]\u001b[A\n",
      "batch:  94%|█████████▎| 445/475 [04:50<00:20,  1.44it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 446/475 [04:50<00:19,  1.46it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 447/475 [04:51<00:18,  1.48it/s]\u001b[A\n",
      "batch:  94%|█████████▍| 448/475 [04:52<00:18,  1.50it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 449/475 [04:52<00:17,  1.51it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 450/475 [04:53<00:16,  1.52it/s]\u001b[A\n",
      "batch:  95%|█████████▍| 451/475 [04:54<00:15,  1.53it/s]\u001b[A\n",
      "batch:  95%|█████████▌| 452/475 [04:54<00:15,  1.51it/s]\u001b[A\n",
      "batch:  95%|█████████▌| 453/475 [04:55<00:14,  1.50it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 454/475 [04:56<00:13,  1.51it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 455/475 [04:56<00:13,  1.52it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 456/475 [04:57<00:12,  1.50it/s]\u001b[A\n",
      "batch:  96%|█████████▌| 457/475 [04:58<00:12,  1.48it/s]\u001b[A\n",
      "batch:  96%|█████████▋| 458/475 [04:58<00:11,  1.50it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 459/475 [04:59<00:10,  1.50it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 460/475 [05:00<00:09,  1.52it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 461/475 [05:00<00:09,  1.53it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 462/475 [05:01<00:09,  1.42it/s]\u001b[A\n",
      "batch:  97%|█████████▋| 463/475 [05:02<00:08,  1.38it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 464/475 [05:02<00:07,  1.41it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 465/475 [05:03<00:07,  1.38it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 466/475 [05:04<00:06,  1.40it/s]\u001b[A\n",
      "batch:  98%|█████████▊| 467/475 [05:05<00:05,  1.43it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 468/475 [05:05<00:04,  1.47it/s]\u001b[A\n",
      "batch:  99%|█████████▊| 469/475 [05:06<00:04,  1.49it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 470/475 [05:06<00:03,  1.50it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 471/475 [05:07<00:02,  1.51it/s]\u001b[A\n",
      "batch:  99%|█████████▉| 472/475 [05:08<00:02,  1.48it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 473/475 [05:09<00:01,  1.47it/s]\u001b[A\n",
      "batch: 100%|█████████▉| 474/475 [05:09<00:00,  1.49it/s]\u001b[A\n",
      "batch: 100%|██████████| 475/475 [05:10<00:00,  1.28it/s]\u001b[A\n",
      "Epoch:  25%|██▌       | 1/4 [05:10<15:32, 310.73s/it]   \u001b[A\n",
      "batch:   0%|          | 0/475 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.0982445480949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "batch:   0%|          | 1/475 [00:00<05:23,  1.46it/s]\u001b[A\n",
      "batch:   0%|          | 2/475 [00:01<05:20,  1.48it/s]\u001b[A\n",
      "batch:   1%|          | 3/475 [00:01<05:15,  1.50it/s]\u001b[A\n",
      "batch:   1%|          | 4/475 [00:02<05:12,  1.50it/s]\u001b[A\n",
      "batch:   1%|          | 5/475 [00:03<05:37,  1.39it/s]\u001b[A\n",
      "batch:   1%|▏         | 6/475 [00:04<05:33,  1.40it/s]\u001b[A\n",
      "batch:   1%|▏         | 7/475 [00:04<05:24,  1.44it/s]\u001b[A\n",
      "batch:   2%|▏         | 8/475 [00:05<05:16,  1.47it/s]\u001b[A\n",
      "batch:   2%|▏         | 9/475 [00:06<05:11,  1.50it/s]\u001b[A\n",
      "batch:   2%|▏         | 10/475 [00:06<05:14,  1.48it/s]\u001b[A\n",
      "batch:   2%|▏         | 11/475 [00:07<05:16,  1.47it/s]\u001b[A\n",
      "batch:   3%|▎         | 12/475 [00:08<05:16,  1.46it/s]\u001b[A\n",
      "batch:   3%|▎         | 13/475 [00:08<05:12,  1.48it/s]\u001b[A\n",
      "batch:   3%|▎         | 14/475 [00:09<05:09,  1.49it/s]\u001b[A\n",
      "batch:   3%|▎         | 15/475 [00:10<05:08,  1.49it/s]\u001b[A\n",
      "batch:   3%|▎         | 16/475 [00:10<05:07,  1.49it/s]\u001b[A\n",
      "batch:   4%|▎         | 17/475 [00:11<05:08,  1.48it/s]\u001b[A\n",
      "batch:   4%|▍         | 18/475 [00:12<05:04,  1.50it/s]\u001b[A\n",
      "batch:   4%|▍         | 19/475 [00:12<05:02,  1.51it/s]\u001b[A\n",
      "batch:   4%|▍         | 20/475 [00:13<05:17,  1.43it/s]\u001b[A\n",
      "batch:   4%|▍         | 21/475 [00:14<05:19,  1.42it/s]\u001b[A\n",
      "batch:   5%|▍         | 22/475 [00:15<05:14,  1.44it/s]\u001b[A\n",
      "batch:   5%|▍         | 23/475 [00:15<05:09,  1.46it/s]\u001b[A\n",
      "batch:   5%|▌         | 24/475 [00:16<05:03,  1.49it/s]\u001b[A\n",
      "batch:   5%|▌         | 25/475 [00:16<04:58,  1.51it/s]\u001b[A\n",
      "batch:   5%|▌         | 26/475 [00:17<04:56,  1.51it/s]\u001b[A\n",
      "batch:   6%|▌         | 27/475 [00:18<04:58,  1.50it/s]\u001b[A\n",
      "batch:   6%|▌         | 28/475 [00:19<05:10,  1.44it/s]\u001b[A\n",
      "batch:   6%|▌         | 29/475 [00:19<05:07,  1.45it/s]\u001b[A\n",
      "batch:   6%|▋         | 30/475 [00:20<05:25,  1.37it/s]\u001b[A\n",
      "batch:   7%|▋         | 31/475 [00:21<05:42,  1.30it/s]\u001b[A\n",
      "batch:   7%|▋         | 32/475 [00:22<05:32,  1.33it/s]\u001b[A\n",
      "batch:   7%|▋         | 33/475 [00:22<05:18,  1.39it/s]\u001b[A\n",
      "batch:   7%|▋         | 34/475 [00:23<05:08,  1.43it/s]\u001b[A\n",
      "batch:   7%|▋         | 35/475 [00:24<05:06,  1.44it/s]\u001b[A\n",
      "batch:   8%|▊         | 36/475 [00:24<04:59,  1.46it/s]\u001b[A\n",
      "batch:   8%|▊         | 37/475 [00:25<04:55,  1.48it/s]\u001b[A\n",
      "batch:   8%|▊         | 38/475 [00:26<04:53,  1.49it/s]\u001b[A\n",
      "batch:   8%|▊         | 39/475 [00:26<05:10,  1.40it/s]\u001b[A\n",
      "batch:   8%|▊         | 40/475 [00:27<05:06,  1.42it/s]\u001b[A\n",
      "batch:   9%|▊         | 41/475 [00:28<04:59,  1.45it/s]\u001b[A\n",
      "batch:   9%|▉         | 42/475 [00:28<04:59,  1.44it/s]\u001b[A\n",
      "batch:   9%|▉         | 43/475 [00:29<04:54,  1.47it/s]\u001b[A\n",
      "batch:   9%|▉         | 44/475 [00:30<04:51,  1.48it/s]\u001b[A\n",
      "batch:   9%|▉         | 45/475 [00:30<04:49,  1.49it/s]\u001b[A\n",
      "batch:  10%|▉         | 46/475 [00:31<04:46,  1.50it/s]\u001b[A\n",
      "batch:  10%|▉         | 47/475 [00:32<04:45,  1.50it/s]\u001b[A\n",
      "batch:  10%|█         | 48/475 [00:32<04:44,  1.50it/s]\u001b[A\n",
      "batch:  10%|█         | 49/475 [00:33<04:44,  1.50it/s]\u001b[A\n",
      "batch:  11%|█         | 50/475 [00:34<04:41,  1.51it/s]\u001b[A\n",
      "batch:  11%|█         | 51/475 [00:34<04:40,  1.51it/s]\u001b[A\n",
      "batch:  11%|█         | 52/475 [00:35<04:42,  1.50it/s]\u001b[A\n",
      "batch:  11%|█         | 53/475 [00:36<04:43,  1.49it/s]\u001b[A\n",
      "batch:  11%|█▏        | 54/475 [00:36<04:44,  1.48it/s]\u001b[A\n",
      "batch:  12%|█▏        | 55/475 [00:37<04:41,  1.49it/s]\u001b[A\n",
      "batch:  12%|█▏        | 56/475 [00:38<04:41,  1.49it/s]\u001b[A\n",
      "batch:  12%|█▏        | 57/475 [00:38<04:46,  1.46it/s]\u001b[A\n",
      "batch:  12%|█▏        | 58/475 [00:39<04:57,  1.40it/s]\u001b[A\n",
      "batch:  12%|█▏        | 59/475 [00:40<04:53,  1.42it/s]\u001b[A\n",
      "batch:  13%|█▎        | 60/475 [00:41<04:46,  1.45it/s]\u001b[A\n",
      "batch:  13%|█▎        | 61/475 [00:41<04:41,  1.47it/s]\u001b[A\n",
      "batch:  13%|█▎        | 62/475 [00:42<04:45,  1.45it/s]\u001b[A\n",
      "batch:  13%|█▎        | 63/475 [00:43<04:50,  1.42it/s]\u001b[A\n",
      "batch:  13%|█▎        | 64/475 [00:43<04:47,  1.43it/s]\u001b[A\n",
      "batch:  14%|█▎        | 65/475 [00:44<04:56,  1.38it/s]\u001b[A\n",
      "batch:  14%|█▍        | 66/475 [00:45<04:50,  1.41it/s]\u001b[A\n",
      "batch:  14%|█▍        | 67/475 [00:46<04:44,  1.43it/s]\u001b[A\n",
      "batch:  14%|█▍        | 68/475 [00:46<04:36,  1.47it/s]\u001b[A\n",
      "batch:  15%|█▍        | 69/475 [00:47<04:35,  1.47it/s]\u001b[A\n",
      "batch:  15%|█▍        | 70/475 [00:48<04:34,  1.48it/s]\u001b[A\n",
      "batch:  15%|█▍        | 71/475 [00:48<04:36,  1.46it/s]\u001b[A\n",
      "batch:  15%|█▌        | 72/475 [00:49<04:37,  1.45it/s]\u001b[A\n",
      "batch:  15%|█▌        | 73/475 [00:50<04:37,  1.45it/s]\u001b[A\n",
      "batch:  16%|█▌        | 74/475 [00:50<04:33,  1.47it/s]\u001b[A\n",
      "batch:  16%|█▌        | 75/475 [00:51<04:32,  1.47it/s]\u001b[A\n",
      "batch:  16%|█▌        | 76/475 [00:52<04:26,  1.50it/s]\u001b[A\n",
      "batch:  16%|█▌        | 77/475 [00:52<04:28,  1.48it/s]\u001b[A\n",
      "batch:  16%|█▋        | 78/475 [00:53<04:28,  1.48it/s]\u001b[A\n",
      "batch:  17%|█▋        | 79/475 [00:54<04:27,  1.48it/s]\u001b[A\n",
      "batch:  17%|█▋        | 80/475 [00:54<04:47,  1.38it/s]\u001b[A\n",
      "batch:  17%|█▋        | 81/475 [00:55<04:43,  1.39it/s]\u001b[A\n",
      "batch:  17%|█▋        | 82/475 [00:56<04:38,  1.41it/s]\u001b[A\n",
      "batch:  17%|█▋        | 83/475 [00:57<04:33,  1.43it/s]\u001b[A\n",
      "batch:  18%|█▊        | 84/475 [00:57<04:27,  1.46it/s]\u001b[A\n",
      "batch:  18%|█▊        | 85/475 [00:58<04:25,  1.47it/s]\u001b[A\n",
      "batch:  18%|█▊        | 86/475 [00:59<04:25,  1.46it/s]\u001b[A\n",
      "batch:  18%|█▊        | 87/475 [00:59<04:35,  1.41it/s]\u001b[A\n",
      "batch:  19%|█▊        | 88/475 [01:00<04:32,  1.42it/s]\u001b[A\n",
      "batch:  19%|█▊        | 89/475 [01:01<04:25,  1.45it/s]\u001b[A\n",
      "batch:  19%|█▉        | 90/475 [01:01<04:20,  1.48it/s]\u001b[A\n",
      "batch:  19%|█▉        | 91/475 [01:02<04:22,  1.46it/s]\u001b[A\n",
      "batch:  19%|█▉        | 92/475 [01:03<04:21,  1.47it/s]\u001b[A\n",
      "batch:  20%|█▉        | 93/475 [01:03<04:17,  1.48it/s]\u001b[A\n",
      "batch:  20%|█▉        | 94/475 [01:04<04:13,  1.50it/s]\u001b[A\n",
      "batch:  20%|██        | 95/475 [01:05<04:14,  1.49it/s]\u001b[A\n",
      "batch:  20%|██        | 96/475 [01:05<04:26,  1.42it/s]\u001b[A\n",
      "batch:  20%|██        | 97/475 [01:06<04:23,  1.44it/s]\u001b[A\n",
      "batch:  21%|██        | 98/475 [01:07<04:17,  1.47it/s]\u001b[A\n",
      "batch:  21%|██        | 99/475 [01:07<04:12,  1.49it/s]\u001b[A\n",
      "batch:  21%|██        | 100/475 [01:08<04:11,  1.49it/s]\u001b[A\n",
      "batch:  21%|██▏       | 101/475 [01:09<04:14,  1.47it/s]\u001b[A\n",
      "batch:  21%|██▏       | 102/475 [01:10<04:21,  1.43it/s]\u001b[A\n",
      "batch:  22%|██▏       | 103/475 [01:10<04:17,  1.44it/s]\u001b[A\n",
      "batch:  22%|██▏       | 104/475 [01:11<04:27,  1.39it/s]\u001b[A\n",
      "batch:  22%|██▏       | 105/475 [01:12<04:22,  1.41it/s]\u001b[A\n",
      "batch:  22%|██▏       | 106/475 [01:12<04:15,  1.44it/s]\u001b[A\n",
      "batch:  23%|██▎       | 107/475 [01:13<04:13,  1.45it/s]\u001b[A\n",
      "batch:  23%|██▎       | 108/475 [01:14<04:07,  1.48it/s]\u001b[A\n",
      "batch:  23%|██▎       | 109/475 [01:14<04:04,  1.50it/s]\u001b[A\n",
      "batch:  23%|██▎       | 110/475 [01:15<04:07,  1.48it/s]\u001b[A\n",
      "batch:  23%|██▎       | 111/475 [01:16<04:14,  1.43it/s]\u001b[A\n",
      "batch:  24%|██▎       | 112/475 [01:16<04:11,  1.44it/s]\u001b[A\n",
      "batch:  24%|██▍       | 113/475 [01:17<04:07,  1.46it/s]\u001b[A\n",
      "batch:  24%|██▍       | 114/475 [01:18<04:03,  1.48it/s]\u001b[A\n",
      "batch:  24%|██▍       | 115/475 [01:18<04:01,  1.49it/s]\u001b[A\n",
      "batch:  24%|██▍       | 116/475 [01:19<04:01,  1.48it/s]\u001b[A\n",
      "batch:  25%|██▍       | 117/475 [01:20<03:59,  1.50it/s]\u001b[A\n",
      "batch:  25%|██▍       | 118/475 [01:20<04:00,  1.48it/s]\u001b[A\n",
      "batch:  25%|██▌       | 119/475 [01:21<04:00,  1.48it/s]\u001b[A\n",
      "batch:  25%|██▌       | 120/475 [01:22<03:57,  1.49it/s]\u001b[A\n",
      "batch:  25%|██▌       | 121/475 [01:23<04:36,  1.28it/s]\u001b[A\n",
      "batch:  26%|██▌       | 122/475 [01:24<04:24,  1.34it/s]\u001b[A\n",
      "batch:  26%|██▌       | 123/475 [01:24<04:12,  1.39it/s]\u001b[A\n",
      "batch:  26%|██▌       | 124/475 [01:25<04:07,  1.42it/s]\u001b[A\n",
      "batch:  26%|██▋       | 125/475 [01:26<04:02,  1.44it/s]\u001b[A\n",
      "batch:  27%|██▋       | 126/475 [01:26<04:04,  1.43it/s]\u001b[A\n",
      "batch:  27%|██▋       | 127/475 [01:27<04:12,  1.38it/s]\u001b[A\n",
      "batch:  27%|██▋       | 128/475 [01:28<04:06,  1.41it/s]\u001b[A\n",
      "batch:  27%|██▋       | 129/475 [01:28<03:59,  1.44it/s]\u001b[A\n",
      "batch:  27%|██▋       | 130/475 [01:29<03:57,  1.45it/s]\u001b[A\n",
      "batch:  28%|██▊       | 131/475 [01:30<04:01,  1.42it/s]\u001b[A\n",
      "batch:  28%|██▊       | 132/475 [01:30<04:01,  1.42it/s]\u001b[A\n",
      "batch:  28%|██▊       | 133/475 [01:31<04:05,  1.39it/s]\u001b[A\n",
      "batch:  28%|██▊       | 134/475 [01:33<05:08,  1.11it/s]\u001b[A\n",
      "batch:  28%|██▊       | 135/475 [01:33<04:56,  1.15it/s]\u001b[A\n",
      "batch:  29%|██▊       | 136/475 [01:34<04:43,  1.20it/s]\u001b[A\n",
      "batch:  29%|██▉       | 137/475 [01:35<04:31,  1.25it/s]\u001b[A\n",
      "batch:  29%|██▉       | 138/475 [01:36<04:22,  1.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  29%|██▉       | 139/475 [01:36<04:23,  1.27it/s]\u001b[A\n",
      "batch:  29%|██▉       | 140/475 [01:37<04:24,  1.27it/s]\u001b[A\n",
      "batch:  30%|██▉       | 141/475 [01:38<04:16,  1.30it/s]\u001b[A\n",
      "batch:  30%|██▉       | 142/475 [01:39<04:04,  1.36it/s]\u001b[A\n",
      "batch:  30%|███       | 143/475 [01:39<04:04,  1.36it/s]\u001b[A\n",
      "batch:  30%|███       | 144/475 [01:40<04:07,  1.34it/s]\u001b[A\n",
      "batch:  31%|███       | 145/475 [01:41<04:04,  1.35it/s]\u001b[A\n",
      "batch:  31%|███       | 146/475 [01:41<04:01,  1.36it/s]\u001b[A\n",
      "batch:  31%|███       | 147/475 [01:42<04:07,  1.32it/s]\u001b[A\n",
      "batch:  31%|███       | 148/475 [01:43<04:14,  1.28it/s]\u001b[A\n",
      "batch:  31%|███▏      | 149/475 [01:44<04:13,  1.29it/s]\u001b[A\n",
      "batch:  32%|███▏      | 150/475 [01:45<04:04,  1.33it/s]\u001b[A\n",
      "batch:  32%|███▏      | 151/475 [01:45<04:11,  1.29it/s]\u001b[A\n",
      "batch:  32%|███▏      | 152/475 [01:46<04:13,  1.28it/s]\u001b[A\n",
      "batch:  32%|███▏      | 153/475 [01:47<04:03,  1.32it/s]\u001b[A\n",
      "batch:  32%|███▏      | 154/475 [01:48<04:01,  1.33it/s]\u001b[A\n",
      "batch:  33%|███▎      | 155/475 [01:48<04:02,  1.32it/s]\u001b[A\n",
      "batch:  33%|███▎      | 156/475 [01:49<04:00,  1.33it/s]\u001b[A\n",
      "batch:  33%|███▎      | 157/475 [01:50<04:04,  1.30it/s]\u001b[A\n",
      "batch:  33%|███▎      | 158/475 [01:51<03:58,  1.33it/s]\u001b[A\n",
      "batch:  33%|███▎      | 159/475 [01:51<03:54,  1.35it/s]\u001b[A\n",
      "batch:  34%|███▎      | 160/475 [01:52<03:54,  1.34it/s]\u001b[A\n",
      "batch:  34%|███▍      | 161/475 [01:53<03:53,  1.35it/s]\u001b[A\n",
      "batch:  34%|███▍      | 162/475 [01:54<03:49,  1.36it/s]\u001b[A\n",
      "batch:  34%|███▍      | 163/475 [01:54<03:44,  1.39it/s]\u001b[A\n",
      "batch:  35%|███▍      | 164/475 [01:55<03:47,  1.37it/s]\u001b[A\n",
      "batch:  35%|███▍      | 165/475 [01:56<03:41,  1.40it/s]\u001b[A\n",
      "batch:  35%|███▍      | 166/475 [01:56<03:38,  1.42it/s]\u001b[A\n",
      "batch:  35%|███▌      | 167/475 [01:57<03:34,  1.44it/s]\u001b[A\n",
      "batch:  35%|███▌      | 168/475 [01:58<03:29,  1.46it/s]\u001b[A\n",
      "batch:  36%|███▌      | 169/475 [01:58<03:26,  1.48it/s]\u001b[A\n",
      "batch:  36%|███▌      | 170/475 [01:59<03:29,  1.46it/s]\u001b[A\n",
      "batch:  36%|███▌      | 171/475 [02:00<03:36,  1.40it/s]\u001b[A\n",
      "batch:  36%|███▌      | 172/475 [02:01<03:36,  1.40it/s]\u001b[A\n",
      "batch:  36%|███▋      | 173/475 [02:01<03:31,  1.43it/s]\u001b[A\n",
      "batch:  37%|███▋      | 174/475 [02:02<03:27,  1.45it/s]\u001b[A\n",
      "batch:  37%|███▋      | 175/475 [02:03<03:25,  1.46it/s]\u001b[A\n",
      "batch:  37%|███▋      | 176/475 [02:03<03:22,  1.48it/s]\u001b[A\n",
      "batch:  37%|███▋      | 177/475 [02:04<03:23,  1.47it/s]\u001b[A\n",
      "batch:  37%|███▋      | 178/475 [02:05<03:21,  1.47it/s]\u001b[A\n",
      "batch:  38%|███▊      | 179/475 [02:05<03:24,  1.45it/s]\u001b[A\n",
      "batch:  38%|███▊      | 180/475 [02:06<03:34,  1.37it/s]\u001b[A\n",
      "batch:  38%|███▊      | 181/475 [02:07<03:34,  1.37it/s]\u001b[A\n",
      "batch:  38%|███▊      | 182/475 [02:08<03:29,  1.40it/s]\u001b[A\n",
      "batch:  39%|███▊      | 183/475 [02:08<03:24,  1.43it/s]\u001b[A\n",
      "batch:  39%|███▊      | 184/475 [02:09<03:20,  1.45it/s]\u001b[A\n",
      "batch:  39%|███▉      | 185/475 [02:10<03:18,  1.46it/s]\u001b[A\n",
      "batch:  39%|███▉      | 186/475 [02:10<03:14,  1.49it/s]\u001b[A\n",
      "batch:  39%|███▉      | 187/475 [02:11<03:13,  1.49it/s]\u001b[A\n",
      "batch:  40%|███▉      | 188/475 [02:12<03:16,  1.46it/s]\u001b[A\n",
      "batch:  40%|███▉      | 189/475 [02:12<03:17,  1.45it/s]\u001b[A\n",
      "batch:  40%|████      | 190/475 [02:13<03:26,  1.38it/s]\u001b[A\n",
      "batch:  40%|████      | 191/475 [02:14<03:24,  1.39it/s]\u001b[A\n",
      "batch:  40%|████      | 192/475 [02:14<03:20,  1.41it/s]\u001b[A\n",
      "batch:  41%|████      | 193/475 [02:15<03:17,  1.43it/s]\u001b[A\n",
      "batch:  41%|████      | 194/475 [02:16<03:17,  1.42it/s]\u001b[A\n",
      "batch:  41%|████      | 195/475 [02:17<03:20,  1.39it/s]\u001b[A\n",
      "batch:  41%|████▏     | 196/475 [02:17<03:17,  1.41it/s]\u001b[A\n",
      "batch:  41%|████▏     | 197/475 [02:18<03:18,  1.40it/s]\u001b[A\n",
      "batch:  42%|████▏     | 198/475 [02:19<03:15,  1.42it/s]\u001b[A\n",
      "batch:  42%|████▏     | 199/475 [02:19<03:12,  1.44it/s]\u001b[A\n",
      "batch:  42%|████▏     | 200/475 [02:20<03:11,  1.44it/s]\u001b[A\n",
      "batch:  42%|████▏     | 201/475 [02:21<03:13,  1.41it/s]\u001b[A\n",
      "batch:  43%|████▎     | 202/475 [02:22<03:10,  1.44it/s]\u001b[A\n",
      "batch:  43%|████▎     | 203/475 [02:22<03:15,  1.39it/s]\u001b[A\n",
      "batch:  43%|████▎     | 204/475 [02:23<03:12,  1.40it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ba69df427fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Update parameters and take a step using the computed gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "step = 0\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    \n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    # Train the data for one epoch\n",
    "    for batch in tqdm(train_dataloader,desc='batch',leave=False):\n",
    "        step = step + 1\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        train_loss_set.append(loss.item()) \n",
    "        vis.plot('loss', 'train_loss', 'Loss',step,loss.item())   \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        \n",
    "        \n",
    "    # Validation\n",
    "\n",
    "    if(step%500==0):\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "                logits = output[0]\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "        vis.plot('accuracy', 'val_acc', 'val_acc',step,eval_accuracy/nb_eval_steps)\n",
    "        torch.save(model, 'random_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.comments.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df2.comments.values.shape[0]):\n",
    "    df2.comments.values[i] = df2.comments.values[i][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df2.comments.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "# prediction_labels = torch.tensor(labels)\n",
    "  \n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:  41%|████      | 381/938 [07:08<10:58,  1.18s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4cb32e70d4b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Move logits and labels to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Store predictions and true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(prediction_dataloader,desc='batch',leave=False):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = output[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12192\n"
     ]
    }
   ],
   "source": [
    "main_preds = []\n",
    "for i in range(len(predictions)):\n",
    "    main_preds += list(np.argmax(predictions[i], axis=1))\n",
    "    \n",
    "print(len(main_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 16,\n",
       " 1,\n",
       " 16,\n",
       " 4,\n",
       " 14,\n",
       " 10,\n",
       " 6,\n",
       " 4,\n",
       " 16,\n",
       " 4,\n",
       " 12,\n",
       " 13,\n",
       " 17,\n",
       " 10,\n",
       " 14,\n",
       " 18,\n",
       " 8,\n",
       " 19,\n",
       " 14,\n",
       " 5,\n",
       " 17,\n",
       " 15,\n",
       " 19,\n",
       " 15,\n",
       " 4,\n",
       " 3,\n",
       " 12,\n",
       " 8,\n",
       " 13,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 19,\n",
       " 17,\n",
       " 0,\n",
       " 12,\n",
       " 16,\n",
       " 19,\n",
       " 12,\n",
       " 1,\n",
       " 17,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 2,\n",
       " 6,\n",
       " 17,\n",
       " 5,\n",
       " 10,\n",
       " 17,\n",
       " 18,\n",
       " 13,\n",
       " 16,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 13,\n",
       " 17,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 13,\n",
       " 6,\n",
       " 8,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 14,\n",
       " 17,\n",
       " 2,\n",
       " 8,\n",
       " 17,\n",
       " 11,\n",
       " 6,\n",
       " 11,\n",
       " 0,\n",
       " 19,\n",
       " 13,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 6,\n",
       " 15,\n",
       " 17,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 19,\n",
       " 15,\n",
       " 8,\n",
       " 11,\n",
       " 17,\n",
       " 11,\n",
       " 2,\n",
       " 16,\n",
       " 3,\n",
       " 19,\n",
       " 14,\n",
       " 1,\n",
       " 11,\n",
       " 18,\n",
       " 9,\n",
       " 17,\n",
       " 17,\n",
       " 3,\n",
       " 10,\n",
       " 14,\n",
       " 10,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 2,\n",
       " 18,\n",
       " 19,\n",
       " 0,\n",
       " 10,\n",
       " 9,\n",
       " 18,\n",
       " 18,\n",
       " 13,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 12,\n",
       " 16,\n",
       " 3,\n",
       " 4,\n",
       " 16,\n",
       " 11,\n",
       " 4,\n",
       " 9,\n",
       " 12,\n",
       " 16,\n",
       " 3,\n",
       " 13,\n",
       " 3,\n",
       " 10,\n",
       " 19,\n",
       " 17,\n",
       " 12,\n",
       " 5,\n",
       " 17,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 11,\n",
       " 14,\n",
       " 12,\n",
       " 15,\n",
       " 10,\n",
       " 18,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 15,\n",
       " 17,\n",
       " 7,\n",
       " 3,\n",
       " 12,\n",
       " 12,\n",
       " 5,\n",
       " 0,\n",
       " 15,\n",
       " 5,\n",
       " 10,\n",
       " 10,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 17,\n",
       " 16,\n",
       " 11,\n",
       " 17,\n",
       " 10,\n",
       " 2,\n",
       " 6,\n",
       " 16,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 10,\n",
       " 4,\n",
       " 19,\n",
       " 17,\n",
       " 19,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 12,\n",
       " 19,\n",
       " 18,\n",
       " 4,\n",
       " 13,\n",
       " 2,\n",
       " 6,\n",
       " 10,\n",
       " 10,\n",
       " 19,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 14,\n",
       " 18,\n",
       " 17,\n",
       " 8,\n",
       " 4,\n",
       " 13,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 13,\n",
       " 4,\n",
       " 17,\n",
       " 9,\n",
       " 10,\n",
       " 0,\n",
       " 17,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 15,\n",
       " 2,\n",
       " 11,\n",
       " 17,\n",
       " 18,\n",
       " 17,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 18,\n",
       " 6,\n",
       " 11,\n",
       " 10,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 19,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 12,\n",
       " 9,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 3,\n",
       " 19,\n",
       " 6,\n",
       " 12,\n",
       " 17,\n",
       " 4,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 15,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 14,\n",
       " 3,\n",
       " 10,\n",
       " 19,\n",
       " 8,\n",
       " 14,\n",
       " 13,\n",
       " 5,\n",
       " 1,\n",
       " 10,\n",
       " 16,\n",
       " 14,\n",
       " 9,\n",
       " 17,\n",
       " 5,\n",
       " 11,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 19,\n",
       " 14,\n",
       " 13,\n",
       " 11,\n",
       " 17,\n",
       " 2,\n",
       " 18,\n",
       " 0,\n",
       " 18,\n",
       " 10,\n",
       " 4,\n",
       " 9,\n",
       " 15,\n",
       " 17,\n",
       " 14,\n",
       " 7,\n",
       " 19,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 17,\n",
       " 15,\n",
       " 10,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 15,\n",
       " 14,\n",
       " 5,\n",
       " 1,\n",
       " 17,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 1,\n",
       " 13,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 3,\n",
       " 19,\n",
       " 6,\n",
       " 15,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 18,\n",
       " 3,\n",
       " 10,\n",
       " 17,\n",
       " 5,\n",
       " 2,\n",
       " 10,\n",
       " 4,\n",
       " 1,\n",
       " 15,\n",
       " 17,\n",
       " 19,\n",
       " 16,\n",
       " 10,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 3,\n",
       " 16,\n",
       " 6,\n",
       " 13,\n",
       " 17,\n",
       " 9,\n",
       " 17,\n",
       " 14,\n",
       " 15,\n",
       " 11,\n",
       " 13,\n",
       " 11,\n",
       " 2,\n",
       " 2,\n",
       " 14,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 17,\n",
       " 10,\n",
       " 18,\n",
       " 11,\n",
       " 1,\n",
       " 18,\n",
       " 7,\n",
       " 11,\n",
       " 0,\n",
       " 8,\n",
       " 15,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 13,\n",
       " 8,\n",
       " 5,\n",
       " 17,\n",
       " 18,\n",
       " 2,\n",
       " 17,\n",
       " 5,\n",
       " 14,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 17,\n",
       " 17,\n",
       " 2,\n",
       " 5,\n",
       " 19,\n",
       " 15,\n",
       " 8,\n",
       " 17,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 15,\n",
       " 17,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 14,\n",
       " 12,\n",
       " 13,\n",
       " 8,\n",
       " 9,\n",
       " 11,\n",
       " 15,\n",
       " 2,\n",
       " 12,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 18,\n",
       " 11,\n",
       " 18,\n",
       " 14,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 15,\n",
       " 8,\n",
       " 11,\n",
       " 0,\n",
       " 6,\n",
       " 10,\n",
       " 15,\n",
       " 18,\n",
       " 18,\n",
       " 11,\n",
       " 19,\n",
       " 12,\n",
       " 0,\n",
       " 10,\n",
       " 18,\n",
       " 12,\n",
       " 13,\n",
       " 8,\n",
       " 13,\n",
       " 7,\n",
       " 10,\n",
       " 14,\n",
       " 19,\n",
       " 6,\n",
       " 19,\n",
       " 10,\n",
       " 19,\n",
       " 12,\n",
       " 19,\n",
       " 1,\n",
       " 17,\n",
       " 2,\n",
       " 17,\n",
       " 9,\n",
       " 14,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 17,\n",
       " 19,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 17,\n",
       " 10,\n",
       " 5,\n",
       " 19,\n",
       " 4,\n",
       " 3,\n",
       " 13,\n",
       " 16,\n",
       " 12,\n",
       " 4,\n",
       " 10,\n",
       " 2,\n",
       " 19,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 17,\n",
       " 18,\n",
       " 17,\n",
       " 4,\n",
       " 12,\n",
       " 11,\n",
       " 6,\n",
       " 12,\n",
       " 5,\n",
       " 17,\n",
       " 1,\n",
       " 0,\n",
       " 10,\n",
       " 16,\n",
       " 14,\n",
       " 3,\n",
       " 15,\n",
       " 0,\n",
       " 17,\n",
       " 13,\n",
       " 4,\n",
       " 11,\n",
       " 11,\n",
       " 18,\n",
       " 7,\n",
       " 19,\n",
       " 6,\n",
       " 19,\n",
       " 2,\n",
       " 17,\n",
       " 5,\n",
       " 11,\n",
       " 15,\n",
       " 15,\n",
       " 17,\n",
       " 0,\n",
       " 14,\n",
       " 1,\n",
       " 9,\n",
       " 13,\n",
       " 8,\n",
       " 17,\n",
       " 8,\n",
       " 17,\n",
       " 1,\n",
       " 17,\n",
       " 8,\n",
       " 15,\n",
       " 10,\n",
       " 13,\n",
       " 10,\n",
       " 4,\n",
       " 1,\n",
       " 15,\n",
       " 4,\n",
       " 19,\n",
       " 1,\n",
       " 7,\n",
       " 17,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 17,\n",
       " 12,\n",
       " 9,\n",
       " 11,\n",
       " 6,\n",
       " 17,\n",
       " 19,\n",
       " 17,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 14,\n",
       " 16,\n",
       " 8,\n",
       " 18,\n",
       " 17,\n",
       " 8,\n",
       " 17,\n",
       " 1,\n",
       " 15,\n",
       " 4,\n",
       " 2,\n",
       " 19,\n",
       " 16,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 18,\n",
       " 11,\n",
       " 17,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 14,\n",
       " 3,\n",
       " 0,\n",
       " 12,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 17,\n",
       " 15,\n",
       " 6,\n",
       " 17,\n",
       " 4,\n",
       " 9,\n",
       " 18,\n",
       " 12,\n",
       " 13,\n",
       " 10,\n",
       " 17,\n",
       " 17,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 19,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 17,\n",
       " 1,\n",
       " 8,\n",
       " 13,\n",
       " 3,\n",
       " 19,\n",
       " 5,\n",
       " 13,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 8,\n",
       " 17,\n",
       " 14,\n",
       " 8,\n",
       " 17,\n",
       " 15,\n",
       " 14,\n",
       " 17,\n",
       " 13,\n",
       " 10,\n",
       " 9,\n",
       " 17,\n",
       " 10,\n",
       " 12,\n",
       " 16,\n",
       " 17,\n",
       " 11,\n",
       " 8,\n",
       " 13,\n",
       " 11,\n",
       " 14,\n",
       " 17,\n",
       " 13,\n",
       " 7,\n",
       " 8,\n",
       " 16,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 11,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 19,\n",
       " 1,\n",
       " 6,\n",
       " 11,\n",
       " 4,\n",
       " 17,\n",
       " 7,\n",
       " 17,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 15,\n",
       " 0,\n",
       " 16,\n",
       " 5,\n",
       " 18,\n",
       " 10,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 15,\n",
       " 3,\n",
       " 13,\n",
       " 10,\n",
       " 14,\n",
       " 3,\n",
       " 18,\n",
       " 17,\n",
       " 19,\n",
       " 17,\n",
       " 3,\n",
       " 10,\n",
       " 19,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 2,\n",
       " 13,\n",
       " 19,\n",
       " 10,\n",
       " 0,\n",
       " 17,\n",
       " 17,\n",
       " 15,\n",
       " 14,\n",
       " 18,\n",
       " 17,\n",
       " 19,\n",
       " 3,\n",
       " 11,\n",
       " 19,\n",
       " 0,\n",
       " 8,\n",
       " 14,\n",
       " 14,\n",
       " 17,\n",
       " 18,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 17,\n",
       " 12,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 17,\n",
       " 15,\n",
       " 4,\n",
       " 11,\n",
       " 0,\n",
       " 15,\n",
       " 1,\n",
       " 5,\n",
       " 19,\n",
       " 10,\n",
       " 11,\n",
       " 0,\n",
       " 3,\n",
       " 10,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 16,\n",
       " 18,\n",
       " 2,\n",
       " 13,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 3,\n",
       " 19,\n",
       " 0,\n",
       " 9,\n",
       " 19,\n",
       " 0,\n",
       " 10,\n",
       " 10,\n",
       " 16,\n",
       " 7,\n",
       " 12,\n",
       " 15,\n",
       " 17,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 19,\n",
       " 0,\n",
       " 14,\n",
       " 19,\n",
       " 11,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 10,\n",
       " 1,\n",
       " 3,\n",
       " 10,\n",
       " 4,\n",
       " 7,\n",
       " 17,\n",
       " 6,\n",
       " 16,\n",
       " 10,\n",
       " 14,\n",
       " 6,\n",
       " 13,\n",
       " 15,\n",
       " 2,\n",
       " 15,\n",
       " 2,\n",
       " 10,\n",
       " 12,\n",
       " 19,\n",
       " 8,\n",
       " 14,\n",
       " 8,\n",
       " 17,\n",
       " 8,\n",
       " 17,\n",
       " 19,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 13,\n",
       " 19,\n",
       " 4,\n",
       " 17,\n",
       " 12,\n",
       " 19,\n",
       " 11,\n",
       " 2,\n",
       " 10,\n",
       " 5,\n",
       " 17,\n",
       " 16,\n",
       " 8,\n",
       " 16,\n",
       " 0,\n",
       " 17,\n",
       " 12,\n",
       " 13,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 19,\n",
       " 13,\n",
       " 9,\n",
       " 15,\n",
       " 11,\n",
       " 16,\n",
       " 11,\n",
       " 15,\n",
       " 11,\n",
       " 4,\n",
       " 18,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 4,\n",
       " 10,\n",
       " 17,\n",
       " 10,\n",
       " 14,\n",
       " 9,\n",
       " 19,\n",
       " 6,\n",
       " 10,\n",
       " 15,\n",
       " 17,\n",
       " 3,\n",
       " 17,\n",
       " 1,\n",
       " 13,\n",
       " 17,\n",
       " 19,\n",
       " 17,\n",
       " 10,\n",
       " 5,\n",
       " 14,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 18,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 16,\n",
       " 14,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 17,\n",
       " 5,\n",
       " 17,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 4,\n",
       " 10,\n",
       " 17,\n",
       " 4,\n",
       " 1,\n",
       " 18,\n",
       " 6,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 12,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 15,\n",
       " 14,\n",
       " 7,\n",
       " 11,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 18,\n",
       " 14,\n",
       " 10,\n",
       " 11,\n",
       " 5,\n",
       " 18,\n",
       " 17,\n",
       " 14,\n",
       " 8,\n",
       " 17,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 14,\n",
       " 19,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 14,\n",
       " 17,\n",
       " 13,\n",
       " 2,\n",
       " 19,\n",
       " 14,\n",
       " 4,\n",
       " 16,\n",
       " 2,\n",
       " 3,\n",
       " 18,\n",
       " 7,\n",
       " 8,\n",
       " 12,\n",
       " 3,\n",
       " 13,\n",
       " 5,\n",
       " 18,\n",
       " 19,\n",
       " 12,\n",
       " 14,\n",
       " 13,\n",
       " 9,\n",
       " 11,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pd.DataFrame()\n",
    "test_preds['Id'] = df2['id']\n",
    "test_preds['Category'] = mapping[main_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29995</td>\n",
       "      <td>29995</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29996</td>\n",
       "      <td>29996</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29997</td>\n",
       "      <td>29997</td>\n",
       "      <td>Overwatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29998</td>\n",
       "      <td>29998</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29999</td>\n",
       "      <td>29999</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id       Category\n",
       "0          0       baseball\n",
       "1          1         europe\n",
       "2          2          anime\n",
       "3          3      worldnews\n",
       "4          4          funny\n",
       "...      ...            ...\n",
       "29995  29995         movies\n",
       "29996  29996         movies\n",
       "29997  29997      Overwatch\n",
       "29998  29998  gameofthrones\n",
       "29999  29999            wow\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='test2.csv' target='_blank'>test2.csv</a><br>"
      ],
      "text/plain": [
       "/network/home/penmetss/comp551/test2.csv"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.to_csv(\"test2.csv\", index=False)\n",
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(a.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('xlnet_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
