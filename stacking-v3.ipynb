{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good reference for stacking: https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading needed libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from time import time\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from sklearn.linear_model import *\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for multinoulli NB\n",
    "df = pd.read_csv('reddit_train.csv')\n",
    "df2 = pd.read_csv('reddit_test.csv')\n",
    "# df = df.sample(frac=1)\n",
    "# df = df.iloc[:3000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'], mapping = df['subreddits'].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "features_train = tfidf.fit_transform(df.comments)\n",
    "features_test = tfidf.transform(df2.comments)\n",
    "labels = df['category_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for fine tuning and check which models are the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# features_main = tfidf.fit_transform(df.comments)\n",
    "# features_test = tfidf.transform(df2.comments)\n",
    "# labels = df.category_id\n",
    "# plt.figure(figsize=(18,6),dpi=100)\n",
    "# for k in [10000, 20000, 30000]:\n",
    "\n",
    "#     features = SelectKBest(chi2, k=k).fit_transform(features_main, labels)\n",
    "\n",
    "#     test_acc = []\n",
    "#     vals = [0.1]\n",
    "#     for i in vals:\n",
    "#         gNB = RidgeClassifier()\n",
    "#         gNB.fit(features[:50000,:],labels[:50000])\n",
    "#         test_acc.append(np.mean(gNB.predict(features[50000:,:])==labels[50000:]))\n",
    "# #         print(i, np.mean(gNB.predict(features[50000:,:])==labels[50000:]))\n",
    "\n",
    "#     plt.plot(vals, test_acc, '.-', label=str(k))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# features_main = tfidf.fit_transform(df.comments)\n",
    "# features_test = tfidf.transform(df2.comments)\n",
    "# labels = df.category_id\n",
    "# plt.figure(figsize=(18,6),dpi=100)\n",
    "# for k in [10000, 20000, 30000]:\n",
    "\n",
    "#     features = SelectKBest(chi2, k=k).fit_transform(features_main, labels)\n",
    "\n",
    "#     test_acc = []\n",
    "#     vals = [0.1]\n",
    "#     for i in vals:\n",
    "#         gNB = RidgeClassifier()\n",
    "#         gNB.fit(features[:50000,:],labels[:50000])\n",
    "#         test_acc.append(np.mean(gNB.predict(features[50000:,:])==labels[50000:]))\n",
    "# #         print(i, np.mean(gNB.predict(features[50000:,:])==labels[50000:]))\n",
    "\n",
    "#     plt.plot(vals, test_acc, '.-', label=str(k))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from a sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(classifier, features_train, features_test, labels, do_cv=True):\n",
    "    if(do_cv):\n",
    "        cv_acc = cross_val_score(classifier, features_train, labels, scoring='accuracy', cv=5)\n",
    "        print(' cross validation: ', cv_acc)\n",
    "    classifier.fit(features_train, labels)\n",
    "    f1 = classifier.predict_proba(features_train)\n",
    "    f2 = classifier.predict_proba(features_test)\n",
    "    print(' training accuracy: ', np.mean(classifier.predict(features_train)==labels))\n",
    "    return f1, f2\n",
    "\n",
    "def selectFeatures(features_train, labels, features_test, k=15000):\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    Xtrain = selector.fit_transform(features_train, labels)\n",
    "    Xtest = selector.transform(features_test)\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "def getFeaturesOneHot(classifier, features_train, features_test, labels, do_cv=True):\n",
    "    if(do_cv):\n",
    "        cv_acc = cross_val_score(classifier, features_train, labels, scoring='accuracy', cv=5)\n",
    "        print(' cross validation: ', cv_acc)\n",
    "    classifier.fit(features_train, labels)\n",
    "    f1 = classifier.predict(features_train)\n",
    "    f2 = classifier.predict(features_test)\n",
    "    print(' training accuracy: ', np.mean(classifier.predict(features_train)==labels))\n",
    "    return to_categorical(f1, num_classes=20), to_categorical(f2, num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cross validation:  [0.5985     0.60178571 0.59757143 0.59671429 0.60064286]\n",
      " training accuracy:  0.7264285714285714\n",
      " cross validation:  [0.5835     0.59071429 0.5845     0.58192857 0.58378571]\n",
      " training accuracy:  0.7068285714285715\n"
     ]
    }
   ],
   "source": [
    "tmp_tr, tmp_te = selectFeatures(features_train, labels, features_test, k=16000)\n",
    "m3_train, m3_test = getFeatures(MultinomialNB(0.05), tmp_tr, tmp_te, labels)\n",
    "tmp_tr, tmp_te = selectFeatures(features_train, labels, features_test, k=11000)\n",
    "m4_train, m4_test = getFeaturesOneHot(LinearSVC(C=0.5), tmp_tr, tmp_te, labels)\n",
    "# m9_train, m9_test = getFeaturesOneHot(RidgeClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from pytorch models (saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions\n",
    "def pickle2np(filename=\"preds/bert-preds-test.p\"):\n",
    "    tmp = pickle.load( open( filename, \"rb\" ) )\n",
    "    preds = list()\n",
    "    for i in range(len(tmp)):\n",
    "        for j in range(len(tmp[i])):\n",
    "            preds.append(list(tmp[i][j]))\n",
    "            \n",
    "    return np.array(preds)\n",
    "\n",
    "m1_train = pickle2np(\"preds/bert-preds-train.p\")\n",
    "m1_test = pickle2np(\"preds/bert-preds-test.p\")\n",
    "m2_train = pickle2np(\"preds/xlnet-preds-train.p\")\n",
    "m2_test = pickle2np(\"preds/xlnet-preds-test.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m3_train = m3_train[:3000, :]\n",
    "# m4_train = m4_train[:3000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 80) (30000, 80) (70000,)\n"
     ]
    }
   ],
   "source": [
    "# stacking the features\n",
    "features_tr = np.concatenate((m1_train, m2_train, m3_train, m4_train),axis=1)\n",
    "features_te = np.concatenate((m1_test, m2_test, m3_test, m4_test),axis=1)\n",
    "labels = df.category_id\n",
    "print(features_tr.shape, features_te.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 24413 is out of bounds for axis 0 with size 3000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5c573a7367bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# uncomment if you want to work on a smaller sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m70000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 24413 is out of bounds for axis 0 with size 3000"
     ]
    }
   ],
   "source": [
    "# uncomment if you want to work on a smaller sample\n",
    "samples = np.random.randint(70000,size=(1000))\n",
    "features = features[samples,:]\n",
    "labels = df.category_id[samples]\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:  0.83285\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbm.predict(features_tr[:60000], num_iteration=gbm.best_iteration)\n",
    "print('train_acc: ', np.mean(np.argmax(y_pred, axis=1)==labels[:60000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.4001\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's multi_logloss: 2.14075\n",
      "[3]\tvalid_0's multi_logloss: 1.96026\n",
      "[4]\tvalid_0's multi_logloss: 1.82044\n",
      "[5]\tvalid_0's multi_logloss: 1.70722\n",
      "[6]\tvalid_0's multi_logloss: 1.61284\n",
      "[7]\tvalid_0's multi_logloss: 1.5321\n",
      "[8]\tvalid_0's multi_logloss: 1.46263\n",
      "[9]\tvalid_0's multi_logloss: 1.40261\n",
      "[10]\tvalid_0's multi_logloss: 1.34992\n",
      "[11]\tvalid_0's multi_logloss: 1.30321\n",
      "[12]\tvalid_0's multi_logloss: 1.26156\n",
      "[13]\tvalid_0's multi_logloss: 1.2246\n",
      "[14]\tvalid_0's multi_logloss: 1.19129\n",
      "[15]\tvalid_0's multi_logloss: 1.16145\n",
      "[16]\tvalid_0's multi_logloss: 1.13441\n",
      "[17]\tvalid_0's multi_logloss: 1.1098\n",
      "[18]\tvalid_0's multi_logloss: 1.08802\n",
      "[19]\tvalid_0's multi_logloss: 1.06878\n",
      "[20]\tvalid_0's multi_logloss: 1.0512\n",
      "[21]\tvalid_0's multi_logloss: 1.03511\n",
      "[22]\tvalid_0's multi_logloss: 1.02059\n",
      "[23]\tvalid_0's multi_logloss: 1.00745\n",
      "[24]\tvalid_0's multi_logloss: 0.995536\n",
      "[25]\tvalid_0's multi_logloss: 0.984328\n",
      "[26]\tvalid_0's multi_logloss: 0.974172\n",
      "[27]\tvalid_0's multi_logloss: 0.96555\n",
      "[28]\tvalid_0's multi_logloss: 0.958144\n",
      "[29]\tvalid_0's multi_logloss: 0.950138\n",
      "[30]\tvalid_0's multi_logloss: 0.94326\n",
      "[31]\tvalid_0's multi_logloss: 0.937125\n",
      "[32]\tvalid_0's multi_logloss: 0.932608\n",
      "[33]\tvalid_0's multi_logloss: 0.928904\n",
      "[34]\tvalid_0's multi_logloss: 0.924168\n",
      "[35]\tvalid_0's multi_logloss: 0.919769\n",
      "[36]\tvalid_0's multi_logloss: 0.916136\n",
      "[37]\tvalid_0's multi_logloss: 0.912892\n",
      "[38]\tvalid_0's multi_logloss: 0.910416\n",
      "[39]\tvalid_0's multi_logloss: 0.908704\n",
      "[40]\tvalid_0's multi_logloss: 0.906388\n",
      "[41]\tvalid_0's multi_logloss: 0.904001\n",
      "[42]\tvalid_0's multi_logloss: 0.902466\n",
      "[43]\tvalid_0's multi_logloss: 0.900596\n",
      "[44]\tvalid_0's multi_logloss: 0.900843\n",
      "[45]\tvalid_0's multi_logloss: 0.899633\n",
      "[46]\tvalid_0's multi_logloss: 0.900415\n",
      "[47]\tvalid_0's multi_logloss: 0.899007\n",
      "[48]\tvalid_0's multi_logloss: 0.901973\n",
      "[49]\tvalid_0's multi_logloss: 0.903182\n",
      "[50]\tvalid_0's multi_logloss: 0.901984\n",
      "[51]\tvalid_0's multi_logloss: 0.903605\n",
      "[52]\tvalid_0's multi_logloss: 0.902429\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's multi_logloss: 0.899007\n",
      "test_acc:  0.7322\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'task': 'train'\n",
    " , 'boosting_type': 'gbdt'\n",
    " , 'objective': 'multiclass'\n",
    " , 'num_class': 20\n",
    " , 'metric': 'multi_logloss'\n",
    " , 'min_data': 1\n",
    " , 'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(features_tr[:60000,:], labels[:60000])\n",
    "lgb_eval = lgb.Dataset(features_tr[60000:,:], labels[60000:], reference=lgb_train)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "y_pred = gbm.predict(features_tr[:60000], num_iteration=gbm.best_iteration)\n",
    "print('train_acc: ', np.mean(np.argmax(y_pred, axis=1)==labels[:60000]))\n",
    "y_pred = gbm.predict(features_tr[60000:], num_iteration=gbm.best_iteration)\n",
    "print('test_acc: ', np.mean(np.argmax(y_pred, axis=1)==labels[60000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc:  0.7565666666666667\n",
      "test_acc:  0.7423\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(features_tr[:60000])\n",
    "print('train_acc: ', np.mean(y_pred==labels[:60000]))\n",
    "y_pred = reg.predict(features_tr[60000:])\n",
    "print('test_acc: ', np.mean(y_pred==labels[60000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 80) (70000,)\n",
      "483.2753665447235\n",
      "train_acc:  7105      True\n",
      "15578    False\n",
      "44994     True\n",
      "34706    False\n",
      "30169     True\n",
      "         ...  \n",
      "68895     True\n",
      "45005     True\n",
      "28733    False\n",
      "29234    False\n",
      "16829     True\n",
      "Name: category_id, Length: 60000, dtype: bool\n",
      "test_acc:  44192     True\n",
      "6004      True\n",
      "5089     False\n",
      "68149     True\n",
      "59670    False\n",
      "         ...  \n",
      "56240    False\n",
      "6718      True\n",
      "36722     True\n",
      "11757     True\n",
      "15685     True\n",
      "Name: category_id, Length: 10000, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(features_tr.shape,labels.shape)\n",
    "reg = XGBClassifier()\n",
    "start_time = time()\n",
    "reg.fit(features_tr[:60000],labels[:60000])\n",
    "print(time()-start_time)\n",
    "y_pred = reg.predict(features_tr[:60000])\n",
    "print('train_acc: ', np.mean(y_pred==labels[:60000]))\n",
    "y_pred = reg.predict(features_tr[60000:])\n",
    "print('test_acc: ', np.mean(y_pred==labels[60000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tuning the meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.89\n",
      "0.1 0.9\n",
      "0.2 0.88\n",
      "0.3 0.88\n",
      "1 0.85\n"
     ]
    }
   ],
   "source": [
    "for eta in [0.01, 0.1, 0.2, 0.3, 1]:\n",
    "    gNB = XGBClassifier(learning_rate=eta)\n",
    "    gNB.fit(features[:900,:],labels[:900])\n",
    "    print(eta, np.mean(gNB.predict(features[900:,:])==labels[900:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit classifier on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 80) (70000,)\n",
      "565.3684413433075\n",
      "train_acc:  0.8282285714285714\n"
     ]
    }
   ],
   "source": [
    "print(features_tr.shape,labels.shape)\n",
    "reg = XGBClassifier()\n",
    "start_time = time()\n",
    "reg.fit(features_tr,labels)\n",
    "print(time()-start_time)\n",
    "y_pred = reg.predict(features_tr)\n",
    "print('train_acc: ', np.mean(y_pred==labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.35294\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's multi_logloss: 2.07322\n",
      "[3]\tvalid_0's multi_logloss: 1.87815\n",
      "[4]\tvalid_0's multi_logloss: 1.72695\n",
      "[5]\tvalid_0's multi_logloss: 1.60418\n",
      "[6]\tvalid_0's multi_logloss: 1.50152\n",
      "[7]\tvalid_0's multi_logloss: 1.4142\n",
      "[8]\tvalid_0's multi_logloss: 1.33899\n",
      "[9]\tvalid_0's multi_logloss: 1.2723\n",
      "[10]\tvalid_0's multi_logloss: 1.21348\n",
      "[11]\tvalid_0's multi_logloss: 1.1612\n",
      "[12]\tvalid_0's multi_logloss: 1.11491\n",
      "[13]\tvalid_0's multi_logloss: 1.07342\n",
      "[14]\tvalid_0's multi_logloss: 1.0361\n",
      "[15]\tvalid_0's multi_logloss: 1.00265\n",
      "[16]\tvalid_0's multi_logloss: 0.9719\n",
      "[17]\tvalid_0's multi_logloss: 0.944219\n",
      "[18]\tvalid_0's multi_logloss: 0.919167\n",
      "[19]\tvalid_0's multi_logloss: 0.896006\n",
      "[20]\tvalid_0's multi_logloss: 0.875473\n",
      "[21]\tvalid_0's multi_logloss: 0.856097\n",
      "[22]\tvalid_0's multi_logloss: 0.839466\n",
      "[23]\tvalid_0's multi_logloss: 0.823055\n",
      "[24]\tvalid_0's multi_logloss: 0.808562\n",
      "[25]\tvalid_0's multi_logloss: 0.795358\n",
      "[26]\tvalid_0's multi_logloss: 0.78333\n",
      "[27]\tvalid_0's multi_logloss: 0.772142\n",
      "[28]\tvalid_0's multi_logloss: 0.762015\n",
      "[29]\tvalid_0's multi_logloss: 0.752669\n",
      "[30]\tvalid_0's multi_logloss: 0.744804\n",
      "[31]\tvalid_0's multi_logloss: 0.736727\n",
      "[32]\tvalid_0's multi_logloss: 0.729503\n",
      "[33]\tvalid_0's multi_logloss: 0.722863\n",
      "[34]\tvalid_0's multi_logloss: 0.717014\n",
      "[35]\tvalid_0's multi_logloss: 0.710891\n",
      "[36]\tvalid_0's multi_logloss: 0.70492\n",
      "[37]\tvalid_0's multi_logloss: 0.700111\n",
      "[38]\tvalid_0's multi_logloss: 0.695195\n",
      "[39]\tvalid_0's multi_logloss: 0.691597\n",
      "[40]\tvalid_0's multi_logloss: 0.6876\n",
      "[41]\tvalid_0's multi_logloss: 0.684043\n",
      "[42]\tvalid_0's multi_logloss: 0.681225\n",
      "[43]\tvalid_0's multi_logloss: 0.678189\n",
      "[44]\tvalid_0's multi_logloss: 0.675251\n",
      "[45]\tvalid_0's multi_logloss: 0.673005\n",
      "[46]\tvalid_0's multi_logloss: 0.672295\n",
      "[47]\tvalid_0's multi_logloss: 0.669615\n",
      "[48]\tvalid_0's multi_logloss: 0.667233\n",
      "[49]\tvalid_0's multi_logloss: 0.665053\n",
      "[50]\tvalid_0's multi_logloss: 0.662816\n",
      "[51]\tvalid_0's multi_logloss: 0.660669\n",
      "[52]\tvalid_0's multi_logloss: 0.658739\n",
      "[53]\tvalid_0's multi_logloss: 0.65687\n",
      "[54]\tvalid_0's multi_logloss: 0.655186\n",
      "[55]\tvalid_0's multi_logloss: 0.653439\n",
      "[56]\tvalid_0's multi_logloss: 0.65244\n",
      "[57]\tvalid_0's multi_logloss: 0.652257\n",
      "[58]\tvalid_0's multi_logloss: 0.65156\n",
      "[59]\tvalid_0's multi_logloss: 0.650718\n",
      "[60]\tvalid_0's multi_logloss: 0.654785\n",
      "[61]\tvalid_0's multi_logloss: 0.653339\n",
      "[62]\tvalid_0's multi_logloss: 0.652273\n",
      "[63]\tvalid_0's multi_logloss: 0.651224\n",
      "[64]\tvalid_0's multi_logloss: 0.650941\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's multi_logloss: 0.650718\n",
      "train_acc:  0.9103666666666667\n",
      "test_acc:  0.8039\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'task': 'train'\n",
    " , 'boosting_type': 'gbdt'\n",
    " , 'objective': 'multiclass'\n",
    " , 'num_class': 20\n",
    " , 'metric': 'multi_logloss'\n",
    " , 'min_data': 1\n",
    " , 'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(features_tr[:60000,:], labels[:60000])\n",
    "lgb_eval = lgb.Dataset(features_tr[60000:,:], labels[60000:], reference=lgb_train)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "y_pred = gbm.predict(features_tr[:60000], num_iteration=gbm.best_iteration)\n",
    "print('train_acc: ', np.mean(np.argmax(y_pred, axis=1)==labels[:60000]))\n",
    "y_pred = gbm.predict(features_tr[60000:], num_iteration=gbm.best_iteration)\n",
    "print('test_acc: ', np.mean(np.argmax(y_pred, axis=1)==labels[60000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8646857142857143"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combined train accuracy\n",
    "preds = np.argmax(reg.predict_proba(features_tr) + gbm.predict(features_tr, num_iteration=gbm.best_iteration), axis=1)\n",
    "np.mean(preds==labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(reg.predict_proba(features_te) + gbm.predict(features_te, num_iteration=gbm.best_iteration), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pd.DataFrame()\n",
    "test_preds['Id'] = df2['id']\n",
    "test_preds['Category'] = mapping[preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='test.csv' target='_blank'>test.csv</a><br>"
      ],
      "text/plain": [
       "/network/home/penmetss/comp551/test.csv"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.to_csv(\"test.csv\", index=False)\n",
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
